{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 - BiDAF, Bi-Directional Attention Flow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOA6fMFNaoWQiHL1zQavCUp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dksifoua/Question-Answering/blob/master/2%20-%20BiDAF%2C%20Bi-Directional%20Attention%20Flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3jaO1fBTsLh",
        "outputId": "0de32081-3b70-4db4-ce01-2feea5c79198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov  2 00:15:26 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGiJMYm_B81X"
      },
      "source": [
        "## Load dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gPQcEJLBv1v"
      },
      "source": [
        "!pip install tqdm --upgrade >> /dev/null 2>&1\n",
        "!pip install spacy --upgrade >> /dev/null 2>&1\n",
        "!python -m spacy download en >> /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWtyKHkMB_8G"
      },
      "source": [
        "import re\n",
        "import json\n",
        "import time\n",
        "import tqdm\n",
        "import spacy\n",
        "import string\n",
        "import itertools\n",
        "import collections\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW1BjtqnCBsT",
        "outputId": "70c06412-be53-44ae-9dbe-cc6caca03f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "SEED = 546\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ggRN9-XCFHt"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "***Download data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb86TTR9CDo1",
        "outputId": "4d32332f-4ae0-4525-c0c9-9472d554d6f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm -rf ./data\n",
        "!mkdir ./data\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json \\\n",
        "    -O ./data/train-v1.1.json\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json \\\n",
        "    -O ./data/dev-v1.1.json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-02 00:15:51--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.111.153, 185.199.108.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30288272 (29M) [application/json]\n",
            "Saving to: ‘./data/train-v1.1.json’\n",
            "\n",
            "./data/train-v1.1.j 100%[===================>]  28.88M  60.8MB/s    in 0.5s    \n",
            "\n",
            "2020-11-02 00:15:53 (60.8 MB/s) - ‘./data/train-v1.1.json’ saved [30288272/30288272]\n",
            "\n",
            "--2020-11-02 00:15:53--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.111.153, 185.199.108.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4854279 (4.6M) [application/json]\n",
            "Saving to: ‘./data/dev-v1.1.json’\n",
            "\n",
            "./data/dev-v1.1.jso 100%[===================>]   4.63M  22.3MB/s    in 0.2s    \n",
            "\n",
            "2020-11-02 00:15:54 (22.3 MB/s) - ‘./data/dev-v1.1.json’ saved [4854279/4854279]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0y727K0CLPi"
      },
      "source": [
        "***Load JSON data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfu-hBviCJlX"
      },
      "source": [
        "def load(path):\n",
        "    with open(path, mode='r', encoding='utf-8') as file:\n",
        "        return json.load(file)['data']\n",
        "    raise FileNotFoundError"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4fMZAv3CNrn",
        "outputId": "be3786f9-0e68-43e1-819c-c784781055f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_raw_data = load('./data/train-v1.1.json')\n",
        "valid_raw_data = load('./data/dev-v1.1.json')\n",
        "print(f'Length of raw train data: {len(train_raw_data):,}')\n",
        "print(f'Length of raw valid data: {len(valid_raw_data):,}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of raw train data: 442\n",
            "Length of raw valid data: 48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERscP6f0CQ0O"
      },
      "source": [
        "***Parse JSON data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATlQnyzzCPiD"
      },
      "source": [
        "def parse(data, nlp=spacy.load('en')):\n",
        "    qas = []\n",
        "    for paragraphs in tqdm.tqdm(data):\n",
        "        for para in paragraphs['paragraphs']:\n",
        "            context = nlp(para['context'], disable=['parser'])\n",
        "            for qa in para['qas']:\n",
        "                id = qa['id']\n",
        "                question = nlp(qa['question'], disable=['parser', 'tagger', 'ner'])\n",
        "                for ans in qa['answers']:\n",
        "                    qas.append({\n",
        "                        'id': id,\n",
        "                        'context': context,\n",
        "                        'question': question,\n",
        "                        'answer': nlp(ans['text'], disable=['parser', 'tagger', 'ner']),\n",
        "                        'answer_start': ans['answer_start'],\n",
        "                    })\n",
        "    return qas"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt9BCilFCUQa",
        "outputId": "7a1a5380-259b-44de-f1ea-64fe56c5e965",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_qas = parse(train_raw_data)\n",
        "valid_qas = parse(valid_raw_data)\n",
        "print()\n",
        "print(f'Length of train qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs: {len(valid_qas):,}')\n",
        "print('==================== Example ====================')\n",
        "print('Id:', train_qas[0]['id'])\n",
        "print('Context:', train_qas[0]['context'])\n",
        "print('Question:', train_qas[0]['question'])\n",
        "print('Answer starts at:', train_qas[0]['answer_start'])\n",
        "print('Answer:', train_qas[0]['answer'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 442/442 [04:43<00:00,  1.56it/s]\n",
            "100%|██████████| 48/48 [00:34<00:00,  1.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Length of train qa pairs: 87,599\n",
            "Length of valid qa pairs: 34,726\n",
            "==================== Example ====================\n",
            "Id: 5733be284776f41900661182\n",
            "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Answer starts at: 515\n",
            "Answer: Saint Bernadette Soubirous\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHPb0qoZCWJI"
      },
      "source": [
        "def test_answer_start(qas):\n",
        "    \"\"\"Test answer_start are correct in train set\"\"\"\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        answer = qa['answer'].text\n",
        "        context = qa['context'].text\n",
        "        answer_start = qa['answer_start']\n",
        "        assert answer == context[answer_start:answer_start + len(answer)]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DlkS-j_CYGP",
        "outputId": "d79ce175-3e01-4f85-a60d-d71139d6c63f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_answer_start(train_qas)\n",
        "test_answer_start(valid_qas)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 87599/87599 [00:07<00:00, 11019.19it/s]\n",
            "100%|██████████| 34726/34726 [00:02<00:00, 11754.15it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouXUgEPDCbLD"
      },
      "source": [
        "***Add targets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buP4vPkYCZw7"
      },
      "source": [
        "def add_targets(qas):\n",
        "    \"\"\"Add start and end index token\"\"\"\n",
        "    for qa in qas:\n",
        "        context = qa['context']\n",
        "        answer = qa['answer']\n",
        "        ans_start = qa['answer_start']\n",
        "        for i in range(len(context)):\n",
        "            if context[i].idx == ans_start:\n",
        "                ans = context[i:i + len(answer)]\n",
        "                qa['target'] = [ans[0].i, ans[-1].i]\n",
        "                break"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7mSkiVkCd_K",
        "outputId": "bbb0a053-13b7-4a0a-aaa0-5ed75554c163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "add_targets(train_qas)\n",
        "add_targets(valid_qas)\n",
        "print(f'Length of train qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs: {len(valid_qas):,}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs: 87,599\n",
            "Length of valid qa pairs: 34,726\n",
            "CPU times: user 1.5 s, sys: 22 ms, total: 1.52 s\n",
            "Wall time: 1.53 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5_zJuHvCffV"
      },
      "source": [
        "def filter_qas(qa):\n",
        "    \"\"\"Remove bad targets\"\"\"\n",
        "    if 'target' in [*qa.keys()]:\n",
        "        start, end = qa['target']\n",
        "        return qa['context'][start:end + 1].text == qa['answer'].text\n",
        "    return False"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkVgJ6IyCg0x",
        "outputId": "2e7d4001-b6d5-422a-b274-4ef0931deb29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "train_qas = [*filter(filter_qas, train_qas)]\n",
        "valid_qas = [*filter(filter_qas, valid_qas)]\n",
        "print(f'Length of train qa pairs after filtering out bad qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs after filtering out bad qa pairs: {len(valid_qas):,}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs after filtering out bad qa pairs: 86,597\n",
            "Length of valid qa pairs after filtering out bad qa pairs: 34,295\n",
            "CPU times: user 1.16 s, sys: 2.01 ms, total: 1.16 s\n",
            "Wall time: 1.16 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSMjuQhwCiNv"
      },
      "source": [
        "def test_targets(qas):\n",
        "    for qa in qas:\n",
        "        if 'target' in [*qa.keys()]:\n",
        "            start, end = qa['target']\n",
        "            assert qa['context'][start:end + 1].text == qa['answer'].text"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr7E1xOjCj-K",
        "outputId": "7e54fee5-81d9-44ad-8ee8-c931532d76a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "test_targets(train_qas)\n",
        "test_targets(valid_qas)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.09 s, sys: 957 µs, total: 1.09 s\n",
            "Wall time: 1.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84HJAR6GdpED"
      },
      "source": [
        "***Build vocabularies***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAmWSXQjCljY"
      },
      "source": [
        "class Vocab:\n",
        "\n",
        "    def __init__(self, pad_token, unk_token):\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.vocab = None\n",
        "        self.word2count = None\n",
        "        self.word2index = None\n",
        "        self.index2word = None\n",
        "    \n",
        "    def build(self, data, min_freq):\n",
        "        \"\"\"\n",
        "        :param List[Union[spacy.tokens.doc.Doc, str]] data\n",
        "        :param int min_freq\n",
        "        \"\"\"\n",
        "        words = [self.pad_token, self.unk_token]\n",
        "        type_0 = type(data[0])\n",
        "        if type_0 == spacy.tokens.doc.Doc:\n",
        "            for item in data: # context and question\n",
        "                words += [word.text.lower() for word in item]\n",
        "        elif type_0 == str: # id\n",
        "            words += data\n",
        "        self.word2count = collections.Counter(words)\n",
        "        self.vocab = sorted(filter(\n",
        "            lambda word: self.word2count[word] >= min_freq or word == self.pad_token or word == self.unk_token, self.word2count\n",
        "        ))\n",
        "        self.word2index = {word: index for index, word in enumerate(self.vocab)}\n",
        "        self.index2word = {index: word for index, word in enumerate(self.vocab)}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "    \n",
        "    def stoi(self, word):\n",
        "        return self.word2index.get(str(word), self.word2index[self.unk_token])\n",
        "\n",
        "    def itos(self, index):\n",
        "        return self.index2word[index]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r99JmT2jWWL"
      },
      "source": [
        "class CharVocab:\n",
        "\n",
        "    def __init__(self, pad_token, unk_token):\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.vocab = None\n",
        "        self.char2count = None\n",
        "        self.char2index = None\n",
        "        self.index2char = None\n",
        "    \n",
        "    def build(self, data, min_freq):\n",
        "        \"\"\"\n",
        "        :param List[Union[spacy.tokens.doc.Doc, str, Tuple]] data\n",
        "        :param int min_freq\n",
        "        \"\"\"\n",
        "        chars = [self.pad_token, self.unk_token]\n",
        "        type_0 = type(data[0])\n",
        "        if type_0 == spacy.tokens.doc.Doc:\n",
        "            for item in data: # context and question\n",
        "                for word in item:\n",
        "                    chars += [*word.text.lower().strip()]\n",
        "        else:\n",
        "            raise Exception\n",
        "        self.char2count = collections.Counter(chars)\n",
        "        self.vocab = sorted(filter(\n",
        "            lambda char: self.char2count[char] >= min_freq or char == self.pad_token or char == self.unk_token, self.char2count\n",
        "        ))\n",
        "        self.char2index = {char: index for index, char in enumerate(self.vocab)}\n",
        "        self.index2char = {index: char for index, char in enumerate(self.vocab)}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "    \n",
        "    def stoi(self, char):\n",
        "        return self.char2index.get(str(char), self.char2index[self.unk_token])\n",
        "\n",
        "    def itos(self, index):\n",
        "        return self.index2char[index]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7tXPZ9Tds_8",
        "outputId": "4887463d-584e-4ad3-983e-f8b56acfe970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "PAD_TOKEN = '<pad>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "\n",
        "ID = Vocab(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN)\n",
        "TEXT = Vocab(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN)\n",
        "CHAR = CharVocab(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN)\n",
        "\n",
        "ids = [*map(lambda qa: qa['id'], train_qas)] + [*map(lambda qa: qa['id'], valid_qas)]\n",
        "contexts, questions = zip(*map(lambda qa: (qa['context'], qa['question']), train_qas))\n",
        "\n",
        "ID.build(data=[*set(ids)], min_freq=0)\n",
        "TEXT.build(data=[*set(contexts)] + [*set(questions)], min_freq=5)\n",
        "CHAR.build(data=[*set(contexts)] + [*set(questions)], min_freq=0)\n",
        "\n",
        "print(f'Length of ID vocabulary: {len(ID):,}')\n",
        "print(f'Length of TEXT vocabulary: {len(TEXT):,}')\n",
        "print(f'Length of CHAR vocabulary: {len(CHAR):,}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of ID vocabulary: 97,108\n",
            "Length of TEXT vocabulary: 26,885\n",
            "Length of CHAR vocabulary: 1,261\n",
            "CPU times: user 9.44 s, sys: 271 ms, total: 9.71 s\n",
            "Wall time: 9.71 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rapWup-Ud2Av"
      },
      "source": [
        "***Build datasets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdtOEqEudz6d"
      },
      "source": [
        "class SQuADV1Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, id_vocab, text_vocab, char_vocab):\n",
        "        self.data = data\n",
        "        self.id_vocab = id_vocab\n",
        "        self.text_vocab = text_vocab\n",
        "        self.char_vocab = char_vocab\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        \n",
        "        id = torch.LongTensor([self.id_vocab.stoi(item['id'])])\n",
        "\n",
        "        context = item['context']\n",
        "        ctx = torch.LongTensor([*map(lambda word: self.text_vocab.stoi(word.text.lower()), context)])\n",
        "        char_ctx = [*map(lambda word: [*map(self.char_vocab.stoi, [*word.text.lower().strip()])], context)]\n",
        "        char_len_ctx = torch.LongTensor([*map(len, char_ctx)])\n",
        "        char_ctx = torch.LongTensor([*itertools.chain.from_iterable(char_ctx)])\n",
        "        \n",
        "        question = item['question']\n",
        "        qst = torch.LongTensor([*map(lambda word: self.text_vocab.stoi(word.text.lower()), question)])\n",
        "        char_qst = [*map(lambda word: [*map(self.char_vocab.stoi, [*word.text.lower().strip()])], question)]\n",
        "        char_len_qst = torch.LongTensor([*map(len, char_qst)])\n",
        "        char_qst = torch.LongTensor([*itertools.chain.from_iterable(char_qst)])\n",
        "        \n",
        "        trg = torch.LongTensor(item['target'])\n",
        "        \n",
        "        return id, ctx, char_ctx, char_len_ctx, qst, char_qst, char_len_qst, trg"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-HEQ-hXd-Ku",
        "outputId": "a4c985d4-5baa-4bfe-9088-9c7fad3df1d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataset = SQuADV1Dataset(data=train_qas, id_vocab=ID, text_vocab=TEXT, char_vocab=CHAR)\n",
        "valid_dataset = SQuADV1Dataset(data=valid_qas, id_vocab=ID, text_vocab=TEXT, char_vocab=CHAR)\n",
        "\n",
        "id, ctx, char_ctx, char_len_ctx, qst, char_qst, char_len_qst, trg = train_dataset[0]\n",
        "print(f'id shape: {id.shape}')\n",
        "print(f'ctx shape: {ctx.shape}')\n",
        "print(f'char_ctx shape: {char_ctx.shape}')\n",
        "print(f'char_len_ctx shape: {char_len_ctx.shape}')\n",
        "print(f'qst shape: {qst.shape}')\n",
        "print(f'char_qst shape: {char_qst.shape}')\n",
        "print(f'char_len_qst shape: {char_len_qst.shape}')\n",
        "print(f'trg shape: {trg.shape}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id shape: torch.Size([1])\n",
            "ctx shape: torch.Size([142])\n",
            "char_ctx shape: torch.Size([572])\n",
            "char_len_ctx shape: torch.Size([142])\n",
            "qst shape: torch.Size([14])\n",
            "char_qst shape: torch.Size([59])\n",
            "char_len_qst shape: torch.Size([14])\n",
            "trg shape: torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oega32XHeGKk"
      },
      "source": [
        "***Build data loaders***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVTo0LX2eDUM"
      },
      "source": [
        "class DotDict(dict):\n",
        "    __getattr__ = dict.get"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Srky9S0INuy"
      },
      "source": [
        "def pad_char_sequence(batch_padded_seq, batch_char_seq, batch_char_len, pad_index):\n",
        "    \"\"\"\n",
        "    :param Tensor[batch_size, seq_len] batch_padded_seq\n",
        "    :param ListTensor[batch_size, seq_len * n_chars] batch_char_seq\n",
        "    :param ListTensor[batch_size, seq_len] batch_char_len\n",
        "    \"\"\"\n",
        "    max_char_len = 0\n",
        "    for char_len in batch_char_len:\n",
        "        m = char_len.max().item()\n",
        "        if m > max_char_len:\n",
        "            max_char_len = m\n",
        "    batch_padded_char = torch.ones((batch_padded_seq.size(0), batch_padded_seq.size(1), max_char_len)) * pad_index\n",
        "    for i in range(batch_padded_seq.size(0)):\n",
        "        seq = batch_padded_seq[i] # [seq_len]\n",
        "        char_seq = batch_char_seq[i] # [seq_len * n_chars]\n",
        "        char_len = batch_char_len[i] # [seq_len]\n",
        "        j = 0\n",
        "        for k, length in enumerate(char_len):\n",
        "            batch_padded_char[i, k, j:length] = char_seq[j:length]\n",
        "            j += length\n",
        "    return batch_padded_char"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPCbAkgieIHA"
      },
      "source": [
        "def add_padding(batch, text_vocab=TEXT, char_vocab=CHAR, include_lengths=True, device=DEVICE):\n",
        "    \"\"\"Pad batch of sequence with different lengths\"\"\"\n",
        "    batch_id, batch_ctx, batch_char_ctx, batch_char_len_ctx, batch_qst, batch_char_qst, batch_char_len_qst, batch_trg = zip(*batch)\n",
        "    if include_lengths:\n",
        "        len_ctx = torch.LongTensor([ctx.size(0) for ctx in batch_ctx]).to(device)\n",
        "        len_qst = torch.LongTensor([qst.size(0) for qst in batch_qst]).to(device)\n",
        "    batch_padded_id = pad_sequence(batch_id, batch_first=True).to(device)\n",
        "    batch_padded_ctx = pad_sequence(batch_ctx, batch_first=True, padding_value=text_vocab.stoi(text_vocab.pad_token)).to(device)\n",
        "    batch_padded_qst = pad_sequence(batch_qst, batch_first=True, padding_value=text_vocab.stoi(text_vocab.pad_token)).to(device)\n",
        "    batch_padded_trg = pad_sequence(batch_trg, batch_first=True).to(device)\n",
        "\n",
        "    pad_index = char_vocab.stoi(text_vocab.pad_token)\n",
        "    batch_padded_char_ctx = pad_char_sequence(batch_padded_seq=batch_padded_ctx,\n",
        "                                              batch_char_seq=batch_char_ctx,\n",
        "                                              batch_char_len=batch_char_len_ctx,\n",
        "                                              pad_index=pad_index)\n",
        "    batch_padded_char_qst = pad_char_sequence(batch_padded_seq=batch_padded_qst,\n",
        "                                              batch_char_seq=batch_char_qst,\n",
        "                                              batch_char_len=batch_char_len_qst,\n",
        "                                              pad_index=pad_index)\n",
        "    return DotDict({\n",
        "        'id': batch_padded_id,\n",
        "        'ctx': (batch_padded_ctx, len_ctx) if include_lengths else batch_padded_ctx,\n",
        "        'char_ctx': batch_padded_char_ctx,\n",
        "        'qst': (batch_padded_qst, len_qst) if include_lengths else batch_padded_qst,\n",
        "        'char_qst': batch_padded_char_qst,\n",
        "        'trg': batch_padded_trg\n",
        "    })"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzjzrs0jeRqH",
        "outputId": "9ac1cf5e-da20-46ec-880e-d294e537f71e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=add_padding)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=add_padding)\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    print('batch.id shape:', batch.id.shape)\n",
        "    print('batch.ctx shape:', batch.ctx[0].shape, batch.ctx[1].shape)\n",
        "    print('batch.char_ctx shape:', batch.char_ctx.shape)\n",
        "    print('batch.qst shape:', batch.qst[0].shape, batch.qst[1].shape)\n",
        "    print('batch.char_qst shape:', batch.char_qst.shape)\n",
        "    print('batch.trg shape:', batch.trg.shape)\n",
        "    break"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch.id shape: torch.Size([32, 1])\n",
            "batch.ctx shape: torch.Size([32, 253]) torch.Size([32])\n",
            "batch.char_ctx shape: torch.Size([32, 253, 15])\n",
            "batch.qst shape: torch.Size([32, 19]) torch.Size([32])\n",
            "batch.char_qst shape: torch.Size([32, 19, 14])\n",
            "batch.trg shape: torch.Size([32, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To4eVORkeW7m"
      },
      "source": [
        "***TODO: Download pretrained GloVe embedding***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEigy0CNeZLl"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "***Character Embedding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnVyXJRoMVWx"
      },
      "source": [
        "class CharacterEmbeddingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, char_vocab_size, char_embedding_size, token_embedding_size, kernel_size, pad_index):\n",
        "        super(CharacterEmbeddingLayer, self).__init__()\n",
        "        self.char_vocab_size = char_vocab_size\n",
        "        self.char_embedding_size = char_embedding_size\n",
        "        self.token_embedding_size = token_embedding_size\n",
        "        self.kernel_size = kernel_size\n",
        "        self.pad_index = pad_index\n",
        "        self.embedding = nn.Embedding(vocab_size, char_embedding_size, padding_idx=pad_index)\n",
        "        self.cond2d = nn.Conv2d(1, token_embedding_size, kernel_size=(char_embedding_size, kernel_size))\n",
        "    \n",
        "    def forward(self, char_sequences):\n",
        "        \"\"\"\n",
        "        :param LongTensor[batch_size, seq_len, char_len]\n",
        "        :return FloatTensor[batch_size, seq_len, token_embedding_size]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, char_len = char_sequences.size()\n",
        "        embedded = self.embedding(char_sequences) # [batch_size, seq_len, char_len, char_embedding_size]\n",
        "        embedded = embedded.transpose(-1, -2) # [batch_size, seq_len, char_embedding_size, char_len]\n",
        "        embedded = embedded.view(batch_size * seq_len, self.char_embedding_size, char_len) # [batch_size * seq_len, char_embedding_size, char_len]\n",
        "        embedded = embedded.unsqueeze(1) # [batch_size * seq_len, 1, char_embedding_size, char_len]\n",
        "        conved = self.cond2d(embedded) # [batch_size * seq_len, token_embedding_size, 1, char_len - kernel_size + 1]\n",
        "        # May be apply ReLU non-linearity???\n",
        "        conved = conved.squeeze(2) # [batch_size * seq_len, token_embedding_size, char_len - kernel_size + 1]\n",
        "        out = F.max_pool2d(conved, kernel_size=conved.size(-1)) # [batch_size * seq_len, token_embedding_size, 1]\n",
        "        out = out.squeeze(-1) # [batch_size * seq_len, token_embedding_size]\n",
        "        out = out.view(batch_size, seq_len, self.token_embedding_size) # [batch_size, seq_len, token_embedding_size]\n",
        "        # May be apply bias + tanh non linearity???\n",
        "        return out"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0RZx7dnc2VL"
      },
      "source": [
        "***Highway Network Layer***\n",
        "\n",
        "[Highway Networks paper](https://arxiv.org/pdf/1505.00387.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUEGI76ac0eS"
      },
      "source": [
        "class HighwayNetworkLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, n_layers):\n",
        "        super(HighwayNetworkLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.fc_flow = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(n_layers)])\n",
        "        self.fc_gate = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(n_layers)])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param FloatTensor[batch_size, seq_len, hidden_size] x\n",
        "        :return FloatTensor[batch_size, seq_len, hidden_size]\n",
        "        \"\"\"\n",
        "        for i in range(self.n_layers):\n",
        "            flow = F.relu(self.fc_flow[i](x))\n",
        "            gate = F.sigmoid(self.fc_gate[i](x))\n",
        "            x = gate * flow + (1 - gate) * x\n",
        "        return x"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmkM5YvfiCEY"
      },
      "source": [
        "***Contextual Embedding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WmZQqxaiAXv"
      },
      "source": [
        "class ContextualEmbeddingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, n_layers):\n",
        "        super(ContextualEmbeddingLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.high_network_layer = HighwayNetworkLayer(hidden_size=hidden_size, n_layers=n_layers)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, inputs, seq_len):\n",
        "        \"\"\"\n",
        "        :param FloatTensor[batch_size, seq_len, hidden_size] inputs\n",
        "        :param FloatTensor[batch_size, seq_len] seq_len\n",
        "        :return FloatTensor[batch_size, seq_len, hidden_size * 2]\n",
        "        \"\"\"\n",
        "        highway = self.high_network_layer(inputs) # [batch_size, seq_len, hidden_size]\n",
        "        packed = pack_padded_sequence(highway, seq_len, batch_first=True, enforce_sorted=False)\n",
        "        out_packed, _ = self.lstm(packed)\n",
        "        out_padded, _ = pad_packed_sequence(out_packed, batch_first=True) # [batch_size, seq_len, hidden_size * 2]\n",
        "        return out_padded"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdscRrrflg98"
      },
      "source": [
        "***Attention Flow Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiY3M145lTC1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}