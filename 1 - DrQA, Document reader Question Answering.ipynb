{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - DrQA, Document reader Question Answering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqjlHFCrrx0cuDmD5sa7i5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dksifoua/Question-Answering/blob/master/1%20-%20DrQA%2C%20Document%20reader%20Question%20Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMGpLvYNKR78",
        "outputId": "11833491-53a3-494d-ec4f-3388b07b6751",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Nov  1 00:16:14 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP-9tyjdylTg"
      },
      "source": [
        "## Load dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm6QVw_hzHB7"
      },
      "source": [
        "!pip install tqdm --upgrade >> /dev/null 2>&1\n",
        "!pip install spacy --upgrade >> /dev/null 2>&1\n",
        "!python -m spacy download en >> /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJQv4DSelwnB"
      },
      "source": [
        "import re\n",
        "import tqdm\n",
        "import json\n",
        "import spacy\n",
        "import string\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw16-fZCDHHu",
        "outputId": "bf7f27d5-38cb-414b-c9b3-e8b8db323cf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "SEED = 546\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx9pL3Hiyn7c"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "***Download data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBy3v4Pke7dq",
        "outputId": "4016d958-c705-48e6-e37a-11a86700476d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm -rf ./data\n",
        "!mkdir ./data\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json \\\n",
        "    -O ./data/train-v1.1.json\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json \\\n",
        "    -O ./data/dev-v1.1.json"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-01 00:18:50--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30288272 (29M) [application/json]\n",
            "Saving to: ‘./data/train-v1.1.json’\n",
            "\n",
            "./data/train-v1.1.j 100%[===================>]  28.88M  73.3MB/s    in 0.4s    \n",
            "\n",
            "2020-11-01 00:18:52 (73.3 MB/s) - ‘./data/train-v1.1.json’ saved [30288272/30288272]\n",
            "\n",
            "--2020-11-01 00:18:52--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4854279 (4.6M) [application/json]\n",
            "Saving to: ‘./data/dev-v1.1.json’\n",
            "\n",
            "./data/dev-v1.1.jso 100%[===================>]   4.63M  20.5MB/s    in 0.2s    \n",
            "\n",
            "2020-11-01 00:18:53 (20.5 MB/s) - ‘./data/dev-v1.1.json’ saved [4854279/4854279]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9H59xUnyvAR"
      },
      "source": [
        "***Load JSON data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbZQjhA5roqo"
      },
      "source": [
        "def load(path):\n",
        "    with open(path, mode='r', encoding='utf-8') as file:\n",
        "        return json.load(file)['data']\n",
        "    raise FileNotFoundError"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqjrkTxHr3rA",
        "outputId": "22ef1aab-1489-4cfe-a6c3-8e55b943c1e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_raw_data = load('./data/train-v1.1.json')\n",
        "valid_raw_data = load('./data/dev-v1.1.json')\n",
        "print(f'Length of raw train data: {len(train_raw_data):,}')\n",
        "print(f'Length of raw valid data: {len(valid_raw_data):,}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of raw train data: 442\n",
            "Length of raw valid data: 48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4sypWlVyxx-"
      },
      "source": [
        "***Parse JSON data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-6GE3YxsQsl"
      },
      "source": [
        "def parse(data, nlp=spacy.load('en')):\n",
        "    qas = []\n",
        "    for paragraphs in tqdm.tqdm(data):\n",
        "        for para in paragraphs['paragraphs']:\n",
        "            context = nlp(para['context'], disable=['parser'])\n",
        "            for qa in para['qas']:\n",
        "                id = qa['id']\n",
        "                question = nlp(qa['question'], disable=['parser', 'tagger', 'ner'])\n",
        "                for ans in qa['answers']:\n",
        "                    qas.append({\n",
        "                        'id': id,\n",
        "                        'context': context,\n",
        "                        'question': question,\n",
        "                        'answer': nlp(ans['text'], disable=['parser', 'tagger', 'ner']),\n",
        "                        'answer_start': ans['answer_start'],\n",
        "                    })\n",
        "    return qas"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MeqBr7Ov9ZU",
        "outputId": "6c6c28ef-ea15-42f4-8198-b70e15f10954",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_qas = parse(train_raw_data)\n",
        "valid_qas = parse(valid_raw_data)\n",
        "print()\n",
        "print(f'Length of train qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs: {len(valid_qas):,}')\n",
        "print('==================== Example ====================')\n",
        "print('Id:', train_qas[0]['id'])\n",
        "print('Context:', train_qas[0]['context'])\n",
        "print('Question:', train_qas[0]['question'])\n",
        "print('Answer starts at:', train_qas[0]['answer_start'])\n",
        "print('Answer:', train_qas[0]['answer'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 442/442 [04:53<00:00,  1.51it/s]\n",
            "100%|██████████| 48/48 [00:34<00:00,  1.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Length of train qa pairs: 87,599\n",
            "Length of valid qa pairs: 34,726\n",
            "==================== Example ====================\n",
            "Id: 5733be284776f41900661182\n",
            "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Answer starts at: 515\n",
            "Answer: Saint Bernadette Soubirous\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bafKgiFv1GHa"
      },
      "source": [
        "def test_answer_start(qas):\n",
        "    \"\"\"Test answer_start are correct in train set\"\"\"\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        answer = qa['answer'].text\n",
        "        context = qa['context'].text\n",
        "        answer_start = qa['answer_start']\n",
        "        assert answer == context[answer_start:answer_start + len(answer)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tTDTQod23-g",
        "outputId": "14edc2f3-3969-4e54-bd18-af6554ae2d2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_answer_start(train_qas)\n",
        "test_answer_start(valid_qas)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 87599/87599 [00:08<00:00, 10567.86it/s]\n",
            "100%|██████████| 34726/34726 [00:03<00:00, 11420.74it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxhCfiNY28zs"
      },
      "source": [
        "***Add targets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4bpqV3S27SQ"
      },
      "source": [
        "def add_targets(qas):\n",
        "    \"\"\"Add start and end index token\"\"\"\n",
        "    for qa in qas:\n",
        "        context = qa['context']\n",
        "        answer = qa['answer']\n",
        "        ans_start = qa['answer_start']\n",
        "        for i in range(len(context)):\n",
        "            if context[i].idx == ans_start:\n",
        "                ans = context[i:i + len(answer)]\n",
        "                qa['target'] = [ans[0].i, ans[-1].i]\n",
        "                break"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waJedA674CCH",
        "outputId": "76a61f04-1d7d-41d0-f7ea-91e4746686c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "add_targets(train_qas)\n",
        "add_targets(valid_qas)\n",
        "print(f'Length of train qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs: {len(valid_qas):,}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs: 87,599\n",
            "Length of valid qa pairs: 34,726\n",
            "CPU times: user 1.52 s, sys: 16 ms, total: 1.53 s\n",
            "Wall time: 1.54 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT65eePm4F9w"
      },
      "source": [
        "def filter_qas(qa):\n",
        "    \"\"\"Remove bad targets\"\"\"\n",
        "    if 'target' in [*qa.keys()]:\n",
        "        start, end = qa['target']\n",
        "        return qa['context'][start:end + 1].text == qa['answer'].text\n",
        "    return False"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_nUlM_n4Mgc",
        "outputId": "0006ad2e-922e-473a-ebb3-eaa0ef9333f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "train_qas = [*filter(filter_qas, train_qas)]\n",
        "valid_qas = [*filter(filter_qas, valid_qas)]\n",
        "print(f'Length of train qa pairs after filtering out bad qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs after filtering out bad qa pairs: {len(valid_qas):,}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs after filtering out bad qa pairs: 86,597\n",
            "Length of valid qa pairs after filtering out bad qa pairs: 34,295\n",
            "CPU times: user 1.15 s, sys: 4.99 ms, total: 1.16 s\n",
            "Wall time: 1.16 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKEhn4eI4P5B"
      },
      "source": [
        "def test_targets(qas):\n",
        "    for qa in qas:\n",
        "        if 'target' in [*qa.keys()]:\n",
        "            start, end = qa['target']\n",
        "            assert qa['context'][start:end + 1].text == qa['answer'].text"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O_fmDJl5R7V",
        "outputId": "f5543b24-3aa4-4a6f-95d2-ccf2cc08acff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "test_targets(train_qas)\n",
        "test_targets(valid_qas)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.12 s, sys: 980 µs, total: 1.12 s\n",
            "Wall time: 1.13 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkLppZOP5Vju"
      },
      "source": [
        "***Add features***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8Y038pt5VIc"
      },
      "source": [
        "def add_features(qas):\n",
        "    \"\"\"Add extra features: Exact Match, Part-of-Speech, Name Entity Recognition & Normalized Term Frequency\"\"\"\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        question = [token.text for token in qa['question']]\n",
        "        context = qa['context']\n",
        "        counts = collections.Counter(map(lambda token: token.text.lower(), context))\n",
        "        freqs = {index: counts[token.text.lower()] for index, token in enumerate(context)}\n",
        "        freqs_norm = sum(freqs.values())\n",
        "        qa['em'], qa['pos'], qa['ner'], qa['ntf'] = zip(\n",
        "            *map(lambda index: [\n",
        "                context[index].text in question, context[index].tag_,\n",
        "                context[index].ent_type_ or 'None',\n",
        "                freqs[index] / freqs_norm\n",
        "            ], range(len(context)))\n",
        "        )"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqwcbDgPSmp8",
        "outputId": "0987df46-749c-4d97-e4ed-acc1d8860582",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "add_features(train_qas)\n",
        "add_features(valid_qas)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 86597/86597 [00:51<00:00, 1668.80it/s]\n",
            "100%|██████████| 34295/34295 [00:21<00:00, 1624.91it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8SrKXlfoU5y"
      },
      "source": [
        "***Build vocabularies***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LYjZhu7m6YI"
      },
      "source": [
        "class Vocab:\n",
        "\n",
        "    def __init__(self, pad_token, unk_token):\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.vocab = None\n",
        "        self.word2count = None\n",
        "        self.word2index = None\n",
        "        self.index2word = None\n",
        "    \n",
        "    def build(self, data, min_freq):\n",
        "        \"\"\"\n",
        "        :param List[Union[spacy.tokens.doc.Doc, str, Tuple]] data\n",
        "        :param int min_freq\n",
        "        \"\"\"\n",
        "        words = [self.pad_token, self.unk_token]\n",
        "        type_0 = type(data[0])\n",
        "        if type_0 == spacy.tokens.doc.Doc:\n",
        "            for item in data: # context and question\n",
        "                words += [word.text.lower() for word in item]\n",
        "        elif type_0 == str: # id\n",
        "            words += data\n",
        "        elif type_0 == tuple: # pos and ner\n",
        "            for item in data:\n",
        "                words += [word.lower() for word in item]\n",
        "        self.word2count = collections.Counter(words)\n",
        "        self.vocab = sorted(filter(\n",
        "            lambda word: self.word2count[word] >= min_freq or word == self.pad_token or word == self.unk_token, self.word2count\n",
        "        ))\n",
        "        self.word2index = {word: index for index, word in enumerate(self.vocab)}\n",
        "        self.index2word = {index: word for index, word in enumerate(self.vocab)}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "    \n",
        "    def stoi(self, word):\n",
        "        return self.word2index.get(str(word), self.word2index[self.unk_token])\n",
        "\n",
        "    def itos(self, index):\n",
        "        return self.index2word[index]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2RP28pUqYoQ",
        "outputId": "4d6da430-4e61-4139-8324-196c2be1aeb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "PAD_TOKEN = '<pad>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "\n",
        "ID = Vocab(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN)\n",
        "POS = Vocab(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN)\n",
        "NER = Vocab(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN)\n",
        "TEXT = Vocab(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN)\n",
        "\n",
        "ids = [*map(lambda qa: qa['id'], train_qas)] + [*map(lambda qa: qa['id'], valid_qas)]\n",
        "pos, ner, contexts, questions = zip(*map(lambda qa: (qa['pos'], qa['ner'], qa['context'], qa['question']), train_qas))\n",
        "\n",
        "ID.build(data=[*set(ids)], min_freq=0)\n",
        "POS.build(data=[*set(pos)], min_freq=0)\n",
        "NER.build(data=[*set(ner)], min_freq=0)\n",
        "TEXT.build(data=[*set(contexts)] + [*set(questions)], min_freq=5)\n",
        "\n",
        "print(f'Length of ID vocabulary: {len(ID):,}')\n",
        "print(f'Length of POS vocabulary: {len(POS):,}')\n",
        "print(f'Length of NER vocabulary: {len(NER):,}')\n",
        "print(f'Length of TEXT vocabulary: {len(TEXT):,}')"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of ID vocabulary: 97,108\n",
            "Length of POS vocabulary: 52\n",
            "Length of NER vocabulary: 21\n",
            "Length of TEXT vocabulary: 26,885\n",
            "CPU times: user 5.61 s, sys: 12.6 ms, total: 5.63 s\n",
            "Wall time: 5.63 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t26xjG9OAikb"
      },
      "source": [
        "***Build datasets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iotg02zlAflQ"
      },
      "source": [
        "class SQuADV1Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, id_vocab, pos_vocab, ner_vocab, text_vocab):\n",
        "        self.data = data\n",
        "        self.id_vocab = id_vocab\n",
        "        self.pos_vocab = pos_vocab\n",
        "        self.ner_vocab = ner_vocab\n",
        "        self.text_vocab = text_vocab\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        id = torch.LongTensor([self.id_vocab.stoi(item['id'])])\n",
        "        ctx = torch.LongTensor([*map(lambda token: self.text_vocab.stoi(token.text.lower()), item['context'])])\n",
        "        qst = torch.LongTensor([*map(lambda token: self.text_vocab.stoi(token.text.lower()), item['question'])])\n",
        "        trg = torch.LongTensor(item['target'])\n",
        "        em = torch.LongTensor(item['em'])\n",
        "        pos = torch.LongTensor([*map(lambda token: self.pos_vocab.stoi(token.lower()), item['pos'])])\n",
        "        ner = torch.LongTensor([*map(lambda token: self.ner_vocab.stoi(token.lower()), item['ner'])])\n",
        "        ntf = torch.FloatTensor(item['ntf'])\n",
        "        return id, ctx, qst, trg, em, pos, ner, ntf"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMgSisEt8ARe",
        "outputId": "3abef599-760f-43ae-f70d-2a4c525c2de4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataset = SQuADV1Dataset(data=train_qas, id_vocab=ID, pos_vocab=POS, ner_vocab=NER, text_vocab=TEXT)\n",
        "valid_dataset = SQuADV1Dataset(data=valid_qas, id_vocab=ID, pos_vocab=POS, ner_vocab=NER, text_vocab=TEXT)\n",
        "\n",
        "id, ctx, qst, trg, em, pos, ner, ntf = train_dataset[0]\n",
        "print(f'id shape: {id.shape}')\n",
        "print(f'ctx shape: {ctx.shape}')\n",
        "print(f'qst shape: {qst.shape}')\n",
        "print(f'trg shape: {trg.shape}')\n",
        "print(f'em shape: {em.shape}')\n",
        "print(f'pos shape: {pos.shape}')\n",
        "print(f'ner shape: {ner.shape}')\n",
        "print(f'ntf shape: {ntf.shape}')"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id shape: torch.Size([1])\n",
            "ctx shape: torch.Size([142])\n",
            "qst shape: torch.Size([14])\n",
            "trg shape: torch.Size([2])\n",
            "em shape: torch.Size([142])\n",
            "pos shape: torch.Size([142])\n",
            "ner shape: torch.Size([142])\n",
            "ntf shape: torch.Size([142])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZQaVaHt_MDc"
      },
      "source": [
        "***Build data loaders***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPKrJOpKTlN1"
      },
      "source": [
        "class DotDict(dict):\n",
        "    __getattr__ = dict.get"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYMYz3-SBYzf"
      },
      "source": [
        "def add_padding(batch, pad_token=PAD_TOKEN, text_vocab=TEXT, pos_vocab=POS, ner_vocab=NER, include_lengths=True, device=DEVICE):\n",
        "    \"\"\"Pad batch of sequence with different lengths\"\"\"\n",
        "    batch_id, batch_ctx, batch_qst, batch_trg, batch_em, batch_pos, batch_ner, batch_ntf = zip(*batch)\n",
        "    if include_lengths:\n",
        "        len_ctx = torch.LongTensor([ctx.size(0) for ctx in batch_ctx]).to(device)\n",
        "        len_qst = torch.LongTensor([qst.size(0) for qst in batch_qst]).to(device)\n",
        "    batch_padded_id = pad_sequence(batch_id, batch_first=True).to(device)\n",
        "    batch_padded_ctx = pad_sequence(batch_ctx, batch_first=True, padding_value=text_vocab.stoi(pad_token)).to(device)\n",
        "    batch_padded_qst = pad_sequence(batch_qst, batch_first=True, padding_value=text_vocab.stoi(pad_token)).to(device)\n",
        "    batch_padded_trg = pad_sequence(batch_trg, batch_first=True).to(device)\n",
        "    batch_padded_em = pad_sequence(batch_em, batch_first=True).to(device)\n",
        "    batch_padded_pos = pad_sequence(batch_pos, batch_first=True, padding_value=pos_vocab.stoi(pad_token)).to(device)\n",
        "    batch_padded_ner = pad_sequence(batch_ner, batch_first=True, padding_value=ner_vocab.stoi(pad_token)).to(device)\n",
        "    batch_padded_ntf = pad_sequence(batch_ntf, batch_first=True).to(device)\n",
        "    return DotDict({\n",
        "        'id': batch_padded_id,\n",
        "        'ctx': (batch_padded_ctx, len_ctx) if include_lengths else batch_padded_ctx,\n",
        "        'qst': (batch_padded_qst, len_qst) if include_lengths else batch_padded_qst,\n",
        "        'trg': batch_padded_trg,\n",
        "        'em': batch_padded_em,\n",
        "        'pos': batch_padded_pos,\n",
        "        'ner': batch_padded_ner,\n",
        "        'ntf': batch_padded_ntf,\n",
        "    })"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu2ovPqxA7Ix",
        "outputId": "3f94c073-2df0-4d9d-f3ea-6b5f8560d60e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=add_padding)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=add_padding)\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    print('batch.id shape:', batch.id.shape)\n",
        "    print('batch.ctx shape:', batch.ctx[0].shape, batch.ctx[1].shape)\n",
        "    print('batch.qst shape:', batch.qst[0].shape, batch.qst[1].shape)\n",
        "    print('batch.trg shape:', batch.trg.shape)\n",
        "    print('batch.em shape:', batch.em.shape)\n",
        "    print('batch.pos shape:', batch.pos.shape)\n",
        "    print('batch.ner shape:', batch.ner.shape)\n",
        "    print('batch.ntf shape:', batch.ntf.shape)\n",
        "    break"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch.id shape: torch.Size([64, 1])\n",
            "batch.ctx shape: torch.Size([64, 253]) torch.Size([64])\n",
            "batch.qst shape: torch.Size([64, 19]) torch.Size([64])\n",
            "batch.trg shape: torch.Size([64, 2])\n",
            "batch.em shape: torch.Size([64, 253])\n",
            "batch.pos shape: torch.Size([64, 253])\n",
            "batch.ner shape: torch.Size([64, 253])\n",
            "batch.ntf shape: torch.Size([64, 253])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2O79r_CLvGu"
      },
      "source": [
        "***TODO: Download pretrained GloVe embedding***\n",
        "\n",
        "It will take about 16 minutes to download from Colab!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxnYdks-L31E"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "***Stacked Bidirectional LSTM Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoHYvqQQCEGw"
      },
      "source": [
        "class StackedBiLSTMsLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_size, hidden_size, n_layers, dropout):\n",
        "        super(StackedBiLSTMsLayer, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.lstms = nn.ModuleList([nn.LSTM(embedding_size if i == 0 else hidden_size * 2, hidden_size,\n",
        "                                            batch_first=True, num_layers=n_layers, bidirectional=True)\n",
        "                                    for i in range(n_layers)])\n",
        "    \n",
        "    def apply_lstm(self, layer, inputs, lengths):\n",
        "        \"\"\"\n",
        "        :param nn.LSTM layer\n",
        "        :param FloatTensor[batch_size, seq_len, embedding_size | hidden_size * 2] inputs\n",
        "        :param LongTensor[batch_size, seq_len] lengths\n",
        "        :return FloatTensor[batch_size, seq_len, hidden_size * 2] out_padded\n",
        "        \"\"\"\n",
        "        inputs = self.dropout(inputs)\n",
        "        packed = pack_padded_sequence(inputs, lengths, batch_first=True, enforce_sorted=False)\n",
        "        out_packed, _ = layer(packed)\n",
        "        out_padded, out_lengths = pad_packed_sequence(out_packed, batch_first=True) # [batch_size, seq_len, hidden_size * 2]\n",
        "        return out_padded, out_lengths\n",
        "    \n",
        "    def forward(self, input_embedded, sequence_lengths):\n",
        "        \"\"\"\n",
        "        :param FloatTensor[batch_size, seq_len, embedding_size] input_embedded\n",
        "        :param LongTensor[batch_size, seq_len] sequence_lengths\n",
        "        :return FloatTensor[batch_size, seq_len, hidden_size * n_layers * 2]\n",
        "        \"\"\"\n",
        "        outputs, lens = [input_embedded], sequence_lengths\n",
        "        for lstm in self.lstms:\n",
        "            out, lens = self.apply_lstm(layer=lstm, inputs=outputs[-1], lengths=lens)\n",
        "            outputs.append(out)\n",
        "        return self.dropout(torch.cat(outputs[1:], dim=-1))"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oazq30eW3_T"
      },
      "source": [
        "***Aligned Question Embedding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATOQC6-pWzq_"
      },
      "source": [
        "class AlignQuestionEmbeddingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AlignQuestionEmbeddingLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, ctx_embed, qst_embed, qst_mask):\n",
        "        \"\"\"\n",
        "        :param FloatTensor[batch_size, ctx_len, embedding_size] ctx_embed\n",
        "        :param FloatTensor[batch_size, qst_len, embedding_size] qst_embed\n",
        "        :param IntTensor[batch_size, qst_len] qst_mask\n",
        "        :return FloatTensor[batch_size, ctx_len, hidden_size]\n",
        "        \"\"\"\n",
        "        ctx_embed = F.relu(self.linear(ctx_embed)) # [batch_size, ctx_len, hidden_size]\n",
        "        qst_embed = F.relu(self.linear(qst_embed)) # [batch_size, qst_len, hidden_size]\n",
        "        scores = torch.bmm(ctx_embed, qst_embed.transpose(-1, -2)) # [batch_size, ctx_len, qst_len]\n",
        "        scores = scores.masked_fill(qst_mask.unsqueeze(1) == 0, 1e-18)\n",
        "        attention_weights = F.softmax(scores, dim=-1) # [batch_size, ctx_len, qst_len]\n",
        "        return torch.bmm(attention_weights, qst_embed)"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImX8ayztYjsK"
      },
      "source": [
        "***Question Encoding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whil-FgVYhgy"
      },
      "source": [
        "class QuestionEncodingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_size, hidden_size, dropout, n_layers):\n",
        "        super(QuestionEncodingLayer, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.stacked_bilstms_layer = StackedBiLSTMsLayer(embedding_size=embedding_size, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
        "        self.linear = nn.Linear(embedding_size, 1)\n",
        "    \n",
        "    def linear_self_attention(self, qst_embed, qst_mask):\n",
        "        \"\"\"\n",
        "        :param FloatTensor[batch_size, qst_len, embedding_size] qst_embed\n",
        "        :param IntTensor[batch_size, qst_len] qst_mask\n",
        "        :return FloatTensor[batch_size, qst_len]\n",
        "        \"\"\"\n",
        "        scores = self.linear(qst_embed).squeeze(-1) # [batch_size, qst_len]\n",
        "        scores = scores.masked_fill(qst_mask == 0, 1e-18)\n",
        "        return F.softmax(scores, dim=-1)\n",
        "\n",
        "    \n",
        "    def forward(self, qst_embed, qst_lengths, qst_mask):\n",
        "        \"\"\"\n",
        "        :param FloatTensor[batch_size, qst_len, embedding_size] qst_embed\n",
        "        :param IntTensor[batch_size, qst_len] qst_lengths\n",
        "        :param IntTensor[batch_size, qst_len] qst_mask\n",
        "        :return FloatTensor[batch_size, hidden_size * n_layers * 2]\n",
        "        \"\"\"\n",
        "        attention_weights = self.linear_self_attention(qst_embed=qst_embed, qst_mask=qst_mask) # [batch_size, qst_len]\n",
        "        lstm_outputs = self.stacked_bilstms_layer(input_embedded=qst_embed, sequence_lengths=qst_lengths)\n",
        "        # lstm_outputs: [batch_size, qst_len, hidden_size * n_layers * 2]\n",
        "        return torch.bmm(attention_weights.unsqueeze(1), lstm_outputs).squeeze(1)"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hFIZnmUaFAu"
      },
      "source": [
        "***BiLinear Attention Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZGOH3IhaEZw"
      },
      "source": [
        "class BiLinearAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, ctx_size, qst_size):\n",
        "        super(BiLinearAttentionLayer, self).__init__()\n",
        "        self.ctx_size = ctx_size\n",
        "        self.qst_size = qst_size\n",
        "        self.linear = nn.Linear(qst_size, ctx_size)\n",
        "    \n",
        "    def forward(self, ctx_encoded, qst_encoded, ctx_mask):\n",
        "        \"\"\"\n",
        "        :param FloatTensor[batch_size, ctx_len, ctx_size] ctx_encoded\n",
        "        :param FloatTensor[batch_size, qst_size] qst_encoded\n",
        "        :param IntTensor[batch_size, ctx_len] ctx_mask\n",
        "        :return FloatTensor[batch_size, ctx_len, hidden_size]\n",
        "        \"\"\"\n",
        "        qst_encoded = self.linear(qst_encoded) # [batch_size, ctx_size]\n",
        "        scores = torch.bmm(ctx_encoded, qst_encoded.unsqueeze(-1)) # [batch_size, ctx_len, 1]\n",
        "        scores = scores.squeeze(-1).masked_fill(ctx_mask == 0, 1e-18) # [batch_size, ctx_len]\n",
        "        return scores"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzrmk6D7agHH"
      },
      "source": [
        "***Document reader Question Answering Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlZlTFXOae_o"
      },
      "source": [
        "class DrQA(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size, n_extra_features, hidden_size, n_layers, dropout, pad_index):\n",
        "        super(DrQA, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_extra_features = n_extra_features\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pad_index = pad_index\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_index)\n",
        "        self.align_question_embedding_layer = AlignQuestionEmbeddingLayer(hidden_size=embedding_size)\n",
        "        self.ctx_stacked_bi_lstm_layer = StackedBiLSTMsLayer(embedding_size=embedding_size * 2 + n_extra_features,\n",
        "                                                             hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
        "        self.qst_encoding_layer = QuestionEncodingLayer(embedding_size=embedding_size, hidden_size=hidden_size, dropout=dropout, n_layers=n_layers)\n",
        "        self.bilinear_attention_layer_start = BiLinearAttentionLayer(ctx_size=hidden_size * n_layers * 2, qst_size=hidden_size * n_layers * 2)\n",
        "        self.bilinear_attention_layer_end = BiLinearAttentionLayer(ctx_size=hidden_size * n_layers * 2, qst_size=hidden_size * n_layers * 2)\n",
        "    \n",
        "    def make_ctx_mask(self, ctx_sequences):\n",
        "        \"\"\"\n",
        "        :param LongTensor[batch_size, ctx_len] ctx_sequences\n",
        "        :return IntTensor[batch_size, ctx_len]\n",
        "        \"\"\"\n",
        "        return ctx_sequences != self.pad_index\n",
        "    \n",
        "    def make_qst_mask(self, qst_sequences):\n",
        "        \"\"\"\n",
        "        :param LongTensor[batch_size, qst_len] qst_sequences\n",
        "        :return IntTensor[batch_size, qst_len]\n",
        "        \"\"\"\n",
        "        return qst_sequences != self.pad_index\n",
        "    \n",
        "    @staticmethod\n",
        "    def decode(starts, ends):\n",
        "        \"\"\"\n",
        "        :param IntTensor[batch_size, ctx_len] starts\n",
        "        :param IntTensor[batch_size, ctx_len] ends\n",
        "        :return list(int) start_indexes\n",
        "        :return list(int) end_indexes\n",
        "        :return list(float) pred_probas\n",
        "        \"\"\"\n",
        "        start_indexes, end_indexes, pred_probas = [], [], []\n",
        "        for i in range(starts.size(0)):\n",
        "            probas = torch.ger(starts[i], ends[i]) # [ctx_len, ctx_len]\n",
        "            proba, index = torch.topk(probas.view(-1), k=1)\n",
        "            start_indexes.append(index.tolist()[0] // probas.size(0))\n",
        "            end_indexes.append(index.tolist()[0] % probas.size(1))\n",
        "            pred_probas.append(proba.tolist()[0])\n",
        "        return start_indexes, end_indexes, pred_probas\n",
        "    \n",
        "    def forward(self, ctx_sequences, ctx_lengths, qst_sequences, qst_lengths, em_sequences, pos_sequences, ner_sequences, ntf_sequences):\n",
        "        \"\"\"\n",
        "        :param LongTensor[batch_size, ctx_len] ctx_sequences\n",
        "        :param Tensor[batch_size,] ctx_lengths\n",
        "        :param LongTensor[batch_size, qst_len] qst_sequences\n",
        "        :param Tensor[batch_size,] qst_lengths\n",
        "        :param LongTensor[batch_size, ctx_len] em_sequences\n",
        "        :param LongTensor[batch_size, ctx_len] pos_sequences\n",
        "        :param LongTensor[batch_size, ctx_len] ner_sequences\n",
        "        :param LongTensor[batch_size, ctx_len] ntf_sequences\n",
        "        :return Tensor[batch_size, ctx_len] starts\n",
        "        :return Tensor[batch_size, ctx_len] ends\n",
        "        \"\"\"\n",
        "        ctx_mask = self.make_ctx_mask(ctx_sequences) # [batch_size, ctx_len]\n",
        "        qst_mask = self.make_qst_mask(qst_sequences) # [batch_size, qst_len]\n",
        "        ctx_embedded = self.dropout(self.embedding(ctx_sequences)) # [batch_size, ctx_len, embedding_size]\n",
        "        qst_embedded = self.dropout(self.embedding(qst_sequences)) # [batch_size, ctx_len, embedding_size]\n",
        "        ctx_aligned = self.align_question_embedding_layer(ctx_embed=ctx_embedded, qst_embed=qst_embedded,\n",
        "                                                          qst_mask=qst_mask) # [batch_size, ctx_len, embedding_size]\n",
        "        ctx_inputs = torch.cat([ctx_embedded, em_sequences.unsqueeze(-1), pos_sequences.unsqueeze(-1), ner_sequences.unsqueeze(-1),\n",
        "                                ntf_sequences.unsqueeze(-1), ctx_aligned], dim=-1) # [batch_size, ctx_len, embedding_size * 2 + 4]\n",
        "        ctx_encoded = self.ctx_stacked_bi_lstm_layer(input_embedded=ctx_inputs, sequence_lengths=ctx_lengths)\n",
        "        # ctx_encoded: [batch_size, ctx_len, hidden_size * n_layers * 2]\n",
        "        qst_encoded = self.qst_encoding_layer(qst_embed=qst_embedded, qst_lengths=qst_lengths, qst_mask=qst_mask)\n",
        "        # qst_encoded: [batch_size, hidden_size * n_layers * 2]\n",
        "        starts = self.bilinear_attention_layer_start(ctx_encoded=ctx_encoded, qst_encoded=qst_encoded, ctx_mask=ctx_mask)\n",
        "        ends = self.bilinear_attention_layer_end(ctx_encoded=ctx_encoded, qst_encoded=qst_encoded, ctx_mask=ctx_mask)\n",
        "        return starts, ends"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x575iXlz3w4m"
      },
      "source": [
        "***Training routines***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31k5BZf43uV4"
      },
      "source": [
        "class AverageMeter:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.value = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "        self.average = 0.\n",
        "        \n",
        "    def reset(self):\n",
        "        self.value = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "        self.average = 0.\n",
        "        \n",
        "    def update(self, value, n=1):\n",
        "        self.value = value\n",
        "        self.sum += value * n\n",
        "        self.count += n\n",
        "        self.average = self.sum / self.count"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSmh2GmU4KXH"
      },
      "source": [
        "def normalize(answer: str):\n",
        "    \"\"\"Performs a series of cleaning steps on the ground truth and predicted answer.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(answer))))"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi2JHYTX6F6A"
      },
      "source": [
        "def get_scores(prediction: str, ground_truth: str):\n",
        "    prediction, ground_truth = normalize(prediction), normalize(ground_truth)\n",
        "    em_score = prediction == ground_truth\n",
        "\n",
        "    prediction_tokens, ground_truth_tokens = prediction.split(), ground_truth.split()\n",
        "    common = collections.Counter(prediction_tokens) & collections.Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        f1_score = 0\n",
        "    else:\n",
        "        precision = 1.0 * num_same / len(prediction_tokens)\n",
        "        recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "        f1_score = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return em_score, f1_score"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv9kxaI34_Oo"
      },
      "source": [
        "def max_metrics_over_ground_truths(prediction: str, ground_truths: list):\n",
        "    scores = [get_scores(prediction, ground_truth) for ground_truth in ground_truths]\n",
        "    em_score = max(scores, key=lambda score: score[0])[0]\n",
        "    f1_score = max(scores, key=lambda score: score[1])[1]\n",
        "    return em_score, f1_score"
      ],
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmxMG01B5JEx"
      },
      "source": [
        "def metrics(predictions: dict, qas=valid_qas):\n",
        "    ground_truths = collections.defaultdict(lambda: [])\n",
        "    for qa in qas:\n",
        "        if qa['id'] in predictions:\n",
        "            ground_truths[qa['id']].append(qa['answer'].text)\n",
        "\n",
        "    em_scores, f1_scores, total = [], [], 0\n",
        "    for id in predictions:\n",
        "        em_score, f1_score = max_metrics_over_ground_truths(predictions[id], ground_truths[id])\n",
        "        em_scores.append(em_score); f1_scores.append(f1_score)\n",
        "        total += 1\n",
        "\n",
        "    em_score = 100.0 * sum(em_scores) / total\n",
        "    f1_score = 100.0 * sum(f1_scores) / total\n",
        "    return em_score, f1_score"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHUdYB6f-NRJ"
      },
      "source": [
        "class Trainer:\n",
        "    \n",
        "    def __init__(self, model, optimizer, criterion, id_field, text_field):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.id_field = id_field\n",
        "        self.text_field = text_field\n",
        "        \n",
        "    def train_step(self, loader, epoch, grad_clip):\n",
        "        loss_tracker = AverageMeter()\n",
        "        self.model.train()\n",
        "        progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "        for i, batch in progress_bar:\n",
        "            self.optimizer.zero_grad()\n",
        "            starts, ends = self.model(*batch.ctx, *batch.qst, batch.em, batch.pos, batch.ner, batch.ntf) # [batch_size, ctx_len]\n",
        "            loss = self.criterion(starts, batch.trg[:, 0]) + self.criterion(ends, batch.trg[:, 1])\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
        "            self.optimizer.step()\n",
        "            loss_tracker.update(loss.item())\n",
        "            progress_bar.set_description(f'Epoch: {epoch+1:02d} -     loss: {loss_tracker.average:.3f}')\n",
        "        return loss_tracker.average\n",
        "    \n",
        "    def validate(self, loader, epoch):\n",
        "        loss_tracker, predictions = AverageMeter(), {}\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "            for i, batch in progress_bar:\n",
        "                starts, ends = self.model(*batch.ctx, *batch.qst, batch.em, batch.pos, batch.ner, batch.ntf) # [batch_size, ctx_len]\n",
        "                loss = self.criterion(starts, batch.trg[:, 0]) + self.criterion(ends, batch.trg[:, 1])\n",
        "                start_indexes, end_indexes, _ = DrQA.decode(starts=F.softmax(starts, dim=-1), ends=F.softmax(ends, dim=-1))\n",
        "                for i in range(starts.size(0)):\n",
        "                    id = self.id_field.itos(batch.id[i].item())\n",
        "                    prediction = batch.ctx[0][i][start_indexes[i]:end_indexes[i]+1]\n",
        "                    predictions[id] = ' '.join([self.text_field.itos(indice.item()) for indice in prediction])\n",
        "                loss_tracker.update(loss.item())\n",
        "                progress_bar.set_description(f'Epoch: {epoch+1:02d} - val_loss: {loss_tracker.average:.3f}')\n",
        "        return loss_tracker.average, predictions\n",
        "    \n",
        "    def train(self, train_loader, valid_loader, n_epochs, grad_clip):\n",
        "        history, best_loss = {'loss': [], 'val_loss': [], 'em': [], 'f1': []}, float('inf')\n",
        "        for epoch in range(n_epochs):\n",
        "            loss = self.train_step(train_loader, epoch, grad_clip)\n",
        "            val_loss, predictions = self.validate(valid_loader, epoch)\n",
        "            em_score, f1_score = metrics(predictions)\n",
        "            print()\n",
        "            print(f'EM={em_score:.3f}% - F1={f1_score:.3f}%')\n",
        "            history['loss'].append(loss); history['val_loss'].append(val_loss)\n",
        "            history['em'].append(em_score); history['f1'].append(f1_score)\n",
        "            if best_loss > val_loss:\n",
        "                best_loss = val_loss\n",
        "                torch.save(self.model.state_dict(), './checkpoints/DrQA.pth')\n",
        "        return history"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFzM-vWUAov9"
      },
      "source": [
        "***Train the model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YCmJQLhAlqo"
      },
      "source": [
        "N_LAYERS = 1\n",
        "EMBED_SIZE = 16\n",
        "HIDDEN_SIZE = 16\n",
        "DROPOUT = 0.25\n",
        "N_EPOCHS = 5\n",
        "LR = 1e-3\n",
        "GRAD_CLIP = 1.0"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ersGVmFxBwAV",
        "outputId": "9b6e08b0-2059-42ae-8728-f16f1b333595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "drqa = DrQA(vocab_size=len(TEXT),\n",
        "            embedding_size=EMBED_SIZE,\n",
        "            n_extra_features=4,\n",
        "            hidden_size=HIDDEN_SIZE,\n",
        "            n_layers=N_LAYERS,\n",
        "            dropout=DROPOUT,\n",
        "            pad_index=TEXT.stoi(PAD_TOKEN))\n",
        "drqa.to(DEVICE)\n",
        "optimizer = optim.RMSprop(params=drqa.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TEXT.stoi(PAD_TOKEN))\n",
        "print(f'Number of parameters of the model: {sum(p.numel() for p in drqa.parameters() if p.requires_grad):,}')\n",
        "print(drqa)\n",
        "trainer = Trainer(model=drqa, optimizer=optimizer, criterion=criterion, id_field=ID, text_field=TEXT)"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters of the model: 443,825\n",
            "DrQA(\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (embedding): Embedding(26885, 16, padding_idx=1318)\n",
            "  (align_question_embedding_layer): AlignQuestionEmbeddingLayer(\n",
            "    (linear): Linear(in_features=16, out_features=16, bias=True)\n",
            "  )\n",
            "  (ctx_stacked_bi_lstm_layer): StackedBiLSTMsLayer(\n",
            "    (dropout): Dropout(p=0.25, inplace=False)\n",
            "    (lstms): ModuleList(\n",
            "      (0): LSTM(36, 16, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (qst_encoding_layer): QuestionEncodingLayer(\n",
            "    (stacked_bilstms_layer): StackedBiLSTMsLayer(\n",
            "      (dropout): Dropout(p=0.25, inplace=False)\n",
            "      (lstms): ModuleList(\n",
            "        (0): LSTM(16, 16, batch_first=True, bidirectional=True)\n",
            "      )\n",
            "    )\n",
            "    (linear): Linear(in_features=16, out_features=1, bias=True)\n",
            "  )\n",
            "  (bilinear_attention_layer_start): BiLinearAttentionLayer(\n",
            "    (linear): Linear(in_features=32, out_features=32, bias=True)\n",
            "  )\n",
            "  (bilinear_attention_layer_end): BiLinearAttentionLayer(\n",
            "    (linear): Linear(in_features=32, out_features=32, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giP1Ov5_DTGr",
        "outputId": "39911c2a-1231-423a-8a17-7f7c42131627",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir -p ./checkpoints\n",
        "history = trainer.train(train_loader=train_dataloader, valid_loader=valid_dataloader, n_epochs=N_EPOCHS, grad_clip=GRAD_CLIP)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 -     loss: 9.031: 100%|██████████| 1354/1354 [01:40<00:00, 13.48it/s]\n",
            "Epoch: 01 - val_loss: 7.494: 100%|██████████| 536/536 [00:45<00:00, 11.79it/s]\n",
            "Epoch: 02 -     loss: 8.660:   0%|          | 2/1354 [00:00<01:48, 12.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "EM=7.679% - F1=14.671%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 -     loss: 7.837: 100%|██████████| 1354/1354 [01:39<00:00, 13.66it/s]\n",
            "Epoch: 02 - val_loss: 6.400: 100%|██████████| 536/536 [00:44<00:00, 12.03it/s]\n",
            "Epoch: 03 -     loss: 7.520:   0%|          | 2/1354 [00:00<01:50, 12.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "EM=11.761% - F1=21.045%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 -     loss: 7.248:  14%|█▍        | 189/1354 [00:13<01:24, 13.77it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1CVgakLDtsU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}