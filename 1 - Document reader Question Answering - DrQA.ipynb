{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - Document reader Question Answering - DrQA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJ3S7XHIgTjRfUEXVkO1MS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dksifoua/Question-Answering/blob/master/1%20-%20Document%20reader%20Question%20Answering%20-%20DrQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h8encuudFC-",
        "outputId": "18592b01-e571-4183-95ae-e6a10e060ac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Oct 11 07:12:52 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFHeYfTa5iBg"
      },
      "source": [
        "## Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cIZzQAImLQO"
      },
      "source": [
        "!pip install tqdm --upgrade >> /dev/null 2>&1\n",
        "!pip install torchtext --upgrade >> /dev/null 2>&1\n",
        "!pip install spacy --upgrade >> /dev/null 2>&1\n",
        "!python -m spacy download en >> /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gHtnSaemPVM"
      },
      "source": [
        "import re\n",
        "import json\n",
        "import tqdm\n",
        "import spacy\n",
        "import warnings\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.data import Dataset, Example, Field\n",
        "from torchtext.data.iterator import BucketIterator\n",
        "from torchtext.data.metrics import bleu_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q7DBR5gmTZa",
        "outputId": "00e16fc9-6177-4c22-f3f6-ba7646ff7bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "\n",
        "SEED = 546\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNZkKM5G5fgi"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "***Download data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZhbHNhT4YRn",
        "outputId": "54faa3cd-69e9-475e-b9ca-527f26f0d1e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "%%time\n",
        "!mkdir -p ./data\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json \\\n",
        "    -O ./data/train.json\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json \\\n",
        "    -O ./data/valid.json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-11 07:13:20--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘./data/train.json’\n",
            "\n",
            "./data/train.json   100%[===================>]  40.17M   135MB/s    in 0.3s    \n",
            "\n",
            "2020-10-11 07:13:20 (135 MB/s) - ‘./data/train.json’ saved [42123633/42123633]\n",
            "\n",
            "--2020-10-11 07:13:20--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘./data/valid.json’\n",
            "\n",
            "./data/valid.json   100%[===================>]   4.17M  27.2MB/s    in 0.2s    \n",
            "\n",
            "2020-10-11 07:13:21 (27.2 MB/s) - ‘./data/valid.json’ saved [4370528/4370528]\n",
            "\n",
            "CPU times: user 10.6 ms, sys: 18.2 ms, total: 28.9 ms\n",
            "Wall time: 946 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs5S3iohgiMM"
      },
      "source": [
        "***Load data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vTuq9jl3oUJ",
        "outputId": "54b90012-8dcf-4c80-be34-10fc5b2388b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def load_json(path):\n",
        "    with open(path, mode='r', encoding='utf-8') as file:\n",
        "        return json.load(file)['data']\n",
        "\n",
        "train_raw = load_json(path='./data/train.json')\n",
        "valid_raw = load_json(path='./data/valid.json')\n",
        "print(f'Length of raw train data: {len(train_raw):,}')\n",
        "print(f'Length of raw valid data: {len(valid_raw):,}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of raw train data: 442\n",
            "Length of raw valid data: 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6rKVZjcgmF7"
      },
      "source": [
        "***Parse data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfhHilFU6fb_",
        "outputId": "bd7c1141-1a24-4603-9988-d53dbce89469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "def parse_json(data):\n",
        "    qas = []\n",
        "    for paragraphs in data:\n",
        "        for para in paragraphs['paragraphs']:\n",
        "            context = re.sub(r'\\s', ' ', para['context'])\n",
        "            for qa in para['qas']:\n",
        "                id = qa['id']\n",
        "                question = re.sub(r'\\s', ' ', qa['question'])\n",
        "                for ans in qa['answers']:\n",
        "                    answer = re.sub(r'\\s', ' ', ans['text'])\n",
        "                    ans_start = ans['answer_start']\n",
        "                    qas.append({\n",
        "                        'id': id,\n",
        "                        'context': context,\n",
        "                        'question': question,\n",
        "                        'answer': answer,\n",
        "                        'answer_start': ans_start,\n",
        "                    })\n",
        "    return qas\n",
        "\n",
        "train_qas = parse_json(train_raw)\n",
        "valid_qas = parse_json(valid_raw)\n",
        "print(f'Length of train qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs: {len(valid_qas):,}')\n",
        "\n",
        "print('Id:', train_qas[0]['id'])\n",
        "print('Context:', train_qas[0]['context'])\n",
        "print('Question:', train_qas[0]['question'])\n",
        "print('Answer starts at:', train_qas[0]['answer_start'])\n",
        "print('Answer:', train_qas[0]['answer'])\n",
        "\n",
        "for i in range(len(train_qas)): # Test labels are correct in train set\n",
        "    assert train_qas[i]['answer'] == train_qas[i]['context'][train_qas[i]['answer_start']:train_qas[i]['answer_start']+len(train_qas[i]['answer'])]\n",
        "\n",
        "for i in range(len(valid_qas)): # Test labels are correct in validation set\n",
        "    assert valid_qas[i]['answer'] == valid_qas[i]['context'][valid_qas[i]['answer_start']:valid_qas[i]['answer_start']+len(valid_qas[i]['answer'])]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs: 86,821\n",
            "Length of valid qa pairs: 20,302\n",
            "Id: 56be85543aeaaa14008c9063\n",
            "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "Question: When did Beyonce start becoming popular?\n",
            "Answer starts at: 269\n",
            "Answer: in the late 1990s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs87TY30gpOG"
      },
      "source": [
        "***Add targets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMWsmFjjt5vB",
        "outputId": "fc919ce9-a7b2-459f-c914-32f244360d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "%%time\n",
        "def add_targets(qas, nlp=spacy.load('en')):\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "        answer = nlp(qa['answer'], disable=['parser','tagger','ner'])\n",
        "        ans_start = qa['answer_start']\n",
        "        for i in range(len(context)):\n",
        "            if context[i].idx == ans_start:\n",
        "                ans = context[i:i + len(answer)]\n",
        "                qa['target'] = (ans[0].i, ans[-1].i)\n",
        "                break\n",
        "\n",
        "def filter_qas(qa, nlp=spacy.load('en')):\n",
        "    if 'target' in [*qa.keys()]:\n",
        "        context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "        start, end = qa['target']\n",
        "        return context[start:end+1].text == qa['answer']\n",
        "    return False\n",
        "\n",
        "def test_targets(qas, nlp=spacy.load('en')):\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        if 'target' in [*qa.keys()]:\n",
        "            context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "            start, end = qa['target']\n",
        "            assert context[start:end + 1].text == qa['answer'], f\"{context[start:end + 1].text} -- {qa['answer']}\"\n",
        "\n",
        "add_targets(train_qas)\n",
        "add_targets(valid_qas)\n",
        "print()\n",
        "print(f'Length of train qa pairs before filtering out bad qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs before filtering out bad qa pairs: {len(valid_qas):,}')\n",
        "train_qas = [*filter(filter_qas, train_qas)]\n",
        "valid_qas = [*filter(filter_qas, valid_qas)]\n",
        "print()\n",
        "print(f'Length of train qa pairs after filtering out bad qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs after filtering out bad qa pairs: {len(valid_qas):,}')\n",
        "test_targets(train_qas, spacy.load('en'))\n",
        "test_targets(valid_qas, spacy.load('en'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 86821/86821 [01:34<00:00, 917.16it/s]\n",
            "100%|██████████| 20302/20302 [00:23<00:00, 860.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Length of train qa pairs before filtering out bad qa pairs: 86,821\n",
            "Length of valid qa pairs before filtering out bad qa pairs: 20,302\n",
            "\n",
            "Length of train qa pairs after filtering out bad qa pairs: 85,835\n",
            "Length of valid qa pairs after filtering out bad qa pairs: 20,094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 85835/85835 [01:22<00:00, 1040.77it/s]\n",
            "100%|██████████| 20094/20094 [00:12<00:00, 1623.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 13s, sys: 1.9 s, total: 5min 15s\n",
            "Wall time: 5min 15s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RZYINhagvj1"
      },
      "source": [
        "***Build datasets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipM5ew0DnQIP"
      },
      "source": [
        "ID = Field(batch_first=True)\n",
        "TEXT = Field(lower=True, tokenizer_language='en', tokenize='spacy', batch_first=True)\n",
        "TARGET = Field(sequential=False, use_vocab=False, batch_first=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHTjO-B0pUkR",
        "outputId": "c1d9a282-6196-4ecf-d658-ed4ae32feb68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "train_dataset = Dataset([Example.fromdict(data=qa, fields={\n",
        "    'id': ('id', TEXT),\n",
        "    'context': ('cxt', TEXT),\n",
        "    'question': ('qst', TEXT),\n",
        "    'answer': ('ans', TEXT),\n",
        "    'target': ('trg', TARGET)\n",
        "}) for qa in tqdm.tqdm(train_qas)], fields={'cxt': TEXT, 'qst': TEXT,\n",
        "                                            'ans': TEXT, 'trg': TARGET})\n",
        "valid_dataset = Dataset([Example.fromdict(data=qa, fields={\n",
        "    'id': ('id', TEXT),\n",
        "    'context': ('cxt', TEXT),\n",
        "    'question': ('qst', TEXT),\n",
        "    'answer': ('ans', TEXT),\n",
        "    'target': ('trg', TARGET)\n",
        "}) for qa in tqdm.tqdm(valid_qas)], fields={'cxt': TEXT, 'qst': TEXT,\n",
        "                                            'ans': TEXT, 'trg': TARGET})\n",
        "print()\n",
        "print(f'Train dataset size: {len(train_dataset.examples):,}')\n",
        "print(f'valid dataset size: {len(valid_dataset.examples):,}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 85835/85835 [02:18<00:00, 618.19it/s]\n",
            "100%|██████████| 20094/20094 [00:34<00:00, 581.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train dataset size: 85,835\n",
            "valid dataset size: 20,094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBhRj5JNqR3M",
        "outputId": "7b2287e1-cec7-45f8-9a84-e13f0fd04967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%%time\n",
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_dataset)\n",
        "ID.build_vocab([*map(lambda x: x.id, train_dataset.examples)] + [*map(lambda x: x.id, valid_dataset.examples)])\n",
        "print(f'Length of vocabulary: {len(TEXT.vocab):,}')\n",
        "print(f'Length of ids: {len(ID.vocab):,}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of vocabulary: 91,390\n",
            "Length of ids: 91,740\n",
            "CPU times: user 2.95 s, sys: 21 ms, total: 2.97 s\n",
            "Wall time: 2.97 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPE-Js_SYGrY"
      },
      "source": [
        "***Download pretrained GloVe embedding***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvzAqFwcYNGR",
        "outputId": "957bbbdb-ea21-45f9-bec8-5e902ac4423e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "%%time\n",
        "!mkdir -p ./data\n",
        "!wget --no-check-certificate \\\n",
        "    http://nlp.stanford.edu/data/glove.840B.300d.zip \\\n",
        "    -O ./data/glove.840B.300d.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-11 07:21:35--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2020-10-11 07:21:35--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2020-10-11 07:21:35--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘./data/glove.840B.300d.zip’\n",
            "\n",
            "./data/glove.840B.3 100%[===================>]   2.03G  2.00MB/s    in 16m 54s \n",
            "\n",
            "2020-10-11 07:38:30 (2.05 MB/s) - ‘./data/glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n",
            "CPU times: user 2.46 s, sys: 551 ms, total: 3.01 s\n",
            "Wall time: 16min 54s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr-O3NjtYPfw"
      },
      "source": [
        "!unzip -q ./data/glove.840B.300d.zip -d ./data\n",
        "!rm -r ./data/glove.840B.300d.zip"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQmJYwONcN16",
        "outputId": "52c9d4a3-b545-4624-9a3b-1625283a3bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def load_glove(path):\n",
        "    glove = {}\n",
        "    with open(path, mode='r', encoding='utf-8') as file:\n",
        "        for line in tqdm.tqdm(file):\n",
        "            values = line.split(' ')\n",
        "            glove[values[0]] = np.asarray(values[1:], dtype='float32')\n",
        "        return glove\n",
        "\n",
        "glove = load_glove(path='./data/glove.840B.300d.txt')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2196017it [02:52, 12756.26it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaZh7XHneKDV",
        "outputId": "e3edc813-2fe5-436d-e548-8857d9cfb4d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def load_embeddings(glove, field, embedding_size=300, most_common=1000):\n",
        "    most_common_words, most_common_indexes = [*map(lambda x: x[0], field.vocab.freqs.most_common(most_common))], []\n",
        "    embedding_matrix = np.zeros((len(field.vocab), embedding_size))\n",
        "    n_words = 0\n",
        "    for index, word in tqdm.tqdm(enumerate(field.vocab.freqs), total=len(field.vocab)):\n",
        "        if word in most_common_words:\n",
        "            most_common_indexes.append(index)\n",
        "        try:\n",
        "            embedding_matrix[index] = glove[word]\n",
        "            n_words += 1\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return embedding_matrix, n_words, most_common_indexes\n",
        "\n",
        "embedding_matrix, n_words, most_common_indexes = load_embeddings(glove, TEXT)\n",
        "print(f'Words found: {n_words}/{len(TEXT.vocab)}')\n",
        "np.save('./data/GloVe_DrQA.npy', embedding_matrix)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 91388/91390 [00:01<00:00, 49379.51it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Words found: 64718/91390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYsaVy3hsnHE"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zelMKFUV-ayF"
      },
      "source": [
        "***Stacked Bidirectional LSTM Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA4xvLwGxcfF"
      },
      "source": [
        "class StackedBiLSTMLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, n_layers, dropout):\n",
        "        super(StackedBiLSTMLayer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.lstms = nn.ModuleList([nn.LSTM(input_size=hidden_size * 2 if i > 0 else input_size, hidden_size=hidden_size,\n",
        "                                            bidirectional=True, batch_first=True) for i in range(n_layers)])\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, seq_len, input_size]\n",
        "        :return Tensor[batch_size, seq_len, hidden_size * n_layers * 2]\n",
        "        \"\"\"\n",
        "        outputs = [inputs]\n",
        "        for i, lstm in enumerate(self.lstms):\n",
        "            out, _ = lstm(self.dropout(outputs[-1])) # [batch_size, seq_len, hidden_size * 2]\n",
        "            outputs.append(out)\n",
        "        outputs = torch.cat(outputs[1:], dim=-1) # [batch_size, seq_len, hidden_size * n_layers * 2]\n",
        "        return self.dropout(outputs)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZqiiy7wyYrm"
      },
      "source": [
        "***Align Question Embedding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeFiGiibqS3D"
      },
      "source": [
        "class AlignQuestionEmbeddingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AlignQuestionEmbeddingLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
        "    \n",
        "    def forward(self, cxt, qst, qst_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len, hidden_size] cxt\n",
        "        :param Tensor[batch_size, qst_len, hidden_size] qst\n",
        "        :param Tensor[batch_size, qst_len] mask\n",
        "        :return Tensor[batch_size, cxt_len, hidden_size] context\n",
        "        \"\"\"\n",
        "        cxt = F.relu(self.fc(cxt)) # [batch_size, cxt_len, hidden_size]\n",
        "        qst = F.relu(self.fc(qst)) # [batch_size, qst_len, hidden_size]\n",
        "        scores = torch.bmm(cxt, qst.transpose(-1, -2)) # [batch_size, cxt_len, qst_len]\n",
        "        if qst_mask is not None:\n",
        "            scores = scores.masked_fill(qst_mask.unsqueeze(1) == 0, 1e-18)\n",
        "        attention_weights = F.softmax(scores, dim=-1) # [batch_size, cxt_len, qst_len]\n",
        "        context = torch.bmm(attention_weights, qst) # [batch_size, cxt_len, hidden_size]\n",
        "        return context"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkpr81T-z2XA"
      },
      "source": [
        "***Question Encoding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OizoayNez7Le"
      },
      "source": [
        "class QuestionEncodingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(QuestionEncodingLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "    def forward(self, qst_sequences, qst_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, seq_len, hidden_size] qst_sequences\n",
        "        :param Tensor[batch_size, seq_len] qst_mask\n",
        "        :return Tensor[batch_size, hidden_size] context\n",
        "        \"\"\"\n",
        "        scores = self.W(qst_sequences) # [batch_size, seq_len, 1]\n",
        "        if qst_mask is not None:\n",
        "            scores = scores.masked_fill(qst_mask.unsqueeze(-1) == 0, 1e-18)\n",
        "        attention_weights = F.softmax(scores, dim=1) # [batch_size, seq_len, 1]\n",
        "        context = torch.bmm(attention_weights.transpose(-1, -2), qst_sequences) # [batch_size, 1, hidden_size]\n",
        "        return context.squeeze(1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFfqiP4V9k99"
      },
      "source": [
        "***BiLinear Attention Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbPaa3U69oO0"
      },
      "source": [
        "class BiLinearAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, cxt_size, qst_size):\n",
        "        super(BiLinearAttentionLayer, self).__init__()\n",
        "        self.cxt_size = cxt_size\n",
        "        self.qst_size = qst_size\n",
        "        self.linear = nn.Linear(qst_size, cxt_size)\n",
        "\n",
        "    def forward(self, cxt_encoded, qst_encoded, cxt_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len, cxt_size] cxt_encoded\n",
        "        :param Tensor[batch_size, qst_size] qst_encoded\n",
        "        :param Tensor[batch_size, cxt_len] cxt_mask\n",
        "        :param Tensor[batch_size, cxt_len] scores\n",
        "        \"\"\"\n",
        "        qst_encoded = self.linear(qst_encoded) # [batch_size, cxt_size]\n",
        "        scores = torch.bmm(cxt_encoded, qst_encoded.unsqueeze(-1)) # [batch_size, cxt_len, 1]\n",
        "        if cxt_mask is not None:\n",
        "            scores = scores.squeeze(-1).masked_fill(cxt_mask == 0, 1e-18) # [batch_size, cxt_len]\n",
        "        return scores"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC8Jh46f-j22"
      },
      "source": [
        "***DrQA Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8xrxN8R7Em1"
      },
      "source": [
        "class DrQA(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, n_layers, dropout, pad_index):\n",
        "        super(DrQA, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pad_index = pad_index\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.align_question_embedding_layer = AlignQuestionEmbeddingLayer(hidden_size=embedding_size)\n",
        "        self.cxt_stacked_bi_lstm_layer = StackedBiLSTMLayer(input_size=embedding_size * 2, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
        "        self.qst_stacked_bi_lstm_layer = StackedBiLSTMLayer(input_size=embedding_size, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
        "        self.qst_encoding_layer = QuestionEncodingLayer(hidden_size=hidden_size * n_layers * 2)\n",
        "        self.bilinear_attention_layer_start = BiLinearAttentionLayer(cxt_size=hidden_size * n_layers * 2, qst_size=hidden_size * n_layers * 2)\n",
        "        self.bilinear_attention_layer_end = BiLinearAttentionLayer(cxt_size=hidden_size * n_layers * 2, qst_size=hidden_size * n_layers * 2)\n",
        "\n",
        "    def load_glove_embeddings(self, path, most_common_indexes):\n",
        "        def tune_embeddings(grad, words=most_common_indexes):\n",
        "            grad[most_common_indexes] = 0\n",
        "            return grad\n",
        "        \n",
        "        self.embedding.weight = nn.Parameter(torch.FloatTensor(np.load(path)))\n",
        "        self.embedding.weight.register_hook(tune_embeddings) # Only fine-tune the 1000 most frequent question words\n",
        "\n",
        "    def make_cxt_mask(self, cxt_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len]\n",
        "        :return Tensor[batch_size, cxt_len]\n",
        "        \"\"\"\n",
        "        return cxt_sequences != self.pad_index\n",
        "    \n",
        "    def make_qst_mask(self, qst_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, qst_len]\n",
        "        :return Tensor[batch_size, qst_len]\n",
        "        \"\"\"\n",
        "        return qst_sequences != self.pad_index\n",
        "    \n",
        "    @staticmethod\n",
        "    def decode(start_scores, end_scores, max_len=None):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len] start_scores\n",
        "        :param Tensor[batch_size, cxt_len] end_scores\n",
        "        \"\"\"\n",
        "        pred_start, pred_end, pred_score = [], [], []\n",
        "        max_len = max_len or start_scores.size(1)\n",
        "        for i in range(start_scores.size(0)):\n",
        "            scores = torch.ger(start_scores[i], end_scores[i]) # Outer product of scores to get a full matrix            \n",
        "            scores = scores.triu_().tril_(max_len - 1) # Zero out negative length and over-length span scores\n",
        "            max_scores, max_idx = scores.view(-1).topk(1)\n",
        "            pred_score.append(max_scores)\n",
        "            pred_start.append(max_idx.tolist()[0] // scores.size(0))\n",
        "            pred_end.append(max_idx.tolist()[0] % scores.size(0))\n",
        "        return pred_start, pred_end, pred_score\n",
        "\n",
        "    def forward(self, cxt_sequences, qst_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len] cxt_sequences\n",
        "        :param Tensor[batch_size, qst_len] qst_sequences\n",
        "        :param Tensor[batch_size, cxt_len] cxt_mask\n",
        "        :param Tensor[batch_size, qst_len] qst_mask\n",
        "        :return Tensor[batch_size, cxt_len] start_scores\n",
        "        :return Tensor[batch_size, cxt_len] end_scores\n",
        "        \"\"\"\n",
        "        cxt_mask = self.make_cxt_mask(cxt_sequences) # [batch_size, cxt_len]\n",
        "        qst_mask = self.make_qst_mask(qst_sequences) # [batch_size, cxt_len]\n",
        "        \n",
        "        cxt_embedded = self.dropout(self.embedding(cxt_sequences)) # [batch_size, cxt_len, embedding_size]\n",
        "        qst_embedded = self.dropout(self.embedding(qst_sequences)) # [batch_size, cxt_len, embedding_size]\n",
        "        \n",
        "        cxt_qst_aligned = self.align_question_embedding_layer(cxt=cxt_embedded, qst=qst_embedded, qst_mask=qst_mask) # [batch_size, cxt_len, embedding_size]\n",
        "        cxt_encoded = self.cxt_stacked_bi_lstm_layer(inputs=torch.cat([cxt_embedded, cxt_qst_aligned], dim=-1)) # [batch_size, cxt_len, embedding_size * n_layers * 2]\n",
        "        \n",
        "        qst_lstm = self.qst_stacked_bi_lstm_layer(inputs=qst_embedded) # [batch_size, qst_len, hidden_size * n_layers * 2]\n",
        "        qst_encoded = self.qst_encoding_layer(qst_sequences=qst_lstm, qst_mask=qst_mask) # [batch_size, hidden_size * n_layers * 2]\n",
        "\n",
        "        start_scores = self.bilinear_attention_layer_start(cxt_encoded=cxt_encoded, qst_encoded=qst_encoded, cxt_mask=cxt_mask) # [batch_size, cxt_len]\n",
        "        end_scores = self.bilinear_attention_layer_end(cxt_encoded=cxt_encoded, qst_encoded=qst_encoded, cxt_mask=cxt_mask) # [batch_size, cxt_len]\n",
        "\n",
        "        starts = F.log_softmax(start_scores, dim=1) # [batch_size, cxt_len]\n",
        "        ends = F.log_softmax(end_scores, dim=1) # [batch_size, cxt_len]\n",
        "\n",
        "        return starts, ends"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNCQG-tjBHcZ"
      },
      "source": [
        "***Training routines***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBWI7liR6gH2"
      },
      "source": [
        "class AverageMeter:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.value = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "        self.average = 0.\n",
        "        \n",
        "    def reset(self):\n",
        "        self.value = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "        self.average = 0.\n",
        "        \n",
        "    def update(self, value, n=1):\n",
        "        self.value = value\n",
        "        self.sum += value * n\n",
        "        self.count += n\n",
        "        self.average = self.sum / self.count"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV2gVekkBMS3"
      },
      "source": [
        "def exact_match_score(predictions, targets):\n",
        "    \"\"\"\n",
        "    :param list[batch_size] predictions\n",
        "    :param list[batch_size] targets\n",
        "    :return float Exact Match score\n",
        "    \"\"\"\n",
        "    return 100.0 * sum(map(lambda y: y[0] == y[1], zip(predictions, targets))) / len(targets)\n",
        "\n",
        "def f1_scores(predictions, targets):\n",
        "    \"\"\"\n",
        "    precision = correct / length_prediction\n",
        "    recall = correct / length_target\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    :param list[batch_size] predictions\n",
        "    :param list[batch_size] targets\n",
        "    :return float F1 score\n",
        "    \"\"\"\n",
        "    f1_scores = []\n",
        "    for (pred, target) in zip(predictions, targets):\n",
        "        if len(pred) == 0:\n",
        "            f1_scores.append(0)\n",
        "            continue\n",
        "        correct = sum([token in target for token in pred])\n",
        "        precision = correct / len(pred)\n",
        "        recall = correct / len(target)\n",
        "        if precision + recall == 0:\n",
        "            f1_scores.append(0)\n",
        "            continue\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "        f1_scores.append(f1)\n",
        "    return np.mean(f1_scores) * 100.0"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72GXApi2H23S"
      },
      "source": [
        "class Trainer:\n",
        "    \n",
        "    def __init__(self, model, optimizer, criterion):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        \n",
        "    def train_step(self, loader, epoch, grad_clip):\n",
        "        loss_tracker = AverageMeter()\n",
        "        self.model.train()\n",
        "        progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "        for i, batch in progress_bar:\n",
        "            cxt, qst, ans, trg = batch.cxt, batch.qst, batch.ans, batch.trg\n",
        "            self.optimizer.zero_grad()\n",
        "            starts, ends = self.model(cxt_sequences=cxt, qst_sequences=qst) # [batch_size, cxt_len]\n",
        "            loss = self.criterion(starts, trg[:, 0]) + self.criterion(ends, trg[:, 1])\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
        "            self.optimizer.step()\n",
        "            loss_tracker.update(loss.item())\n",
        "            progress_bar.set_description(f'Epoch: {epoch+1:02d} -     loss: {loss_tracker.average:.3f}%')\n",
        "        return loss_tracker.average\n",
        "    \n",
        "    def validate(self, loader, epoch):\n",
        "        loss_tracker = AverageMeter()\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "            for i, batch in progress_bar:\n",
        "                cxt, qst, ans, trg = batch.cxt, batch.qst, batch.ans, batch.trg\n",
        "                starts, ends = self.model(cxt_sequences=cxt, qst_sequences=qst) # [batch_size, cxt_len]\n",
        "                loss = self.criterion(starts, trg[:, 0]) + self.criterion(ends, trg[:, 1])\n",
        "                loss_tracker.update(loss.item())\n",
        "                progress_bar.set_description(f'Epoch: {epoch+1:02d} - val_loss: {loss_tracker.average:.3f}')\n",
        "        return loss_tracker.average\n",
        "    \n",
        "    def train(self, train_loader, valid_loader, n_epochs, grad_clip, max_len):\n",
        "        history, best_loss = {'loss': [], 'val_loss': []}, np.inf\n",
        "        for epoch in range(n_epochs):\n",
        "            loss = self.train_step(train_loader, epoch, grad_clip)\n",
        "            val_loss = self.validate(valid_loader, epoch)\n",
        "            if best_loss > val_loss:\n",
        "                best_loss = val_loss\n",
        "                torch.save(self.model.state_dict(), './checkpoints/DrQA.pth')\n",
        "            history['loss'].append(loss); history['val_loss'].append(val_loss)\n",
        "        return history"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlWgRbN9ROLA"
      },
      "source": [
        "***Train the model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ-a11PIRRRq"
      },
      "source": [
        "N_LAYERS = 3\n",
        "HIDDEN_SIZE = 128\n",
        "EMBED_SIZE = 300\n",
        "DROPOUT = 0.3\n",
        "N_EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "GRAD_CLIP = 10.0\n",
        "MAX_LEN = 15"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LodDrpgLRo_h",
        "outputId": "b3e7660d-330c-40e0-bfff-c1f7984a0c0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "drqa = DrQA(vocab_size=len(TEXT.vocab), embedding_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, n_layers=N_LAYERS,\n",
        "            dropout=DROPOUT, pad_index=TEXT.vocab.stoi[TEXT.pad_token])\n",
        "drqa.load_glove_embeddings('./data/GloVe_DrQA.npy', most_common_indexes)\n",
        "drqa.to(DEVICE)\n",
        "optimizer = optim.Adamax(params=drqa.parameters())\n",
        "criterion = nn.NLLLoss(ignore_index=TEXT.vocab.stoi[TEXT.pad_token])\n",
        "print(f'Number of parameters of the model: {sum(p.numel() for p in drqa.parameters() if p.requires_grad):,}')\n",
        "print(drqa)\n",
        "trainer = Trainer(model=drqa, optimizer=optimizer, criterion=criterion)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters of the model: 31,458,149\n",
            "DrQA(\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (embedding): Embedding(91390, 300)\n",
            "  (align_question_embedding_layer): AlignQuestionEmbeddingLayer(\n",
            "    (fc): Linear(in_features=300, out_features=300, bias=True)\n",
            "  )\n",
            "  (cxt_stacked_bi_lstm_layer): StackedBiLSTMLayer(\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (lstms): ModuleList(\n",
            "      (0): LSTM(600, 128, batch_first=True, bidirectional=True)\n",
            "      (1): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "      (2): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (qst_stacked_bi_lstm_layer): StackedBiLSTMLayer(\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (lstms): ModuleList(\n",
            "      (0): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
            "      (1): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "      (2): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (qst_encoding_layer): QuestionEncodingLayer(\n",
            "    (W): Linear(in_features=768, out_features=1, bias=True)\n",
            "  )\n",
            "  (bilinear_attention_layer_start): BiLinearAttentionLayer(\n",
            "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "  )\n",
            "  (bilinear_attention_layer_end): BiLinearAttentionLayer(\n",
            "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahAzyTraSsy5",
        "outputId": "8539dad7-b45f-454c-9677-252a902d8d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "train_iterator, valid_iterator =  BucketIterator.splits((train_dataset, valid_dataset), batch_size=BATCH_SIZE, sort=False, device=DEVICE)\n",
        "!mkdir -p ./checkpoints\n",
        "history = trainer.train(train_loader=train_iterator, valid_loader=valid_iterator, n_epochs=N_EPOCHS, grad_clip=GRAD_CLIP, max_len=MAX_LEN)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 -     loss: 6.393%: 100%|██████████| 2683/2683 [04:22<00:00, 10.22it/s]\n",
            "Epoch: 01 - val_loss: 5.341: 100%|██████████| 628/628 [00:15<00:00, 39.83it/s]\n",
            "Epoch: 02 -     loss: 5.162%: 100%|██████████| 2683/2683 [04:23<00:00, 10.17it/s]\n",
            "Epoch: 02 - val_loss: 5.017: 100%|██████████| 628/628 [00:15<00:00, 39.88it/s]\n",
            "Epoch: 03 -     loss: 4.718%: 100%|██████████| 2683/2683 [04:23<00:00, 10.17it/s]\n",
            "Epoch: 03 - val_loss: 4.717: 100%|██████████| 628/628 [00:15<00:00, 40.59it/s]\n",
            "Epoch: 04 -     loss: 4.420%: 100%|██████████| 2683/2683 [04:24<00:00, 10.15it/s]\n",
            "Epoch: 04 - val_loss: 4.658: 100%|██████████| 628/628 [00:15<00:00, 41.10it/s]\n",
            "Epoch: 05 -     loss: 4.186%: 100%|██████████| 2683/2683 [04:22<00:00, 10.23it/s]\n",
            "Epoch: 05 - val_loss: 4.459: 100%|██████████| 628/628 [00:15<00:00, 40.54it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17zun4BjVqgW",
        "outputId": "240fba91-61a6-426b-99e2-9bbc5191e3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(history['loss'], label='train')\n",
        "plt.plot(history['val_loss'], label='valid')\n",
        "plt.title('Loss history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUVfrH8c+TTkgljRQgAUIPCYQqLQgKIoKF4trWbfaKYPutdXV1VSy4uvZ13UURQQQRVKSjoAImEFpCJ4QSWiD0JOf3xx0gxAQTyOROZp7363VfzMy9M/eb0Zln7j3nniPGGJRSSnkuL7sDKKWUspcWAqWU8nBaCJRSysNpIVBKKQ+nhUAppTycFgKllPJwWgiUOk8i8qGIPHOO9UUi0rQ2Myl1PrQQqDpPRDaLSH+7c5RnjAkyxmw81zYikiEiebWVSamKaCFQqg4TER+7M6i6TwuBclsi4i8ir4pIvmN5VUT8HesiRWS6iBwQkX0islBEvBzrHhKR7SJySETWiUi/c+wmXES+cmz7o4g0K7N/IyLNHbcHichqx3bbRWS0iNQHZgJxjtNIRSIS9xu5M0Qkz5FxJ/BvEckWkSvK7NdXRPaISIeaf1eVO9JCoNzZ/wHdgDQgFegC/NWx7gEgD4gCYoBHASMiLYG7gM7GmGBgALD5HPu4FngKCAfWA89Wst37wK2O12wHzDHGHAYuA/Idp5GCjDH5v5EboCHQAGgC3AJ8BNxQZv0gYIcx5pdz5FbqNC0Eyp1dDzxtjNltjCnA+sK+0bHuJBALNDHGnDTGLDTWwFslgD/QRkR8jTGbjTEbzrGPKcaYn4wxxcB4rC/vipx0vGaIMWa/MWb5eeYGKAWeMMYcN8YcBf4HDBKREMf6G4H/nuP1lTqLFgLlzuKALWXub3E8BvAi1i/4b0Vko4g8DGCMWQ/cBzwJ7BaRCSISR+V2lrl9BAiqZLtrsH6pbxGR+SLS/TxzAxQYY46duuM4ivgeuEZEwrCOMsaf4/WVOosWAuXO8rFOn5zS2PEYxphDxpgHjDFNgSHAqFNtAcaYj40xPR3PNcA/LjSIMeZnY8xQIBr4Aph4alV1cp/jOf/BOj00HFhsjNl+oZmV59BCoNyFr4gElFl8gE+Av4pIlIhEAo9jnUZBRAaLSHMREaAQ65RQqYi0FJGLHY2zx4CjWKdizpuI+InI9SISaow5CRws85q7gAgRCS3zlEpzn8MXQEfgXqw2A6WqTAuBchczsL60Ty1PAs8AS4EVwEpgueMxgGTgO6AIWAy8aYyZi9U+8DywB+u0TzTwSA3kuxHYLCIHgduw2gEwxqzF+uLf6OjBFPcbuSvkaCuYDCQBn9dAXuVBRCemUco9iMjjQAtjzA2/ubFSZejFKEq5ARFpAPyJs3sXKVUlempIqTpORP4CbANmGmMW2J1H1T1OPTXk6Mr2HtYFNAb4ozFmcZn1GcBUYJPjoc+NMU87LZBSSqlfcfapodeAr40xw0TEDwisYJuFxpjBTs6hlFKqEk4rBI7ucL2BmwGMMSeAExf6upGRkSYxMfG8nnv48GHq169/oRFqnKvmAtfNprmqR3NVjzvmWrZs2R5jTFRF65x5RJAEFGANipUKLAPudYyvUlZ3EcnCumBmtDFmVfkXEpFbsMZUISYmhpdeeum8AhUVFREUVNmFn/Zx1Vzgutk0V/Vorupxx1x9+/bdUulKY4xTFqATUAx0ddx/DfhbuW1CgCDH7UFA7m+9bnp6ujlfc+fOPe/nOpOr5jLGdbNprurRXNXjjrmApaaS71Vn9hrKA/KMMT867k/CuvKxbBE6aIwpctyegXV1aKQTMymllCrHaYXAGLMT2OYY1hegH7C67DYi0tBxiT8i0sWRZ6+zMimllPo1Z/cauhsY7+gxtBH4g4jcBmCMeQsYBtwuIsVYwwJc6ziEUUqpGnPy5Eny8vI4duzYb28MhIaGsmbNGienqr6q5AoICCAhIQFfX98qv65TC4ExJhOrraCst8qs/yfwT2dmUEqpvLw8goODSUxMxHES4pwOHTpEcHBwLSSrnt/KZYxh79695OXlkZSUVOXX1SuLlVJu79ixY0RERFSpCNRlIkJERESVj3xO0UKglPII7l4ETjmfv9NjCsGOwqOMX3OckyUXNLS8Ukq5HY8pBFnbCpm1pZjXZ+faHUUp5WEOHDjAm2++We3nDRo0iAMHDjgh0dk8phAMbNeQHnE+vDFvA79s3W93HKWUB6msEBQXF5/zeTNmzCAsLMxZsU7zmEIAcH1rPxqGBPDAxCyOniixO45SykM8/PDDbNiwgbS0NDp37kyvXr0YMmQIbdq0AeDKK68kPT2dtm3b8s4775x+XmJiInv27GHz5s20bt2au+++m7Zt23LppZdy9OjRGsvnURPTBPoKLw5P4bp3f+S5mWt4emg7uyMppWrZU1+uYnX+wXNuU1JSgre3d5Vfs01cCE9c0bbS9c8//zzZ2dlkZmYyb948Lr/8crKzs0938fzggw9o0KABR48epXPnzlxzzTVERESc9Rq5ubm89957fPjhh4wYMYLJkydzww01MxmdRx0RAFzULJI/9kjio8VbWJBTYHccpZQH6tKly1n9/MeNG0dqairdunVj27Zt5Ob+ui0zKSmJ9u3bA5Cens7mzZtrLI9HHRGc8uDAlizILeDBSSv45r7ehAZW/Qo8pVTddq5f7qc4+4KyskNJz5s3j++++47FixcTGBhIRkZGhdcB+Pv7n77t7e1do6eGPO6IACDA15tXRqSxp+g4j03NtjuOUsrNBQcHc+jQoQrXFRYWEh4eTmBgIGvXrmXJkiW1nM5DCwFASkIo9/RLZlpWPl9m5dsdRynlxiIiIujRowft2rVjzJgxZ60bOHAgxcXFtG7dmocffphu3brVej6PPDV0yh0ZzZizdjd//SKbLkkNiAkJsDuSUspNffzxxxU+7u/vz8yZMytcd6odIDIykuzs7NNHFaNHj67RbB57RADg4+3FyyNSOV5cwphJK9CBT5VSnsijCwFA06ggHh3UmgU5Bfzvx612x1FKqVrn8YUA4MZuTeiVHMnfv1rDpj3lp1RWSin3poUAa7S+F4el4ufjxaiJmRTrwHRKKQ+ihcChYWgAf7uyHb9sPcBb8zfYHUcppWqNFoIyhqTGMbh9LK9+l0v29kK74yilVK3QQlDOM1e2IyLIj/s/zeTYSR2YTilV+4KCggDIz89n2LBhFW6TkZHB0qVLa2R/WgjKCQv044VhqeTuLuKlb9bZHUcp5cHi4uKYNGmS0/ejhaACfVpEcUO3xrz//SYWb9hrdxylVB338MMP88Ybb5y+/+STT/LMM8/Qr18/OnbsSEpKClOnTv3V8zZv3ky7dtYoyUePHuXmm2+mdevWXHXVVToMdW14dFBrvl+/l9GfZfH1fb0IDtCB6ZRyCzMfhp0rz7lJvZJi8K7G12PDFLjs+UpXjxw5kvvuu48777wTgIkTJ/LNN99wzz33EBISwp49e+jWrRtDhgypdM7hf/3rXwQGBrJmzRpWrFhBx44dq57vN+gRQSUC/XwYOyKVHYVHeerL1XbHUUrVYR06dGD37t3k5+eTlZVFeHg4DRs25NFHH6V9+/b079+f7du3s2vXrkpfY8GCBYwcORKA9u3bnx6SuiboEcE5dGwczh0Zzfnn3PVc0iaGAW0b2h1JKXWhzvHL/ZSjThiGevjw4UyaNImdO3cycuRIxo8fT0FBAcuWLcPX15fExMQKh5+uDXpE8Bvu6ZdM27gQHv18JXuKjtsdRylVR40cOZIJEyYwadIkhg8fTmFhIdHR0fj6+jJ37ly2bNlyzuf37t2bzz77DIDs7GxWrFhRY9m0EPwGPx8vXhmZxqHjxTw8eaUOTKeUOi9t27bl0KFDxMfHExsby/XXX8/SpUtJSUnho48+olWrVud8/u23305RURGtW7fm8ccfJz09vcay6amhKmgRE8yDA1ryzFdr+GxpHiM6N7I7klKqDlq58kwjdWRkJIsXL65wu6KiIsCavD4725o8q169enz44YdOmTlNjwiq6I89kujWtAFPfbmKbfuO2B1HKaVqjBaCKvLyEl4anoqXCA98lkVJqZ4iUkq5By0E1ZAQHsgTQ9ry06Z9vL9oo91xlFLV4Cnte+fzd2ohqKZrOsZzaZsYXvomh7U7D9odRylVBQEBAezdu9fti4Exhr179xIQUL1pd7WxuJpEhOeuTmHAqwu4/9Mspt7ZAz8fradKubKEhATy8vIoKCio0vbHjh2r9pdpbahKroCAABISEqr1uloIzkNEkD/PXd2ev3y0lFe/y+HBgefu9qWUspevry9JSUlV3n7evHl06NDBiYnOj7Ny6U/Z83RJmxhGdErgrfkbWLZln91xlFLqvGkhuACPDW5DXFg9Rk3M4vDxYrvjKKXUeXFqIRCRMBGZJCJrRWSNiHQvt15EZJyIrBeRFSJSc8Pp1YLgAF/GDk9l674jPDtjjd1xlFLqvDj7iOA14GtjTCsgFSj/bXkZkOxYbgH+5eQ8Na5r0wj+0qspH/+4lblrd9sdRymlqs1phUBEQoHewPsAxpgTxpgD5TYbCnxkLEuAMBGJdVYmZxl1SQtaxgTz4OQV7D98wu44SilVLeKsfrUikga8A6zGOhpYBtxrjDlcZpvpwPPGmEWO+7OBh4wxS8u91i1YRwzExMSkT5gw4bwyFRUVnZ4LtKZtOVjC04uP0THGmztS/SudXKK2c10oV82muapHc1WPO+bq27fvMmNMpwpXGmOcsgCdgGKgq+P+a8Dfym0zHehZ5v5soNO5Xjc9Pd2cr7lz5573c6vin3NyTZOHppspy/Oq9Txn57oQrppNc1WP5qoed8wFLDWVfK86s40gD8gzxvzouD8JKN8YvB0oO5RnguOxOum2Ps1IbxLOY1OzyT9Qc/OJKqWUMzmtEBhjdgLbRKSl46F+WKeJypoG3OToPdQNKDTG7HBWJmfz9hJeHpFKSalhzKQsSnVgOqVUHeDsXkN3A+NFZAWQBvxdRG4Tkdsc62cAG4H1wLvAHU7O43RNIurz18vb8P36vXy0eLPdcZRS6jc5dYgJY0wmVltBWW+VWW+AO52ZwQ6/69KIWat38tzMtfRMjqJ5tOs1Oiml1Cl6ZbETiAj/uKY9gX7ejJqYycmSUrsjKaVUpbQQOEl0SADPXpXCirxC3pi73u44SilVKS0ETjQoJZYr0+J4fc56sraVv5ZOKaVcgxYCJ3tqaDuig/25f2Imx06W2B1HKaV+RQuBk4XW8+XFYalsLDjM8zPX2h1HKaV+RQtBLeiZHMnNFyXy4Q+bWZS7x+44Sil1Fi0EteShga1oGlWfMZOyKDx60u44Sil1mhaCWlLPz5tXRqSx+9Bxnpy2yu44Sil1mhaCWpTaKIy7+jZnyi/bmbGyzo6koZRyM1oIatldFzenfUIoj05Zye6Dx+yOo5RSWghqm6+3Fy+PSOPoiRIemrzi1PDbSillGy0ENmgeHcQjl7Vi7roCPvlpm91xlFIeTguBTW7qnkiP5hE889Vqdh/RsYiUUvbRQmATLy/hxWGpeHsJ76w4TonOXaCUsokWAhvFhdXjb0Pbsf5AKW8v2GB3HKWUh9JCYLOhaXF0ivHmlVk5rMovtDuOUsoDaSGwmYhwc1t/wgL9GPVpFseLdWA6pVTt0kLgAoL8hBeuac+6XYd4+dscu+MopTyMFgIX0bdVNL/r0ph3Fm7kx4177Y6jlPIgWghcyF8vb03jBoE88FkWh47pwHRKqdqhhcCF1Pf3YezwVPIPHOWZ6WvsjqOU8hBaCFxMp8QG3NqnGZ8u3cas1bvsjqOU8gBaCFzQ/f1b0Do2hEc+X8HeouN2x1FKuTktBC7Iz8eLV0emcfBoMY9OWakD0ymlnEoLgYtq2TCY0QNa8M2qXUxevt3uOEopN6aFwIX9qWdTuiQ24Mlpq8jbf8TuOEopN6WFwIV5ewljR6RijGH0Z1mU6sB0Sikn0ELg4ho1COSJK9qyZOM+Pvh+k91xlFJuSAtBHTC8UwL9W0fzwjfryNl1yO44Sik3o4WgDhARnru6PcH+Ptz/aSYninUiG6VUzdFCUEdEBfvz7FUprMo/yOtzcu2Oo5RyI1oI6pCB7RpyTccE3pi7nuVb99sdRynlJrQQ1DFPDGlDbGg9Rn2ayZETxXbHUUq5AS0EdUxIgC8vDU9l894jPDdjrd1xlFJuQAtBHdS9WQR/6pnEf5dsYX5Ogd1xlFJ1nBaCOmrMgJYkRwcx5rMsDhw5YXccpVQd5tRCICKbRWSliGSKyNIK1meISKFjfaaIPO7MPO4kwNebV0amse/wCR6busruOEqpOsynFvbR1xiz5xzrFxpjBtdCDrfTLj6U+/on89K3OVzSJoYhqXF2R1JK1UF6aqiOu61PMzo0DuOvU1ays/CY3XGUUnWQOHOsexHZBOwHDPC2MeadcuszgMlAHpAPjDbG/Oo8h4jcAtwCEBMTkz5hwoTzylNUVERQUNB5PdeZLjTXzsOlPP7DUVqEefNAJ39ExGWyOYvmqh7NVT3umKtv377LjDGdKlxpjHHaAsQ7/o0GsoDe5daHAEGO24OA3N96zfT0dHO+5s6de97PdaaayPXR4s2myUPTzUc/bLrg1yrLnd8zZ9Bc1aO5qudCcgFLTSXfq049NWSM2e74dzcwBehSbv1BY0yR4/YMwFdEIp2ZyV3d0LUxvVtE8eyMNWwsKLI7jlKqDnFaIRCR+iISfOo2cCmQXW6bhuI4jyEiXRx59jorkzsTEV4c1h5/H2/un5hFcYkOTKeUqhpnHhHEAItEJAv4CfjKGPO1iNwmIrc5thkGZDu2GQdc6ziEUechJiSAZ65sR9a2A/xr3ga74yil6gindR81xmwEUit4/K0yt/8J/NNZGTzRFalxfLt6F6/NziWjZTQpCaF2R1JKuTjtPuqG/ja0LRFBftw/MZNjJ0vsjqOUcnFaCNxQWKAfLw5LZf3uIl78Zp3dcZRSLk4LgZvq3SKKG7s14f1Fm/hhw7ku7FZKeTotBG7skUGtSIqsz+iJWRw8dtLuOEopF6WFwI0F+vnw8ohUdh06zlPTVtsdRynloqpUCBzXBHg5brcQkSEi4uvcaKomdGgczp0ZzZi8PI+vs3faHUcp5YKqekSwAAgQkXjgW+BG4ENnhVI16+5+ybSLD+HRKSvZfUgHplNKna2qhUCMMUeAq4E3jTHDgbbOi+UExwqJLFgMxZ43iYuvtxevjEij6Hgxj0xeiV6zp5Qqq8qFQES6A9cDXzke83ZOJCdZPZV2q56Hl1vDN/8HuzzrnHlyTDAPDWzF7LW7mbh0m91xlFIupKqF4D7gEWCKMWaViDQF5jovlhOkXseKlMegyUXw49vwr+7wTl/4+X04esDudLXiDxcl0r1pBE9/uZqte4/YHUcp5SKqVAiMMfONMUOMMf9wNBrvMcbc4+RsNcvbh30RnWDkf+GBtTDgOSg+Dl+NgrEtYfKfYeM8KHXfwdq8vISXRqTiJcIDn2VSUqqniJRSVe819LGIhDhGEc0GVovIGOdGc6L6kdD9Drj9e/jLXOhwA+R8Cx8NhddSYd7zcGCr3SmdIj6sHk8OacvPm/fz3sKNdsdRSrmAqp4aamOMOQhcCcwEkrB6DtVtIhDfES4fC6PXwTXvQ0RTqxC82t4qDCs+g5NH7U5ao67uGM+AtjGM/TaHNTsO2h1HKWWzqhYCX8d1A1cC04wxJ7Gmn3QfvvUgZRjcNBXuWwEZj8C+jfD5n+GlljB9FGxfDm7Q40ZE+PtVKYTU8+X+TzM5XqwD0ynlyapaCN4GNgP1gQUi0gRw35+SYY0h4yG4JwtumgYtBkDmeHi3L/yrByx+Aw7X7fF7IoL8ef7qFNbuPMSr3+XaHUcpZaOqNhaPM8bEG2MGOaa/3AL0dXI2+3l5QdM+cM278MA6GPwK+AbAN4/C2Fbw6Q2Q8w2UFNud9Lz0bxPDyE6NeHv+BpZu3md3HKWUTaraWBwqIi+LyFLHMhbr6MBz1AuDTn+Ev8yB2xdD11thy2L4eAS80ha+exL2rLc7ZbU9dkUb4sPrMWpiFkXH62ZBU0pdmKqeGvoAOASMcCwHgX87K5TLi2kDA56FUWtg5HiI6wDfj4N/psP7A2D5f+H4IbtTVkmQvw9jh6exbf8Rnv3Ksy6yU0pZqloImhljnjDGbHQsTwFNnRmsTvDxg9aD4boJMGo19H8KjuyFaXdZDcxf3AlbfnD5BuYuSQ24pXdTPvlpG3PW7rI7jlKqllW1EBwVkZ6n7ohID8C9+lReqOCG0PM+uOtn+NMsSLkGVn8B/74MXk+HhWPhYL7dKSs16pIWtGoYzIOTVrLvsOeNx6SUJ6tqIbgNeENENovIZqwJ5291Wqq6TAQadYEhr8PoHLjyX1aRmP201ZYwfjis+sK6qtmF+Pt48/KINAqPnuD/pujAdEp5kqr2GsoyxqQC7YH2xpgOwMVOTeYO/OpD2nXwhxlw93LoOQp2ZsNnv7d6Hc182LrvItrEhTDqkpbMzN7JF5nb7Y6jlKol1ZqhzBhz0HGFMcAoJ+RxXxHNoN9jcH82XD8ZknrD0vfhrR6kLx0FP70LR/fbnZJbejelU5NwHp+6ivwDevZPKU9wIVNVSo2l8CRe3pDcH0b8x7o24bIXEFMKM0ZbDcyT/ggb5tg2+J23lzB2RColpYbRn2VRqqeIlHJ7PhfwXP2GuFCBDaDrrSw92pKMluHwy/9gxUTIngyhjazTSmnXQXhircZqElGfxwa34ZHPV7J5p/BY5A4GtmuIiNZ+pdzROQuBiByi4i98Aeo5JZGnik21lkv+ButmWEVh/gsw/x+Q2As63AitrwC/wFqJc23nRoTV8+XpL37h9vHLSYkP5YFLW9CnRZQWBKXczDkLgTEmuLaCKAffAGh3tbUU5kHWJ1ZRmHILzAixHu9wI8SnWz2UnEREuCwlFv89a9kXkswrs3K4+d8/0yWxAWMGtqRzYgOn7VspVbsupI1AOVtoAvQeA3f/Ajd/Ba0uh6xP4b1+8GY3+OF1KNrt1AheIgxLT2DO6D48PbQtm/YeZvhbi7n53z+Rvb3QqftWStUOLQR1gZcXJPaEq96yrk24Yhz4h8C3f7XmYP7kOlg7A0pOOi2Cv483N3VPZMGYvjx8WSt+2XqAwa8v4s7xy1m/u8hp+1VKOd+FNBYrOwSEQPrvraVgnXXaKGsCrPsK6kdD6rXWjGtRLZ2y+3p+3tzWpxnXdW3Mews28t6iTczM3sHVHRO4t18yjRrUThuGUqrm6BFBXRbVEi79mzXO0bWfWFc0L3kT3ugC7/WHZR/CMedMGxES4MuoS1uy4MG+/KFHEtOy8rl47DyemJrN7kPHnLJPpZRzaCFwB96+0GoQXDveGhH10mes0U+/vBdeagFTboPNi5wy+F1kkD+PDW7D/DEZDEtvxP9+3ErvF+byj6/XcuCIjlmkVF2ghcDdBEXDRXfDHUvgz7OtU0Vrv4IPL4dxHWD+i1ZvpBoWG1qP565OYfaoPgxo25C35m+g1z/m8vrsXJ3nQCkXp4XAXYlAQie44lXrCuar3rF6Ic19Bl5pB/+9GrI/r/HB7xIj6/PatR2YeW8vujWLYOysHPq8MJf3F23i2EmdG1kpV6SNxZ7ALxBSR1rL/s2Q+TH8Mh4m/QHqhUPKcKuBOTa1xnbZqmEI797UieVb9zP223X8bfpq3lu4kXv7JXNNegK+3vobRClXoZ9GTxOeCH0fhftWwI1ToNnFsOw/8HZveKsn/Pg2HKm5+Ys7Ng5n/J+7Mf7PXYkJCeDhz1dyycvzmZq5ndJSHaVEKVfg1ELgmL9gpYhkisjSCtaLiIwTkfUiskJEOjozjyrDy9sqAsM+gNHrYNBLIN4w80EY2xIm/h5yv0NKa+Z0To/mkUy54yLeu6kTAb7e3Dshk0HjFvLd6l0694FSNquNU0N9jTF7Kll3GZDsWLoC/3L8q2pTvXDo8hdr2bnSOm204lNY/QXdfcPg+LXW6aMLHNZCROjfJoaLW0Xz5Yp8XpmVw58/WkqHxmGMGdCSi5pF1uAfpZSqKrtPDQ0FPjKWJUCYiMTanMmzNUyBy56HB9bCyP9RGNoalv7bGtZiXAeY8ywU5FzQLry8hKFp8cwa1Yfnrk5hx4FjXPfuj1z/3hJ+2Wr/nAxKeRpx5mG5iGwC9mONYPq2MeadcuunA88bYxY57s8GHjLGLC233S3ALQAxMTHpEyZMOK88RUVFBAUFnddznclVc4GVLTRAiCpYQvTu+YTvX4lQyqGgpuyK6cPu6F6c8I+4oH2cKDHM3VbM9A0nOHQSOkR7c3WyH42CK/+d4qrvmeaqHs1VPReSq2/fvsuMMZ0qXGmMcdoCxDv+jQaygN7l1k8Hepa5PxvodK7XTE9PN+dr7ty55/1cZ3LVXMZUkO3gDmMWv2nM2xnGPBFizBOhxvz7cmOWfmjMkX0XtK9Dx06acd/lmHaPf20SH55u7v1kudlUUFS1XC5Cc1WP5qqeC8kFLDWVfK869dSQMWa749/dwBSgS7lNtgONytxPcDymXFVwQ+h2O9wy15qHOeMROLQDvrwHXky2BsBbNQVOVn+ayyB/H+7ul8zCh/pya+9mfL1qJ/1fns8jn69kR6FOm6mUszitEIhIfREJPnUbuBQoP1P7NOAmR++hbkChMWaHszKpGhbRDDIegruWwi3zoOutsH0ZfHazVRSm3AbrZ0NJ9a4sDgv04+HLWrFgTF+u79qYScu20efFeTwzfTV7i2r2AjillHN7DcUAUxyzWfkAHxtjvhaR2wCMMW8BM4BBwHrgCPAHJ+ZRziICcR2s5ZKnrXGNVn4Gq6dZE+vUj4K2V0P7EdXqeRQdEsBTQ9vx515NeW12Lh98v4lPftrKn3o1pZXOlKpUjXFaITDGbAR+damqowCcum2AO52VQdnAyxua9rGWQS/B+llWUVj2Ifz0tnVBW8pwa6niUNmNGgTy0vBUbuvTlJdn5TBudi71fWGr3wZ+3z2Ren7eTv2TlHJ3dncfVe7MN8CaZ3nERzAmF4a+CS+ThbcAABjCSURBVOFJsHCsNVT2W73g+3FQWLVmoebRwbx5fTrT7+5Js1Bvnp+5lt4vzuW/izdzorjUuX+LUm5MC4GqHQGh0OF6uOkLa6jsgc9bw2fPegxeaQsfDraOGqowvEW7+FBGdQpg4q3dSYwI5LGpq7h47DwmL8ujRIetUKratBCo2neq59Ff5pTreeSYP+GT66yRUU8cOefLdElqwMRbu/PhHzoTFujLA59lMeDVBcxcuUOHrVCqGnT0UWWvUz2P+jwIOzJh5SRrWfcV+AVZp5ZShkFSBnj/+n9XESGjZTR9WkTxdfZOXvp2HbePX05KfCijB7Skd3IkcgHDYijlCbQQKNdQ1Z5HKcOteRZ+9XThspRYLmkTwxeZ1jhGv//gJ7okNWDMgJZ0Tmxgwx+lVN2ghUC5nrI9jy4fC7nf/qrnUWJIF2gb+6ueRz7eXgxLT+CK1Fg+/Xkbr89Zz/C3FtO3ZRQPXNqSdvGh9vxNSrkwLQTKtfn4W6eHWl8BxwphzXRY+RlNNk6CNyZag+SljIB210Bo/Omn+ft4c1P3RIanN+I/izfzr3kbGPz6Ii5PieX+S1rQPNr1xpFRyi7aWKzqjjI9jxZ3/8DR88jvTM+jf1/+q55H9fy8ua1PMxY82Jd7Lm7O3HW7ufSV+Yz5LIu8/edujFbKU2ghUHXSCf/wX/c8KtpVpufR787qeRRaz5dRl7ZkwYN9+UOPJKZm5dP3pXk8OW0Vuw8ds/mvUcpeempI1X1n9TzKstoTsifDuhlWz6NWg61G5qYZRAb589jgNvy5VxLjZq/nv0u28OnP27i5RyK39m5KWKCf3X+NUrVOC4FyHyIQl2YtlzwNW7539DyaCismQGAktLsaUkYQm9CJ565O4dbeTXnluxzemr+B/y3Zwq29m/KHHknU99ePhvIcempIuScvb0jqDUNeh9G5MHI8JPaE5R/B+/3htVSY/TcSS7fx2rUdmHFPL7omRfDStzn0fmEuHyzaxLGTNTNfs1KuTn/2KPfn4w+tB1vLsYOw1up5xKKXYeFL0DCF1inDee/KYSzv24yx367j6emreXfhRu7tl8yw9AR8vPU3k3Jf+n+38iwBIZB2Hdw4BUathYH/AG9/mPU4vNKWjrNvYHzaGj69sSUxIQE8/PlKLnllAdOy8inVcYyUm9JCoDxXcAx0uw3+MtvqedT3Uavn0fT76Dq5G1MavM60jF0Ee53gnk9+YdC4hcxes0vHMVJuR08NKQVWz6M+D0LvMad7Hkn2ZNqvm8lUvyDyki/m9d1p3PKfA7RvHMGYAS25qFmk3amVqhFaCJQqq4KeR7LyMxqtnsoLx6fxdEgDvtzTlZfe60pg0+6MHtiKtEZhdqdW6oJoIVCqMqd6HiX1tmZby51FwMrPGJbzNcP9Z5K3PZopb1/ElMQruG7wAFo2DLY7sVLnRQuBUlVRpueROHoeNcyayJ2bpuGV9wWr32zClzGXElA/HLPJG6kXDqcW33pVnqdZKTtoIVCquhw9j3zSroOi3Rz5ZSKhSz7mioJ3oQDY/MLZ23v7O4pC2JnicHoJg4BKHvcPBS/tz6GcTwuBUhciKJrAXncR2OsuDu3JY8K0GRQcNeTl5xNCEcnBJ+kYBckhJwkqLYKj++HANtixwrp98nDlry1e1kB7ZQtEZUWj/DY+OlSGqjotBErVkODIBJKbtuAvGRkUHDrOjJU7mJq5nb/lHgCgU5NwhqbFMSglloggf+tJxSfg2AGrKJy1VPTYfti38cx6ztGN1S+oTNEIo21RMRz8vJKjkjIFxq++nsbyQFoIlHKCqGB/fn9RIr+/KJGte4/w5Yp8pmZu57Gpq3jyy9X0So5kaFocl7RpSFBQNARFV28HpaVwvLCSouG4X6bABB7JgxxHESk5UfnrevlWfqRR4VGJ43ZAqNW4ruokLQRKOVnjiEDu7NucO/s2Z+3Og0zNzGdaZj73f5pFgO9K+rWOYWhqHH1aRuHvU8UvUy+vM1/GVfDzvHlkZGSAMXDy6K+PNCo7KjmYD7tWW/dPHDrHHsRqOzln0ShfYBpYeZTttBAoVYtaNQyh1cAQxlzakuVb9zMtK5/pK3bw1YodhAT4MCglliGpcXRtGoG3lxNO0YiAX6C1lJnRrUpKTlrFocKiUcGRyf4tZ7Y1pRW+5EW+obC7NzTuDo27QcP24O1bA3+oqg4tBErZwMtL6JTYgE6JDXhscBu+X7+HaZn5fJmVz4SftxEd7M8VqXEMTYsjJT4UcYXz9t6+EBRlLdVRWmodTZQvGocL2Ld8Jg13roQ1X1rb+gZCQqczhSGhM/jr9RnOpoVAKZv5enuR0TKajJbRHD1Rwuy1u5iWmc9/F2/h/UWbSIqsf7ooNIuqg3Mtezl6PwWEQnjiWavWHm1Jw4wMOLgDti2BrUtg62JY8KJ1FCFe1rzUpwpD4+4Q3NCWP8OdaSFQyoXU8/NmcPs4BrePo/DISb5etYOpmfm8PieXcbNzaRcfwtDUeAanxhIbWs/uuDUnJBbaXmUtAMcPQd7PZwrD8o/gx7esdeGJZxeGyBba0+kCaSFQykWFBvoysnNjRnZuzK6Dx5i+YgfTMrfz7Iw1/H3mGromNWBIajyDUhq63xSb/sHQ7GJrAat9YscKqyhsXQy5syDrE2tdvQaOouAoDLFpeh1FNWkhUKoOiAkJ4E89k/hTzyQ27TnMtMx8pmZt59EpK3liWjZ9WkQxJC2e/q2jCfRzw4+1ty8kpFvLRXdZvY32bnAUBsdRw7oZ1rY+ARDf6UxhaNTZOi2lKuWG/8co5d6SIutzb/9k7unXnFX5B5mWZXVH/W7NbgL9vLmkTQxD0+LolRyFr7vOrCYCkc2tpeON1mNFux1FwVEYFr0C5iVAIKbd2UcN1e0x5ea0EChVR4kI7eJDaRcfysMDW/HT5n1MzcxnZrbVrhAe6MuglFiGpsVT6gn99YOioc0QawE4cRjylp4pDFmfwM/vWutCG59dGKJaefS4TloIlHIDXl5Ct6YRdGsawVND2rIwt4Cpmfl8vnw743/cSoMAYfjRNQxJi6NNbIhrdEd1Nr/60LSPtQCUFMOu7DOFYdN8WDnRWhcQCo3OFAavc1197Ya0ECjlZvx8vOjXOoZ+rWM4cqKYWat38cHslby/aBNvL9hI8+gghqbGMSQtjiYR9e2OW3u8fc5MOtTtNqudYf/mM4Vh6xLI/QaAnuIDm8u2M3SBwAb25nciLQRKubFAPx+GpsUTeiCX1M4XMSN7B9My8xk7K4exs3JIbRTG0NQ4BqfGEh0cYHfc2iUCDZKsJe131mOH98C2H8n7fiKNzXZY/AZ8/6q1Lqr1mcLQuBuENXabbqtaCJTyEOH1/bi+axOu79qE/ANHmb4in6mZ+Tw9fTXPfLWai5pFMiQtjgFtGxJaz0OHeagfCa0uZ+PO+jTOyIATRyB/+ZkjhuzJsOzf1rYh8WcXhug2dXbgPacXAhHxBpYC240xg8utuxl4EdjueOifxpj3nJ1JKU8XF1aPW3o345bezVi/u4hpWdboqA9OWsFfp2TTt1UUQ9PiubhVNAG+dfPLrUb4BUJiT2sBKC2B3avPnE7astgqDgD+IdYppFPFIT7dmp2uDqiNI4J7gTVASCXrPzXG3FULOZRSFWgeHcSoS1pwf/9kVuQVMjUzny9X5PPNql0E+ftwadsYhqbF06NZBD7u2h21qry8rSEvGqZAl79Y7QyF285uZ5jzjGNbX6s94nQ7QzeoH2Fv/ko4tRCISAJwOfAsMMqZ+1JKXRgRIbVRGKmNwvi/y1uzZONepmXmMyN7B58v305kkB+Xp8QyJC2ejo3DPKPn0W8RsdoKwhpD+xHWY0f2OYbHcBSGH9+GH1631kW2OPt0UniSS7QziHFi/2IRmQQ8BwQDoys5NfQc1kyvOcD9xphtFbzOLcAtADExMekTJkw4rzxFRUUEBbneoF2umgtcN5vmqp4LyXWy1LCioIQlO4rJ3F3CyVKIrCd0i/WhW6wPCcHnf5Tgju9XeV4lJwg+tJ7QwtWEFq4h5OAafIutKUqP+4VTGNqawtA2FIa25nD9JMw52hkuJFffvn2XGWM6VbTOaYVARAYDg4wxd4hIBhUXggigyBhzXERuBUYaYy4+1+t26tTJLF269LwyzTs1OYeLcdVc4LrZNFf11FSuQ8dOMmv1LqZm5rNo/R5KSg2tGgYzJC2OK9rH0ahBoC25appTc5WWQsHaMsNjLIHCrdY63/rWkBinjhjiO4H/mS/+C8klIpUWAmeeGuoBDBGRQUAAECIi/zPG3HBqA2PM3jLbvwe84MQ8SqkLFBzgy9UdE7i6YwJ7iqx5madl5vPC1+t44et1pJeZlzny1LzM6mxeXhDTxlo6/8l6rDCvzPAYS2De84AB8YbY9qcLg+8J5/xwd1ohMMY8AjwCUOaI4Iay24hIrDFmh+PuEKxGZaVUHRAZ5M9N3RO5qXsi2/ZZ8zJPy8zn8amreOrL1fRsbs3LfGnbhgT5a0/1cwpNgJRh1gJwrBC2lWlnWPoBLHmTxglDgCtrfPe1/l9HRJ4GlhpjpgH3iMgQoBjYB9xc23mUUheuUYNA7shozh0Z1rzM0zLzmZaVz6iJWfj7rKR/6xiGpMWRUZ15mT1ZQCgk97cWgOITsCOL/JXraOSE3dVKITDGzAPmOW4/Xubx00cNSin3cHpe5gEtWb71ANMyt1vzMq/cQXCAD4PaxTIkLY5uTV2zK6VL8vGDRp05uuGwc17eKa+qlPJ4IkJ6k3DSm4Tz2OA2/LBhL1Mz8/lq5Q4+XWrNy9w6tIQjETvo0SyS0EAPvZrZBWghUEo5nY+3F71bRNG7RRTPnmzHnLW7mb4in7lrdjJ//HK8BFIbhdE7OYreLSJJTQjTi9dqkRYCpVStCvD1ZlBKLINSYpk9Zy5hzVKZn7OHhbkFvD4nl9dm5xIc4EOPZpH0bhFFr+TIandLVdWjhUApZRtvLyG9SQPSmzRg1CUtOHDkBD9s2MuCnAIW5BTw9aqdADSNrE+vZKswdGsaQX3thVSj9N1USrmMsEC/00cLxhg2FBxmQU4BC3MLmLg0j/8s3oKvt9X20LtFFL2To2gTG4KXl/3DNNRlWgiUUi5JRGgeHUTz6CD+2DOJ48UlLNu8n/m5BSzM2XP6IraI+n70TI6kd7J1Gik6xMPmVagBWgiUUnWCv483FzWP5KLmkTxyGew+dIzv1+9hgaN9YWpmPgCtGgafPlrolBju2cNoV5EWAqVUnRQdHMBVHRK4qkMCpaWGNTsPni4KH36/mXcWbCTA14uuSRH0So6kT4somkcH6aipFdBCoJSq87y8hLZxobSNC+X2jGYcOVHMjxv3Md/RvvDMV2t45qs1xIYG0Cs5kl7JUfRsHkl4fT+7o7sELQRKKbcT6OdD31bR9G0VDUDe/iMsyt3DgtwCvs7eycSleYhA+/hQRxfVKDo0DsPXQ69d0EKglHJ7CeGBXNulMdd2aUxJqSEr7wALc6zC8Oa8Dbw+Zz1B/j50bxZB7xZR+B8ptTtyrdJCoJTyKN5eQsfG4XRsHM69/ZMpPHqSxRv2sCB3DwtyCpi1ehcAb6yaa127kBxF92YRBAe47xAYWgiUUh4ttJ4vA9vFMrCdde3Cpj2H+WDGYnaYID5fvp3/LdmKj6N4nLqorV18KN5udO2CFgKllHIQEZpGBdG/iS8ZGZ05UVzKsi37WZhbwILcAsbOymHsrBzCA33p0fzMEBixofXsjn5BtBAopVQl/Hy86N4sgu7NInhwYCv2Fh1nkePahQW5BUxfYc2r1SImiF7J1qB6XZMa1LlrF7QQKKVUFUUE+TM0LZ6hafEYY1i785B1tJCzh/8u2cL7izbh5+NF16QGp08jtYwJdvlrF7QQKKXUeRARWseG0Do2hFt6N+PoiRJ+3LSXhY5G57/PWMvfZ6wlOtjfcbQQSc/mkUS44FzOWgiUUqoG1PPzJqNlNBktrWsXdhQePd1FdfbaXUxengdAu/gQx7hIUaQ3CcfPx/5rF7QQKKWUE8SG1mNE50aM6NyIklLDyu2FLMyxGp3fXrCRN+dtINDPm+5NI043OidF1rflNJIWAqWUcjJvLyGtURhpjcK4u18yh46d5IcNe0+3L8xeuxuAhPB69EqOok+LSLo3iyS0Xu1cu6CFQCmlallwgC8D2jZkQNuGAGzZa827sCB3D19m5fPJT1tPF4/eyVH0ckzf6SxaCJRSymZNIupzY/f63Ng9kZMlpfyy9YCjMBTw6uwcXvkuh5AAHy5r4kVGRs3vXwuBUkq5EF9vL7okNaBLUgNGD2jJvsMnWLR+DwtzCmhQXOCUfdrfXK2UUqpSDer7MSQ1jheHp9I11jm/3bUQKKWUh9NCoJRSHk4LgVJKeTgtBEop5eG0ECillIfTQqCUUh5OC4FSSnk4LQRKKeXhxBhjd4ZqEZECYMt5Pj0S2FODcWqKq+YC182muapHc1WPO+ZqYoyJqmhFnSsEF0JElhpjOtmdozxXzQWum01zVY/mqh5Py6WnhpRSysNpIVBKKQ/naYXgHbsDVMJVc4HrZtNc1aO5qsejcnlUG4FSSqlf87QjAqWUUuVoIVBKKQ/nloVARAaKyDoRWS8iD1ew3l9EPnWs/1FEEl0k180iUiAimY7lz7WU6wMR2S0i2ZWsFxEZ58i9QkQ6ukiuDBEpLPN+PV4LmRqJyFwRWS0iq0Tk3gq2qfX3q4q5av39cuw3QER+EpEsR7anKtim1j+TVcxl12fSW0R+EZHpFayr+ffKGONWC+ANbACaAn5AFtCm3DZ3AG85bl8LfOoiuW4G/mnDe9Yb6AhkV7J+EDATEKAb8KOL5MoAptfyexULdHTcDgZyKvjvWOvvVxVz1fr75divAEGO277Aj0C3ctvY8ZmsSi67PpOjgI8r+u/ljPfKHY8IugDrjTEbjTEngAnA0HLbDAX+47g9CegnIuICuWxhjFkA7DvHJkOBj4xlCRAmIrEukKvWGWN2GGOWO24fAtYA8eU2q/X3q4q5bOF4H4ocd30dS/leKrX+maxirlonIgnA5cB7lWxS4++VOxaCeGBbmft5/PoDcXobY0wxUAhEuEAugGscpxMmiUgjJ2eqqqpmt0N3x6H9TBFpW5s7dhySd8D6JVmWre/XOXKBTe+X41RHJrAbmGWMqfQ9q8XPZFVyQe1/Jl8FHgRKK1lf4++VOxaCuuxLINEY0x6YxZmqryq2HGv8lFTgdeCL2tqxiAQBk4H7jDEHa2u/v+U3ctn2fhljSowxaUAC0EVE2tXWvs+lCrlq9TMpIoOB3caYZc7cT3nuWAi2A2WrdoLjsQq3EREfIBTYa3cuY8xeY8xxx933gHQnZ6qqqryntc4Yc/DUob0xZgbgKyKRzt6viPhifdmON8Z8XsEmtrxfv5XLrverXIYDwFxgYLlVdnwmfzOXDZ/JHsAQEdmMdfr4YhH5X7ltavy9csdC8DOQLCJJIuKH1Zgyrdw204DfO24PA+YYR8uLnbnKnUcegnWe1xVMA25y9IbpBhQaY3bYHUpEGp46NyoiXbD+f3bql4djf+8Da4wxL1eyWa2/X1XJZcf75dhXlIiEOW7XAy4B1pbbrNY/k1XJVdufSWPMI8aYBGNMItZ3xBxjzA3lNqvx98rnQp7siowxxSJyF/ANVk+dD4wxq0TkaWCpMWYa1gfmvyKyHqsx8loXyXWPiAwBih25bnZ2LgAR+QSrR0mkiOQBT2A1nGGMeQuYgdUTZj1wBPiDi+QaBtwuIsXAUeDaWijoPYAbgZWOc8sAjwKNy+Sy4/2qSi473i+wejT9R0S8sYrPRGPMdLs/k1XMZctnsjxnv1c6xIRSSnk4dzw1pJRSqhq0ECillIfTQqCUUh5OC4FSSnk4LQRKKeXhtBAoVY6IlJQZbTJTKhgp9gJeO1EqGU1VKbu43XUEStWAo45hB5TyCHpEoFQVichmEXlBRFY6xrFv7ng8UUTmOAYmmy0ijR2Px4jIFMcgb1kicpHjpbxF5F2xxsD/1nFVq1K20UKg1K/VK3dqaGSZdYXGmBTgn1ijRII1gNt/HAOTjQfGOR4fB8x3DPLWEVjleDwZeMMY0xY4AFzj5L9HqXPSK4uVKkdEiowxQRU8vhm42Biz0THA205jTISI7AFijTEnHY/vMMZEikgBkFBm0LJTQ0TPMsYkO+4/BPgaY55x/l+mVMX0iECp6jGV3K6O42Vul6BtdcpmWgiUqp6RZf5d7Lj9A2cG/roeWOi4PRu4HU5PgBJaWyGVqg79JaLUr9UrM4InwNfGmFNdSMNFZAXWr/rfOR67G/i3iIwBCjgz2ui9wDsi8iesX/63A7YP361UedpGoFQVOdoIOhlj9tidRamapKeGlFLKw+kRgVJKeTg9IlBKKQ+nhUAppTycFgKllPJwWgiUUsrDaSFQSikP9/9ZB9HCdGP/KgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNv6RkD8tYhb"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSdUKxAOwwkv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AsQxpqntb_M"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DszQjY3WtdL0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}