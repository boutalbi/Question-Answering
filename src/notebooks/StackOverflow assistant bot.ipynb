{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "To detect intent of users questions we will need two text collections:\n",
    "\n",
    "- `tagged_posts.tsv` — StackOverflow posts, tagged with one programming language (positive samples).\n",
    "- `dialogues.tsv` — dialogue phrases from movie subtitles (negative samples).\n",
    "\n",
    "For those questions, that have programming-related intent, we will proceed as follow predict programming language (we allowed only one tag per question here) and rank candidates within the tag using embeddings. For the ranking part, we will need:\n",
    "\n",
    "- `word_embeddings.tsv` — word embeddings, that you trained with StarSpace in the 3rd assignment. It's not a problem if you didn't do it, because we can offer an alternative solution for you.\n",
    "\n",
    "As a result of this notebook, we should obtain the following new objects that we will then use in the running bot:\n",
    "\n",
    "- `intent_recognizer.pkl` — intent recognition model;\n",
    "- `tag_classifier.pkl` — programming language classification model;\n",
    "- `tfidf_vectorizer.pkl` — vectorizer used during training;\n",
    "- `thread_embeddings_by_tags` — folder with thread embeddings, arranged by tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./data'):\n",
    "    !mkdir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://github.com/hse-aml/natural-language-processing/releases/download/project/tagged_posts.tsv \\\n",
    "    -O ./data/tagged_posts.tsv > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://github.com/hse-aml/natural-language-processing/releases/download/project/dialogues.tsv \\\n",
    "    -O ./data/dialogues.tsv > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 781\n",
    "sample_size = 200000\n",
    "\n",
    "df_stackoverflow = pd.read_csv('./data/tagged_posts.tsv', sep='\\t').sample(sample_size, random_state=seed)\n",
    "df_dialogues = pd.read_csv('./data/dialogues.tsv', sep='\\t').sample(sample_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>631024</th>\n",
       "      <td>9071076</td>\n",
       "      <td>C++ virtual method overload/override compiler ...</td>\n",
       "      <td>c_cpp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353311</th>\n",
       "      <td>5298353</td>\n",
       "      <td>Check a condition and also identify the patter...</td>\n",
       "      <td>php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547617</th>\n",
       "      <td>23947511</td>\n",
       "      <td>isset($_POST['x']) only works if the submit bu...</td>\n",
       "      <td>php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70588</th>\n",
       "      <td>1353559</td>\n",
       "      <td>Trying to make this star output using a for lo...</td>\n",
       "      <td>c_cpp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534998</th>\n",
       "      <td>7753016</td>\n",
       "      <td>Django+Postgres: \"current transaction is abort...</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          post_id                                              title     tag\n",
       "631024    9071076  C++ virtual method overload/override compiler ...   c_cpp\n",
       "353311    5298353  Check a condition and also identify the patter...     php\n",
       "1547617  23947511  isset($_POST['x']) only works if the submit bu...     php\n",
       "70588     1353559  Trying to make this star output using a for lo...   c_cpp\n",
       "534998    7753016  Django+Postgres: \"current transaction is abort...  python"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stackoverflow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154349</th>\n",
       "      <td>What's that got to do with you?</td>\n",
       "      <td>dialogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105643</th>\n",
       "      <td>Nooo.  Is it your story?</td>\n",
       "      <td>dialogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122343</th>\n",
       "      <td>No Bela, that's \"incorporates.\"  Look, just sa...</td>\n",
       "      <td>dialogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183491</th>\n",
       "      <td>For getting a divorce?</td>\n",
       "      <td>dialogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129003</th>\n",
       "      <td>No danger of attack, as long as you don't trig...</td>\n",
       "      <td>dialogue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text       tag\n",
       "154349                    What's that got to do with you?  dialogue\n",
       "105643                           Nooo.  Is it your story?  dialogue\n",
       "122343  No Bela, that's \"incorporates.\"  Look, just sa...  dialogue\n",
       "183491                             For getting a divorce?  dialogue\n",
       "129003  No danger of attack, as long as you don't trig...  dialogue"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dialogues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Intent and language recognition\n",
    "\n",
    "We want to write a bot, which will not only **answer programming-related questions**, but also will be able to **maintain a dialogue**. We would also like to detect the *intent* of the user from the question (we could have had a 'Question answering mode' check-box in the bot, but it wouldn't fun at all). So the first thing we need to do is to **distinguish programming-related questions from general ones**.\n",
    "\n",
    "It would also be good to predict which programming language a particular question referees to. By doing so, we will speed up question search by a factor of the number of languages (10 here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text):\n",
    "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
    "\n",
    "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    bad_symbols_re = re.compile('[^0-9a-z #+_]')\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "    text = text.lower()\n",
    "    text = replace_by_space_re.sub(' ', text)\n",
    "    text = bad_symbols_re.sub('', text)\n",
    "    text = ' '.join([x for x in text.split() if x and x not in stopwords_set])\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 s, sys: 6.38 s, total: 1min 3s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_stackoverflow.title = df_stackoverflow.title.apply(text_prepare)\n",
    "df_dialogues.text = df_dialogues.text.apply(text_prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent recognition\n",
    "\n",
    "We will do a binary classification on TF-IDF representations of texts. Labels will be either `dialogue` for general questions or `stackoverflow` for programming-related questions. First, we prepare the data for this task:\n",
    "\n",
    "- concatenate dialogue and stackoverflow examples into one sample\n",
    "- split it into train and test in proportion 90/10 %, use random_state=0 for reproducibility\n",
    "- transform it into TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tfidf_features(X_train, X_test, to_='./out'):\n",
    "    if not os.path.exists(to_):\n",
    "        !mkdir {to_}\n",
    "    vect = TfidfVectorizer(min_df=5, max_df=0.9, ngram_range=(1, 2), token_pattern='(\\S+)')\n",
    "    vect.fit(X_train)\n",
    "    with open(os.path.join(to_, 'tfidf_vectorizer.pkl'), 'wb') as file:\n",
    "        pickle.dump(vect, file)\n",
    "    X_train = vect.transform(X_train)\n",
    "    X_test = vect.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size=360000, test size=40000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = np.concatenate([df_dialogues.text.values, df_stackoverflow.title.values])\n",
    "y = ['dialogue'] * df_dialogues.shape[0] + ['stackoverflow'] * df_stackoverflow.shape[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=seed)\n",
    "print(f'Train size={len(X_train)}, test size={len(X_test)}')\n",
    "\n",
    "X_train_tfidf, X_test_tfidf = extract_tfidf_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "intent_recognizer = LogisticRegression(penalty='l2', C=10, random_state=seed, solver='liblinear')\n",
    "intent_recognizer.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = intent_recognizer.predict(X_test_tfidf)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Test accuracy={test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(intent_recognizer, open('./out/intent_recognizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Language Classifcation\n",
    "\n",
    "We will train one more classifier for the programming-related questions. It will predict exactly one tag (=programming language) and will be also based on Logistic Regression with TF-IDF features.\n",
    "\n",
    "First, let us prepare the data for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_stackoverflow.title.values\n",
    "y = df_stackoverflow.tag.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "print(f'Train size={len(X_train)}, test size={len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = pickle.load(open('./out/tfidf_vectorizer.pkl', 'rb'))\n",
    "\n",
    "X_train_tfidf, X_test_tfidf = vectorizer.transform(X_train), vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tag_classifier = OneVsRestClassifier(LogisticRegression(penalty='l2', C=5, random_state=seed, solver='liblinear'))\n",
    "tag_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = tag_classifier.predict(X_test_tfidf)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Test accuracy={test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tag_classifier, open('./out/tag_classifier.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Ranking questions with embeddings\n",
    "\n",
    "To find a relevant answer (a thread from StackOverflow) on a question we will use vector representations to calculate similarity between the question and existing threads. We create `question_to_vec` function, which can make such a representation based on word vectors.\n",
    "\n",
    "However, it would be costly to compute such a representation for all possible answers in online mode of the bot (e.g. when bot is running and answering questions from many users). This is the reason why we will create a database with pre-computed representations. These representations will be arranged by non-overlaping tags (programming languages), so that the search of the answer can be performed only within one tag each time. This will make our bot even more efficient and allow not to store all the database in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    vec = []\n",
    "    for token in question.split():\n",
    "        if token in embeddings:\n",
    "            vec.append(embeddings[token])\n",
    "    if len(vec) == 0:\n",
    "        return np.zeros((dim,))\n",
    "    return np.stack(vec).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to precompute representations for all possible answers, we need to load the whole posts dataset, unlike we did for the intent classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.read_csv('./data/tagged_posts.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_tag = posts_df.groupby('tag').count().max(axis=1)\n",
    "counts_by_tag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each tag, we need to create two data structures, which will serve as online search index:\n",
    "\n",
    "- `tag_post_ids` — a list of post_ids with shape `(counts_by_tag[tag],)`. It will be needed to show the title and link to the thread;\n",
    "- `tag_vectors` — a matrix with shape `(counts_by_tag[tag], embeddings_dim)` where embeddings for each answer are stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./out/thread_embeddings_by_tags'):\n",
    "    !mkdir ./out/thread_embeddings_by_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag, count in tqdm.tqdm(counts_by_tag.items()):\n",
    "    tag_posts = posts_df[posts_df['tag'] == tag]\n",
    "    \n",
    "    tag_post_ids = tag_posts.post_id.tolist()\n",
    "    \n",
    "    tag_vectors = np.zeros((count, embeddings_dim), dtype=np.float32)\n",
    "    for i, title in enumerate(tag_posts['title']):\n",
    "        tag_vectors[i, :] = question_to_vec(title, w2v_embeddings, embeddings_dim)\n",
    "\n",
    "    # Dump post ids and vectors to a file.\n",
    "    filename = os.path.join('./out/thread_embeddings_by_tags', os.path.normpath('%s.pkl' % tag))\n",
    "    pickle.dump((tag_post_ids, tag_vectors), open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chatterbot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3747ba2fdfe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chatterbot'"
     ]
    }
   ],
   "source": [
    "import chatterbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chatterbot\n",
      "  Using cached ChatterBot-1.0.5-py2.py3-none-any.whl (67 kB)\n",
      "Collecting spacy<2.2,>=2.1\n",
      "  Using cached spacy-2.1.9-cp37-cp37m-manylinux1_x86_64.whl (30.8 MB)\n",
      "Requirement already satisfied: pint>=0.8.1 in /root/anaconda3/lib/python3.7/site-packages (from chatterbot) (0.11)\n",
      "Processing /root/.cache/pip/wheels/23/b9/73/57aaccb6957d94ed63f474b51a9f7f992c5eff4635052c0557/PyYAML-5.1.2-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: python-dateutil<2.8,>=2.7 in /root/anaconda3/lib/python3.7/site-packages (from chatterbot) (2.7.5)\n",
      "Requirement already satisfied: nltk<4.0,>=3.2 in /root/anaconda3/lib/python3.7/site-packages (from chatterbot) (3.4.5)\n",
      "Requirement already satisfied: pymongo<4.0,>=3.3 in /root/anaconda3/lib/python3.7/site-packages (from chatterbot) (3.10.1)\n",
      "Requirement already satisfied: pytz in /root/anaconda3/lib/python3.7/site-packages (from chatterbot) (2019.3)\n",
      "Requirement already satisfied: mathparse<0.2,>=0.1 in /root/anaconda3/lib/python3.7/site-packages (from chatterbot) (0.1.2)\n",
      "Requirement already satisfied: sqlalchemy<1.3,>=1.2 in /root/anaconda3/lib/python3.7/site-packages (from chatterbot) (1.2.19)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /root/anaconda3/lib/python3.7/site-packages (from spacy<2.2,>=2.1->chatterbot) (0.2.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /root/anaconda3/lib/python3.7/site-packages (from spacy<2.2,>=2.1->chatterbot) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /root/anaconda3/lib/python3.7/site-packages (from spacy<2.2,>=2.1->chatterbot) (1.18.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /root/anaconda3/lib/python3.7/site-packages (from spacy<2.2,>=2.1->chatterbot) (2.23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /root/anaconda3/lib/python3.7/site-packages (from spacy<2.2,>=2.1->chatterbot) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /root/anaconda3/lib/python3.7/site-packages (from spacy<2.2,>=2.1->chatterbot) (1.0.2)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /root/anaconda3/lib/python3.7/site-packages (from spacy<2.2,>=2.1->chatterbot) (2.0.1)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /root/anaconda3/lib/python3.7/site-packages (from spacy<2.2,>=2.1->chatterbot) (7.0.8)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /root/anaconda3/lib/python3.7/site-packages (from spacy<2.2,>=2.1->chatterbot) (0.9.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /root/anaconda3/lib/python3.7/site-packages (from spacy<2.2,>=2.1->chatterbot) (2.0.3)\n",
      "Requirement already satisfied: setuptools in /root/anaconda3/lib/python3.7/site-packages (from pint>=0.8.1->chatterbot) (46.1.1.post20200323)\n",
      "Requirement already satisfied: six>=1.5 in /root/anaconda3/lib/python3.7/site-packages (from python-dateutil<2.8,>=2.7->chatterbot) (1.14.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /root/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /root/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /root/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (2.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /root/anaconda3/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.8->spacy<2.2,>=2.1->chatterbot) (4.44.1)\n",
      "Installing collected packages: spacy, pyyaml, chatterbot\n",
      "Killed\n"
     ]
    }
   ],
   "source": [
    "!pip install chatterbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /root/anaconda3\n",
      "\n",
      "  removed specs:\n",
      "    - pyyaml\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2019.11.28         |           py37_1         156 KB\n",
      "    conda-4.8.3                |           py37_0         2.8 MB\n",
      "    decorator-4.4.2            |             py_0          14 KB\n",
      "    idna-2.9                   |             py_1          49 KB\n",
      "    ipython-7.13.0             |   py37h5ca1d4c_0         991 KB\n",
      "    jedi-0.16.0                |           py37_1         769 KB\n",
      "    json5-0.9.4                |             py_0          21 KB\n",
      "    jupyter_client-6.1.2       |             py_0          82 KB\n",
      "    jupyterlab_server-1.1.0    |             py_0          27 KB\n",
      "    openssl-1.1.1e             |       h7b6447c_0         2.5 MB\n",
      "    parso-0.6.2                |             py_0          70 KB\n",
      "    prompt-toolkit-3.0.4       |             py_0         244 KB\n",
      "    prompt_toolkit-3.0.4       |                0          11 KB\n",
      "    pycparser-2.20             |             py_0          92 KB\n",
      "    pygments-2.6.1             |             py_0         654 KB\n",
      "    requests-2.23.0            |           py37_0          92 KB\n",
      "    setuptools-46.1.1          |           py37_0         512 KB\n",
      "    tornado-6.0.4              |   py37h7b6447c_1         611 KB\n",
      "    tqdm-4.44.1                |             py_0          57 KB\n",
      "    wcwidth-0.1.9              |             py_0          24 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         9.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  prompt-toolkit     pkgs/main/noarch::prompt-toolkit-3.0.4-py_0\n",
      "\n",
      "The following packages will be REMOVED:\n",
      "\n",
      "  alabaster-0.7.12-py37_0\n",
      "  anaconda-2020.02-py37_0\n",
      "  anaconda-client-1.7.2-py37_0\n",
      "  anaconda-navigator-1.9.12-py37_0\n",
      "  anaconda-project-0.8.4-py_0\n",
      "  argh-0.26.2-py37_0\n",
      "  astroid-2.3.3-py37_0\n",
      "  astropy-4.0-py37h7b6447c_0\n",
      "  atomicwrites-1.3.0-py37_1\n",
      "  autopep8-1.4.4-py_0\n",
      "  babel-2.8.0-py_0\n",
      "  backports-1.0-py_2\n",
      "  backports.functools_lru_cache-1.6.1-py_0\n",
      "  backports.shutil_get_terminal_size-1.0.0-py37_2\n",
      "  backports.tempfile-1.0-py_1\n",
      "  backports.weakref-1.0.post1-py_1\n",
      "  beautifulsoup4-4.8.2-py37_0\n",
      "  bitarray-1.2.1-py37h7b6447c_0\n",
      "  bkcharts-0.2-py37_0\n",
      "  blas-1.0-mkl\n",
      "  blosc-1.16.3-hd408876_0\n",
      "  bokeh-1.4.0-py37_0\n",
      "  boto-2.49.0-py37_0\n",
      "  bottleneck-1.3.2-py37heb32a55_0\n",
      "  bzip2-1.0.8-h7b6447c_0\n",
      "  cairo-1.14.12-h8948797_3\n",
      "  click-7.0-py37_0\n",
      "  cloudpickle-1.3.0-py_0\n",
      "  clyent-1.2.2-py37_1\n",
      "  colorama-0.4.3-py_0\n",
      "  conda-build-3.18.11-py37_0\n",
      "  conda-verify-3.4.2-py_1\n",
      "  contextlib2-0.6.0.post1-py_0\n",
      "  curl-7.68.0-hbc83047_0\n",
      "  cycler-0.10.0-py37_0\n",
      "  cython-0.29.15-py37he6710b0_0\n",
      "  cytoolz-0.10.1-py37h7b6447c_0\n",
      "  dask-2.11.0-py_0\n",
      "  dask-core-2.11.0-py_0\n",
      "  diff-match-patch-20181111-py_0\n",
      "  distributed-2.11.0-py37_0\n",
      "  docutils-0.16-py37_0\n",
      "  et_xmlfile-1.0.1-py37_0\n",
      "  fastcache-1.1.0-py37h7b6447c_0\n",
      "  filelock-3.0.12-py_0\n",
      "  flake8-3.7.9-py37_0\n",
      "  flask-1.1.1-py_0\n",
      "  fribidi-1.0.5-h7b6447c_0\n",
      "  fsspec-0.6.2-py_0\n",
      "  future-0.18.2-py37_0\n",
      "  get_terminal_size-1.0.0-haa9412d_0\n",
      "  gevent-1.4.0-py37h7b6447c_0\n",
      "  glob2-0.7-py_0\n",
      "  gmpy2-2.0.8-py37h10f8cd9_2\n",
      "  graphite2-1.3.13-h23475e2_0\n",
      "  greenlet-0.4.15-py37h7b6447c_0\n",
      "  h5py-2.10.0-py37h7918eee_0\n",
      "  harfbuzz-1.8.8-hffaf4a1_0\n",
      "  hdf5-1.10.4-hb1b8bf9_0\n",
      "  heapdict-1.0.1-py_0\n",
      "  html5lib-1.0.1-py37_0\n",
      "  hypothesis-5.5.4-py_0\n",
      "  imageio-2.6.1-py37_0\n",
      "  imagesize-1.2.0-py_0\n",
      "  intel-openmp-2020.0-166\n",
      "  intervaltree-3.0.2-py_0\n",
      "  isort-4.3.21-py37_0\n",
      "  itsdangerous-1.1.0-py37_0\n",
      "  jbig-2.1-hdba287a_0\n",
      "  jdcal-1.4.1-py_0\n",
      "  jeepney-0.4.2-py_0\n",
      "  joblib-0.14.1-py_0\n",
      "  jupyter-1.0.0-py37_7\n",
      "  jupyter_console-6.1.0-py_0\n",
      "  keyring-21.1.0-py37_0\n",
      "  kiwisolver-1.1.0-py37he6710b0_0\n",
      "  krb5-1.17.1-h173b8e3_0\n",
      "  lazy-object-proxy-1.4.3-py37h7b6447c_0\n",
      "  libarchive-3.3.3-h5d8350f_5\n",
      "  libcurl-7.68.0-h20c2e04_0\n",
      "  libgfortran-ng-7.3.0-hdf63c60_0\n",
      "  liblief-0.9.0-h7725739_2\n",
      "  libspatialindex-1.9.3-he6710b0_0\n",
      "  libssh2-1.8.2-h1ba5d50_0\n",
      "  libtiff-4.1.0-h2733197_0\n",
      "  libtool-2.4.6-h7b6447c_5\n",
      "  libxslt-1.1.33-h7d1a2b0_0\n",
      "  llvmlite-0.31.0-py37hd408876_0\n",
      "  locket-0.2.0-py37_1\n",
      "  lxml-4.5.0-py37hefd8a0e_0\n",
      "  lz4-c-1.8.1.2-h14c3975_0\n",
      "  lzo-2.10-h49e0be7_2\n",
      "  matplotlib-3.1.3-py37_0\n",
      "  matplotlib-base-3.1.3-py37hef1b27d_0\n",
      "  mccabe-0.6.1-py37_1\n",
      "  mkl-2020.0-166\n",
      "  mkl-service-2.3.0-py37he904b0f_0\n",
      "  mkl_fft-1.0.15-py37ha843d7b_0\n",
      "  mkl_random-1.1.0-py37hd6b4f25_0\n",
      "  mock-4.0.1-py_0\n",
      "  more-itertools-8.2.0-py_0\n",
      "  mpc-1.1.0-h10f8cd9_1\n",
      "  mpfr-4.0.1-hdf1c602_3\n",
      "  mpmath-1.1.0-py37_0\n",
      "  msgpack-python-0.6.1-py37hfd86e86_1\n",
      "  multipledispatch-0.6.0-py37_0\n",
      "  networkx-2.4-py_0\n",
      "  nltk-3.4.5-py37_0\n",
      "  nose-1.3.7-py37_2\n",
      "  numba-0.48.0-py37h0573a6f_0\n",
      "  numexpr-2.7.1-py37h423224d_0\n",
      "  numpy-1.18.1-py37h4f9e942_0\n",
      "  numpy-base-1.18.1-py37hde5b4d6_1\n",
      "  numpydoc-0.9.2-py_0\n",
      "  olefile-0.46-py37_0\n",
      "  openpyxl-3.0.3-py_0\n",
      "  packaging-20.1-py_0\n",
      "  pandas-1.0.1-py37h0573a6f_0\n",
      "  pango-1.42.4-h049681c_0\n",
      "  partd-1.1.0-py_0\n",
      "  patchelf-0.10-he6710b0_0\n",
      "  path-13.1.0-py37_0\n",
      "  path.py-12.4.0-0\n",
      "  pathlib2-2.3.5-py37_0\n",
      "  pathtools-0.1.2-py_1\n",
      "  patsy-0.5.1-py37_0\n",
      "  pep8-1.7.1-py37_0\n",
      "  pillow-7.0.0-py37hb39fc2d_0\n",
      "  pixman-0.38.0-h7b6447c_0\n",
      "  pkginfo-1.5.0.1-py37_0\n",
      "  pluggy-0.13.1-py37_0\n",
      "  ply-3.11-py37_0\n",
      "  psutil-5.6.7-py37h7b6447c_0\n",
      "  py-1.8.1-py_0\n",
      "  py-lief-0.9.0-py37h7725739_2\n",
      "  pycodestyle-2.5.0-py37_0\n",
      "  pycrypto-2.6.1-py37h14c3975_9\n",
      "  pycurl-7.43.0.5-py37h1ba5d50_0\n",
      "  pydocstyle-4.0.1-py_0\n",
      "  pyflakes-2.1.1-py37_0\n",
      "  pylint-2.4.4-py37_0\n",
      "  pyodbc-4.0.30-py37he6710b0_0\n",
      "  pyparsing-2.4.6-py_0\n",
      "  pytables-3.6.1-py37h71ec239_0\n",
      "  pytest-5.3.5-py37_0\n",
      "  pytest-arraydiff-0.3-py37h39e3cac_0\n",
      "  pytest-astropy-0.8.0-py_0\n",
      "  pytest-astropy-header-0.1.2-py_0\n",
      "  pytest-doctestplus-0.5.0-py_0\n",
      "  pytest-openfiles-0.4.0-py_0\n",
      "  pytest-remotedata-0.3.2-py37_0\n",
      "  python-jsonrpc-server-0.3.4-py_0\n",
      "  python-language-server-0.31.7-py37_0\n",
      "  python-libarchive-c-2.8-py37_13\n",
      "  pytz-2019.3-py_0\n",
      "  pywavelets-1.1.1-py37h7b6447c_0\n",
      "  pyxdg-0.26-py_0\n",
      "  pyyaml-5.3-py37h7b6447c_0\n",
      "  qdarkstyle-2.8-py_0\n",
      "  qtawesome-0.6.1-py_0\n",
      "  qtconsole-4.6.0-py_1\n",
      "  ripgrep-11.0.2-he32d670_0\n",
      "  rope-0.16.0-py_0\n",
      "  rtree-0.9.3-py37_0\n",
      "  scikit-image-0.16.2-py37h0573a6f_0\n",
      "  scikit-learn-0.22.1-py37hd81dba3_0\n",
      "  scipy-1.4.1-py37h0b6359f_0\n",
      "  seaborn-0.10.0-py_0\n",
      "  secretstorage-3.1.2-py37_0\n",
      "  simplegeneric-0.8.1-py37_2\n",
      "  singledispatch-3.4.0.3-py37_0\n",
      "  snappy-1.1.7-hbae5bb6_3\n",
      "  snowballstemmer-2.0.0-py_0\n",
      "  sortedcollections-1.1.2-py37_0\n",
      "  sortedcontainers-2.1.0-py37_0\n",
      "  soupsieve-1.9.5-py37_0\n",
      "  sphinx-2.4.0-py_0\n",
      "  sphinxcontrib-1.0-py37_1\n",
      "  sphinxcontrib-applehelp-1.0.1-py_0\n",
      "  sphinxcontrib-devhelp-1.0.1-py_0\n",
      "  sphinxcontrib-htmlhelp-1.0.2-py_0\n",
      "  sphinxcontrib-jsmath-1.0.1-py_0\n",
      "  sphinxcontrib-qthelp-1.0.2-py_0\n",
      "  sphinxcontrib-serializinghtml-1.1.3-py_0\n",
      "  sphinxcontrib-websupport-1.2.0-py_0\n",
      "  spyder-4.0.1-py37_0\n",
      "  spyder-kernels-1.8.1-py37_0\n",
      "  sqlalchemy-1.3.13-py37h7b6447c_0\n",
      "  statsmodels-0.11.0-py37h7b6447c_0\n",
      "  sympy-1.5.1-py37_0\n",
      "  tbb-2020.0-hfd86e86_0\n",
      "  tblib-1.6.0-py_0\n",
      "  toolz-0.10.0-py_0\n",
      "  ujson-1.35-py37h14c3975_0\n",
      "  unicodecsv-0.14.1-py37_0\n",
      "  unixodbc-2.3.7-h14c3975_0\n",
      "  watchdog-0.10.2-py37_0\n",
      "  werkzeug-1.0.0-py_0\n",
      "  wrapt-1.11.2-py37h7b6447c_0\n",
      "  wurlitzer-2.0.0-py37_0\n",
      "  xlrd-1.2.0-py37_0\n",
      "  xlsxwriter-1.2.7-py_0\n",
      "  xlwt-1.3.0-py37_0\n",
      "  xmltodict-0.12.0-py_0\n",
      "  yapf-0.28.0-py_0\n",
      "  zict-1.0.0-py_0\n",
      "  zstd-1.3.7-h0b5b093_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  certifi                                 2019.11.28-py37_0 --> 2019.11.28-py37_1\n",
      "  conda                                        4.8.2-py37_0 --> 4.8.3-py37_0\n",
      "  decorator                                      4.4.1-py_0 --> 4.4.2-py_0\n",
      "  idna                  pkgs/main/linux-64::idna-2.8-py37_0 --> pkgs/main/noarch::idna-2.9-py_1\n",
      "  ipython                             7.12.0-py37h5ca1d4c_0 --> 7.13.0-py37h5ca1d4c_0\n",
      "  jedi                                        0.14.1-py37_0 --> 0.16.0-py37_1\n",
      "  json5                                          0.9.1-py_0 --> 0.9.4-py_0\n",
      "  jupyter_client     pkgs/main/linux-64::jupyter_client-5.~ --> pkgs/main/noarch::jupyter_client-6.1.2-py_0\n",
      "  jupyterlab_server                              1.0.6-py_0 --> 1.1.0-py_0\n",
      "  openssl                                 1.1.1d-h7b6447c_4 --> 1.1.1e-h7b6447c_0\n",
      "  parso                                          0.5.2-py_0 --> 0.6.2-py_0\n",
      "  prompt_toolkit                                 3.0.3-py_0 --> 3.0.4-0\n",
      "  pycparser          pkgs/main/linux-64::pycparser-2.19-py~ --> pkgs/main/noarch::pycparser-2.20-py_0\n",
      "  pygments                                       2.5.2-py_0 --> 2.6.1-py_0\n",
      "  requests                                    2.22.0-py37_1 --> 2.23.0-py37_0\n",
      "  setuptools                                  45.2.0-py37_0 --> 46.1.1-py37_0\n",
      "  tornado                              6.0.3-py37h7b6447c_3 --> 6.0.4-py37h7b6447c_1\n",
      "  tqdm                                          4.42.1-py_0 --> 4.44.1-py_0\n",
      "  wcwidth                                        0.1.8-py_0 --> 0.1.9-py_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json5-0.9.4          | 21 KB     | ##################################### | 100% \n",
      "wcwidth-0.1.9        | 24 KB     | ##################################### | 100% \n",
      "jedi-0.16.0          | 769 KB    | ##################################### | 100% \n",
      "decorator-4.4.2      | 14 KB     | ##################################### | 100% \n",
      "pycparser-2.20       | 92 KB     | ##################################### | 100% \n",
      "idna-2.9             | 49 KB     | ##################################### | 100% \n",
      "openssl-1.1.1e       | 2.5 MB    | ##################################### | 100% \n",
      "requests-2.23.0      | 92 KB     | ##################################### | 100% \n",
      "jupyter_client-6.1.2 | 82 KB     | ##################################### | 100% \n",
      "tqdm-4.44.1          | 57 KB     | ##################################### | 100% \n",
      "jupyterlab_server-1. | 27 KB     | ##################################### | 100% \n",
      "pygments-2.6.1       | 654 KB    | ##################################### | 100% \n",
      "setuptools-46.1.1    | 512 KB    | ##################################### | 100% \n",
      "prompt-toolkit-3.0.4 | 244 KB    | ##################################### | 100% \n",
      "parso-0.6.2          | 70 KB     | ##################################### | 100% \n",
      "conda-4.8.3          | 2.8 MB    | ##################################### | 100% \n",
      "prompt_toolkit-3.0.4 | 11 KB     | ##################################### | 100% \n",
      "certifi-2019.11.28   | 156 KB    | ##################################### | 100% \n",
      "tornado-6.0.4        | 611 KB    | ##################################### | 100% \n",
      "ipython-7.13.0       | 991 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda remove PyYAML -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
