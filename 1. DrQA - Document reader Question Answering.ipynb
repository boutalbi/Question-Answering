{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. DrQA - Document reader Question Answering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNuQVFcQFmYjRAv9fRIamyp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dksifoua/Question-Answering/blob/master/1.%20DrQA%20-%20Document%20reader%20Question%20Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwsiprY7pMVw",
        "outputId": "9bb675c3-8f83-4a83-e8b9-1039bed4d7aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Oct 18 16:57:21 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h9hVHEzrilz"
      },
      "source": [
        "## Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F9JhdaNrVUs"
      },
      "source": [
        "!pip install tqdm --upgrade >> /dev/null 2>&1\n",
        "!pip install torchtext --upgrade >> /dev/null 2>&1\n",
        "!pip install spacy --upgrade >> /dev/null 2>&1\n",
        "!python -m spacy download en >> /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLfWd1KErkbM"
      },
      "source": [
        "import re\n",
        "import json\n",
        "import tqdm\n",
        "import spacy\n",
        "import warnings\n",
        "import collections\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.data import Dataset, Example, Field\n",
        "from torchtext.data.iterator import BucketIterator"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atwALYPxr89L",
        "outputId": "1e11d139-b595-4ea1-d149-cfd0ba1070ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "\n",
        "SEED = 546\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BauDCqB6sI0t"
      },
      "source": [
        "## Prepare Data\n",
        "\n",
        "***Donwload data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g2TLNTyr9YE",
        "outputId": "78c20cc2-0a80-44ce-ccf9-160cd6a8dcf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "%%time\n",
        "!mkdir -p ./data\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://worksheets.codalab.org/rest/bundles/0x7e0a0a21057c4d989aa68da42886ceb9/contents/blob/ \\\n",
        "    -O ./data/train.json\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://worksheets.codalab.org/rest/bundles/0x8f29fe78ffe545128caccab74eb06c57/contents/blob/ \\\n",
        "    -O ./data/valid.json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-18 12:45:52--  https://worksheets.codalab.org/rest/bundles/0x7e0a0a21057c4d989aa68da42886ceb9/contents/blob/\n",
            "Resolving worksheets.codalab.org (worksheets.codalab.org)... 40.114.41.203\n",
            "Connecting to worksheets.codalab.org (worksheets.codalab.org)|40.114.41.203|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Syntax error in Set-Cookie: codalab_session=\"\"; expires=Thu, 01 Jan 1970 00:00:00 GMT; Max-Age=-1; Path=/ at position 70.\n",
            "Length: unspecified [application/json]\n",
            "Saving to: ‘./data/train.json’\n",
            "\n",
            "./data/train.json       [       <=>          ]  28.88M  22.9MB/s    in 1.3s    \n",
            "\n",
            "2020-10-18 12:45:54 (22.9 MB/s) - ‘./data/train.json’ saved [30288272]\n",
            "\n",
            "--2020-10-18 12:45:54--  https://worksheets.codalab.org/rest/bundles/0x8f29fe78ffe545128caccab74eb06c57/contents/blob/\n",
            "Resolving worksheets.codalab.org (worksheets.codalab.org)... 40.114.41.203\n",
            "Connecting to worksheets.codalab.org (worksheets.codalab.org)|40.114.41.203|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Syntax error in Set-Cookie: codalab_session=\"\"; expires=Thu, 01 Jan 1970 00:00:00 GMT; Max-Age=-1; Path=/ at position 70.\n",
            "Length: unspecified [application/json]\n",
            "Saving to: ‘./data/valid.json’\n",
            "\n",
            "./data/valid.json       [   <=>              ]   4.63M  9.71MB/s    in 0.5s    \n",
            "\n",
            "2020-10-18 12:45:55 (9.71 MB/s) - ‘./data/valid.json’ saved [4854279]\n",
            "\n",
            "CPU times: user 26.1 ms, sys: 20.5 ms, total: 46.6 ms\n",
            "Wall time: 3.16 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4NvE3J_sRgM"
      },
      "source": [
        "***Load data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igx9jwwisO4a"
      },
      "source": [
        "def load_json(path):\n",
        "    with open(path, mode='r', encoding='utf-8') as file:\n",
        "        return json.load(file)['data']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0ykhvuHsTwD",
        "outputId": "aed3e446-bd01-4de7-94f7-4c3e1e5e2eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_raw_data = load_json(path='./data/train.json')\n",
        "valid_raw_data = load_json(path='./data/valid.json')\n",
        "print(f'Length of raw train data: {len(train_raw_data):,}')\n",
        "print(f'Length of raw valid data: {len(valid_raw_data):,}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of raw train data: 442\n",
            "Length of raw valid data: 48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzmme91esbmW"
      },
      "source": [
        "***Parse data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L4v8MI2sZJ6"
      },
      "source": [
        "def parse_json(data):\n",
        "    qas = []\n",
        "    for paragraphs in data:\n",
        "        for para in paragraphs['paragraphs']:\n",
        "            for qa in para['qas']:\n",
        "                for ans in qa['answers']:\n",
        "                    qas.append({\n",
        "                        'id': qa['id'],\n",
        "                        'context': para['context'],\n",
        "                        'question': qa['question'],\n",
        "                        'answer': ans['text'],\n",
        "                        'answer_start': ans['answer_start'],\n",
        "                    })\n",
        "    return qas"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO8fIMPHso_k",
        "outputId": "fb9c3cfb-aa98-4244-bfdb-034e0a117b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "train_qas = parse_json(train_raw_data)\n",
        "valid_qas = parse_json(valid_raw_data)\n",
        "print(f'Length of train qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs: {len(valid_qas):,}')\n",
        "\n",
        "print('Id:', train_qas[0]['id'])\n",
        "print('Context:', train_qas[0]['context'])\n",
        "print('Question:', train_qas[0]['question'])\n",
        "print('Answer starts at:', train_qas[0]['answer_start'])\n",
        "print('Answer:', train_qas[0]['answer'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs: 87,599\n",
            "Length of valid qa pairs: 34,726\n",
            "Id: 5733be284776f41900661182\n",
            "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Answer starts at: 515\n",
            "Answer: Saint Bernadette Soubirous\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmDA7j8NtB3v"
      },
      "source": [
        "for i in range(len(train_qas)): # Test answer_start are correct in train set\n",
        "    assert train_qas[i]['answer'] == train_qas[i]['context'][train_qas[i]['answer_start']:train_qas[i]['answer_start'] + len(train_qas[i]['answer'])]\n",
        "\n",
        "for i in range(len(valid_qas)): # Test answer_start are correct in validation set\n",
        "    assert valid_qas[i]['answer'] == valid_qas[i]['context'][valid_qas[i]['answer_start']:valid_qas[i]['answer_start'] + len(valid_qas[i]['answer'])]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDmhP4kUvvD9"
      },
      "source": [
        "***Add targets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0x5HTy7tRlT"
      },
      "source": [
        "def add_targets(qas, nlp=spacy.load('en')):\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "        answer = nlp(qa['answer'], disable=['parser','tagger','ner'])\n",
        "        ans_start = qa['answer_start']\n",
        "        for i in range(len(context)):\n",
        "            if context[i].idx == ans_start:\n",
        "                ans = context[i:i + len(answer)]\n",
        "                qa['target'] = (ans[0].i, ans[-1].i)\n",
        "                break"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Yaahbfv4RQ",
        "outputId": "4c707d5a-7f43-435e-a647-9b43bac2f096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "add_targets(train_qas)\n",
        "add_targets(valid_qas)\n",
        "print()\n",
        "print(f'Length of train qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs: {len(valid_qas):,}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 87599/87599 [01:23<00:00, 1045.97it/s]\n",
            "100%|██████████| 34726/34726 [00:35<00:00, 977.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Length of train qa pairs: 87,599\n",
            "Length of valid qa pairs: 34,726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqv15GeYv_a3"
      },
      "source": [
        "def filter_qas(qa, nlp=spacy.load('en')):\n",
        "    if 'target' in [*qa.keys()]:\n",
        "        context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "        start, end = qa['target']\n",
        "        return context[start:end+1].text == qa['answer']\n",
        "    return False"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvu5BeNWwC6R",
        "outputId": "f547b537-b798-4585-9d74-c666873b44c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%%time\n",
        "train_qas = [*filter(filter_qas, train_qas)]\n",
        "valid_qas = [*filter(filter_qas, valid_qas)]\n",
        "print(f'Length of train qa pairs after filtering out bad qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs after filtering out bad qa pairs: {len(valid_qas):,}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs after filtering out bad qa pairs: 86,597\n",
            "Length of valid qa pairs after filtering out bad qa pairs: 34,295\n",
            "CPU times: user 1min 44s, sys: 78.3 ms, total: 1min 44s\n",
            "Wall time: 1min 44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FiohtqEwIww"
      },
      "source": [
        "def test_targets(qas, nlp=spacy.load('en')):\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        if 'target' in [*qa.keys()]:\n",
        "            context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "            start, end = qa['target']\n",
        "            assert context[start:end + 1].text == qa['answer']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb7D_8RAwLka",
        "outputId": "dddf300b-5e59-480b-c049-812881dcc6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_targets(train_qas)\n",
        "test_targets(valid_qas)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 86597/86597 [01:20<00:00, 1069.70it/s]\n",
            "100%|██████████| 34295/34295 [00:33<00:00, 1012.24it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwLcpPyEx9h1"
      },
      "source": [
        "***Add features***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSY0WXF9yAe0"
      },
      "source": [
        "def add_features(qas, nlp=spacy.load('en')):\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        question = [token.text for token in nlp(qa['question'], disable=['parser','tagger','ner'])]\n",
        "        context = nlp(qa['context'], disable=['parser'])\n",
        "        cxt_word2count = collections.Counter([token.text for token in context])\n",
        "        cxt_norm = sum(cxt_word2count.values())\n",
        "        qa['em'], qa['pos'], qa['ner'], qa['tf'] = zip(*map(\n",
        "            lambda token: (\n",
        "                1 if token.text in question else 0, token.tag_,\n",
        "                token.ent_type_ if token.ent_type_ != '' else 'None',\n",
        "                cxt_word2count[token.text] / cxt_norm\n",
        "            ), context\n",
        "        ))"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHLsz3Qh1p81",
        "outputId": "3c44e829-419d-4b7b-d95b-07f2f2f68729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "add_features(train_qas)\n",
        "add_features(valid_qas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  5%|▍         | 4290/86597 [01:02<17:50, 76.91it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnmJYtVBwRMz"
      },
      "source": [
        "***Build datasets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tukx3UoIwQzg"
      },
      "source": [
        "ID = Field(tokenize=None, batch_first=True)\n",
        "EM = Field(use_vocab=False, batch_first=True)\n",
        "POS = Field(tokenize=None, batch_first=True)\n",
        "NER = Field(tokenize=None, batch_first=True)\n",
        "TF = Field(pad_token=0, use_vocab=False, batch_first=True)\n",
        "TEXT = Field(lower=True, tokenizer_language='en', tokenize='spacy', batch_first=True)\n",
        "TARGET = Field(sequential=False, use_vocab=False, batch_first=True)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj5cpRtC3ucE",
        "outputId": "2bdefb43-d546-46c8-c38b-83070bf6f221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "train_dataset = Dataset([Example.fromdict(data=qa, fields={\n",
        "    'id': ('id', ID),\n",
        "    'context': ('cxt', TEXT),\n",
        "    'question': ('qst', TEXT),\n",
        "    'answer': ('ans', TEXT),\n",
        "    'em': ('em', EM),\n",
        "    'pos': ('pos', POS),\n",
        "    'ner': ('ner', NER),\n",
        "    'tf': ('tf', TF)\n",
        "}) for qa in tqdm.tqdm(train_qas)], fields={\n",
        "    'id': ID,\n",
        "    'cxt': TEXT,\n",
        "    'qst': TEXT,\n",
        "    'ans': TEXT,\n",
        "    'em': EM,\n",
        "    'pos': POS,\n",
        "    'ner': NER,\n",
        "    'tf': TF\n",
        "})\n",
        "\n",
        "valid_dataset = Dataset([Example.fromdict(data=qa, fields={\n",
        "    'id': ('id', ID),\n",
        "    'context': ('cxt', TEXT),\n",
        "    'question': ('qst', TEXT),\n",
        "    'answer': ('ans', TEXT),\n",
        "    'em': ('em', EM),\n",
        "    'pos': ('pos', POS),\n",
        "    'ner': ('ner', NER),\n",
        "    'tf': ('tf', TF)\n",
        "}) for qa in tqdm.tqdm(valid_qas)], fields={\n",
        "    'id': ID,\n",
        "    'cxt': TEXT,\n",
        "    'qst': TEXT,\n",
        "    'ans': TEXT,\n",
        "    'em': EM,\n",
        "    'pos': POS,\n",
        "    'ner': NER,\n",
        "    'tf': TF\n",
        "})\n",
        "print()\n",
        "print(f'Length of train dataset: {len(train_dataset.examples):,}')\n",
        "print(f'Length of valid dataset: {len(valid_dataset.examples):,}')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 86597/86597 [02:03<00:00, 698.73it/s]\n",
            "100%|██████████| 34295/34295 [00:50<00:00, 682.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Length of train dataset: 86,597\n",
            "Length of valid dataset: 34,295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVG7DeCS5VDL",
        "outputId": "376a145c-a834-4b59-b116-ab8b861e498c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "ID.build_vocab([*map(lambda x: x.id, train_dataset.examples)] + [*map(lambda x: x.id, valid_dataset.examples)])\n",
        "TEXT.build_vocab(train_dataset)\n",
        "POS.build_vocab(train_dataset)\n",
        "NER.build_vocab(train_dataset)\n",
        "print(f'Length of ID vocabulary: {len(ID.vocab):,}')\n",
        "print(f'Length of TEXT vocabulary: {len(TEXT.vocab):,}')\n",
        "print(f'Length of POS vocabulary: {len(POS.vocab):,}')\n",
        "print(f'Length of NER vocabulary: {len(NER.vocab):,}')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of ID vocabulary: 97,108\n",
            "Length of TEXT vocabulary: 91,446\n",
            "Length of POS vocabulary: 52\n",
            "Length of NER vocabulary: 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b-2EALviYnU",
        "outputId": "fceaf609-4b50-4e5c-fc90-9df91abb3fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "train_iterator, valid_iterator =  BucketIterator.splits((train_dataset, valid_dataset), batch_size=32, sort=False, device=DEVICE)\n",
        "for batch in train_iterator:\n",
        "    print(batch.tf)\n",
        "    break"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-c6c4d5bab248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mBucketIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \"\"\"\n\u001b[1;32m    233\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type str)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3tghw_a7-wv",
        "outputId": "4b226412-650c-423d-b9bd-abc3373e55a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_dataset.examples[1].pos"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('RB',\n",
              " ',',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " '.',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'POS',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " '.',\n",
              " 'RB',\n",
              " 'IN',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'CC',\n",
              " 'VBG',\n",
              " 'PRP',\n",
              " ',',\n",
              " 'VBZ',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " 'IN',\n",
              " 'NNS',\n",
              " 'VBN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " '``',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NNPS',\n",
              " \"''\",\n",
              " '.',\n",
              " 'RB',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'VBZ',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " '.',\n",
              " 'RB',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " ',',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NN',\n",
              " 'CC',\n",
              " 'NN',\n",
              " '.',\n",
              " 'PRP',\n",
              " 'VBZ',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " ',',\n",
              " 'NNP',\n",
              " 'WRB',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'RB',\n",
              " 'VBD',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'IN',\n",
              " 'CD',\n",
              " '.',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " '-LRB-',\n",
              " 'CC',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'WDT',\n",
              " 'VBZ',\n",
              " 'IN',\n",
              " 'CD',\n",
              " 'NNS',\n",
              " 'CC',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " '-RRB-',\n",
              " ',',\n",
              " 'VBZ',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " ',',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " '.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4sUa7385vZ-"
      },
      "source": [
        "***Download pretrained GloVe embedding***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0j7q5s55xX4",
        "outputId": "5341d696-b241-40a1-b726-a0cd1f50009a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "%%time\n",
        "!wget --no-check-certificate \\\n",
        "    http://nlp.stanford.edu/data/glove.840B.300d.zip \\\n",
        "    -O ./data/glove.840B.300d.zip\n",
        "!unzip -q ./data/glove.840B.300d.zip -d ./data\n",
        "!rm -r ./data/glove.840B.300d.zip"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-18 14:21:28--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2020-10-18 14:21:28--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2020-10-18 14:21:28--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘./data/glove.840B.300d.zip’\n",
            "\n",
            "./data/glove.840B.3 100%[===================>]   2.03G  2.07MB/s    in 16m 54s \n",
            "\n",
            "2020-10-18 14:38:22 (2.05 MB/s) - ‘./data/glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n",
            "CPU times: user 2.22 s, sys: 571 ms, total: 2.79 s\n",
            "Wall time: 18min 48s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKGPiljI55eO"
      },
      "source": [
        "def load_glove(path):\n",
        "    glove = {}\n",
        "    with open(path, mode='r', encoding='utf-8') as file:\n",
        "        for line in tqdm.tqdm(file):\n",
        "            values = line.split(' ')\n",
        "            glove[values[0]] = np.asarray(values[1:], dtype='float32')\n",
        "        return glove"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO2dEmr258Mg",
        "outputId": "9e019aad-6883-4211-a560-8eed08290e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "glove = load_glove(path='./data/glove.840B.300d.txt')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2196017it [02:36, 14048.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 25s, sys: 5.33 s, total: 2min 30s\n",
            "Wall time: 2min 36s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGJghoCy5-8x"
      },
      "source": [
        "def load_embeddings(glove, field, embedding_size=300, most_common=1000):\n",
        "    most_common_words, most_common_indexes = [*map(lambda x: x[0], field.vocab.freqs.most_common(most_common))], []\n",
        "    embedding_matrix = np.zeros((len(field.vocab), embedding_size))\n",
        "    n_words = 0\n",
        "    for index, word in tqdm.tqdm(enumerate(field.vocab.freqs), total=len(field.vocab)):\n",
        "        if word in most_common_words:\n",
        "            most_common_indexes.append(index)\n",
        "        try:\n",
        "            embedding_matrix[index] = glove[word]\n",
        "            n_words += 1\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return embedding_matrix, n_words, most_common_indexes"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SrTMgXs6BLH",
        "outputId": "6c6c90e4-b94b-4e54-e505-6511d7b4b300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "embedding_matrix, n_words, most_common_indexes = load_embeddings(glove, TEXT)\n",
        "print(f'\\nWords found: {n_words}/{len(TEXT.vocab)}')\n",
        "np.save('./data/GloVe_DrQA.npy', embedding_matrix)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 91444/91446 [00:01<00:00, 53529.53it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Words found: 64754/91446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlaFbXfvPoB5"
      },
      "source": [
        "# Free up the RAM\n",
        "del glove\n",
        "del embedding_matrix"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o2SPOFH6E9d"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "***Stacked Bidirectional LSTM Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdYGHa6t6JLy"
      },
      "source": [
        "class StackedBiLSTMsLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, n_layers, dropout):\n",
        "        super(StackedBiLSTMsLayer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.lstms = nn.ModuleList([nn.LSTM(input_size if i == 0 else hidden_size * 2, hidden_size,\n",
        "                                            num_layers=n_layers, bidirectional=True) for i in range(n_layers)])\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, seq_len, input_size] inputs\n",
        "        :return Tensor[batch_size, seq_len, hidden_size * n_layers * 2]\n",
        "        \"\"\"\n",
        "        outputs = [inputs]\n",
        "        for lstm in self.lstms:\n",
        "            out, _ = lstm(self.dropout(outputs[-1])) # [batch_size, seq_len, hidden_size * 2]\n",
        "            outputs.append(out)\n",
        "        return self.dropout(torch.cat(outputs[1:], dim=-1))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hA_h6tl-JlV"
      },
      "source": [
        "***Aligned Question Embedding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDF-OkXw-PaK"
      },
      "source": [
        "class AlignQuestionEmbeddingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AlignQuestionEmbeddingLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, cxt_embed, qst_embed, qst_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len, embedding_size] cxt_embded\n",
        "        :param Tensor[batch_size, qst_len, embedding_size] qst_embed\n",
        "        :param Tensor[batch_size, qst_len] qst_mask\n",
        "        :return Tensor[batch_size, cxt_len, hidden_size]\n",
        "        \"\"\"\n",
        "        cxt_embed = F.relu(self.linear(cxt_embed)) # [batch_size, cxt_len, hidden_size]\n",
        "        qst_embed = F.relu(self.linear(qst_embed)) # [batch_size, qst_len, hidden_size]\n",
        "        scores = torch.bmm(cxt_embed, qst_embed.transpose(-1, -2)) # [batch_size, cxt_len, qst_len]\n",
        "        scores = scores.masked_fill(qst_mask.unsqueeze(1) == 0, 1e-18)\n",
        "        attention_weights = F.softmax(scores, dim=-1) # [batch_size, cxt_len, qst_len]\n",
        "        return torch.bmm(attention_weights, qst_embed)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDgZjcbkBtAc"
      },
      "source": [
        "***Question Encoding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGe68fl-Bsjh"
      },
      "source": [
        "class QuestionEncodingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, dropout, n_layers):\n",
        "        super(QuestionEncodingLayer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.stacked_bilstms_layer = StackedBiLSTMsLayer(input_size=input_size, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "    \n",
        "    def linear_self_attention(self, qst_embed, qst_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, qst_len, embedding_size] qst_embed\n",
        "        :param Tensor[batch_size, qst_len] qst_mask\n",
        "        :return Tensor[batch_size, qst_len]\n",
        "        \"\"\"\n",
        "        scores = self.linear(qst_embed).squeeze(-1) # [batch_size, qst_len]\n",
        "        scores = scores.masked_fill(qst_mask == 0, 1e-18)\n",
        "        return F.softmax(scores, dim=-1)\n",
        "\n",
        "    \n",
        "    def forward(self, qst_embed, qst_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, qst_len, embedding_size] qst_embed\n",
        "        :param Tensor[batch_size, qst_len] qst_mask\n",
        "        :return Tensor[batch_size, hidden_size * n_layers * 2]\n",
        "        \"\"\"\n",
        "        attention_weights = self.linear_self_attention(qst_embed=qst_embed, qst_mask=qst_mask) # [batch_size, qst_len]\n",
        "        lstm_outputs = self.stacked_bilstms_layer(inputs=qst_embed) # [batch_size, qst_len, hidden_size * n_layers * 2]\n",
        "        return torch.bmm(attention_weights.unsqueeze(1), lstm_outputs).squeeze(1)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y9icfoaFo9x"
      },
      "source": [
        "***BiLinear Attention Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtEJqs2VFocN"
      },
      "source": [
        "class BiLinearAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, cxt_size, qst_size):\n",
        "        super(BiLinearAttentionLayer, self).__init__()\n",
        "        self.cxt_size = cxt_size\n",
        "        self.qst_size = qst_size\n",
        "        self.linear = nn.Linear(qst_size, cxt_size)\n",
        "    \n",
        "    def forward(self, cxt_encoded, qst_encoded, cxt_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len, cxt_size] cxt_encoded\n",
        "        :param Tensor[batch_size, qst_size] qst_encoded\n",
        "        :param Tensor[batch_size, cxt_len] cxt_mask\n",
        "        :return Tensor[batch_size, cxt_len, hidden_size]\n",
        "        \"\"\"\n",
        "        qst_encoded = self.linear(qst_encoded) # [batch_size, cxt_size]\n",
        "        scores = torch.bmm(cxt_encoded, qst_encoded.unsqueeze(-1)) # [batch_size, cxt_len, 1]\n",
        "        scores = scores.squeeze(-1).masked_fill(qst_mask == 0, 1e-18) # [batch_size, cxt_len]\n",
        "        return F.log_softmax(scores, dim=-1) if self.training else F.softmax(scores, dim=-1)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XGXeFMpIqYw"
      },
      "source": [
        "***Document reader Question Answering Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FPLlPwKIpzK"
      },
      "source": [
        "class DrQA(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size, n_extra_features, hidden_size, n_layers, dropout, pad_index):\n",
        "        super(DrQA, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_extra_features = n_extra_features\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.pad_index = pad_index\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_index)\n",
        "        self.align_question_embedding_layer = AlignQuestionEmbeddingLayer(hidden_size=embedding_size)\n",
        "        self.cxt_stacked_bi_lstm_layer = StackedBiLSTMsLayer(input_size=embedding_size * 2 + n_extra_features,\n",
        "                                                             hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
        "        self.qst_encoding_layer = QuestionEncodingLayer(input_size=embedding_size, hidden_size=hidden_size, dropout=dropout, n_layers=n_layers)\n",
        "        self.bilinear_attention_layer_start = BiLinearAttentionLayer(cxt_size=hidden_size * n_layers * 2, qst_size=hidden_size * n_layers * 2)\n",
        "        self.bilinear_attention_layer_end = BiLinearAttentionLayer(cxt_size=hidden_size * n_layers * 2, qst_size=hidden_size * n_layers * 2)\n",
        "    \n",
        "    def load_glove_embeddings(self, path, most_common_indexes):\n",
        "        def tune_embeddings(grad, words=most_common_indexes):\n",
        "            grad[most_common_indexes] = 0\n",
        "            return grad\n",
        "        \n",
        "        self.embedding.weight = nn.Parameter(torch.FloatTensor(np.load(path)))\n",
        "        self.embedding.weight.register_hook(tune_embeddings) # Only fine-tune the 1000 most frequent question words\n",
        "    \n",
        "    def make_cxt_mask(self, cxt_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len]\n",
        "        :return Tensor[batch_size, cxt_len]\n",
        "        \"\"\"\n",
        "        return cxt_sequences != self.pad_index\n",
        "    \n",
        "    def make_qst_mask(self, qst_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, qst_len]\n",
        "        :return Tensor[batch_size, qst_len]\n",
        "        \"\"\"\n",
        "        return qst_sequences != self.pad_index\n",
        "    \n",
        "    @staticmethod\n",
        "    def decode(starts, ends):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len] starts\n",
        "        :param Tensor[batch_size, cxt_len] ends\n",
        "        :return list(int) start_indexes\n",
        "        :return list(int) end_indexes\n",
        "        :return list(float) pred_probas\n",
        "        \"\"\"\n",
        "        start_indexes, end_indexes, pred_probas = [], [], []\n",
        "        for i in range(starts.size(0)):\n",
        "            probas = torch.ger(starts[i], ends[i]) # [cxt_len, cxt_len]\n",
        "            proba, index = torch.topk(proba.view(-1), k=1)\n",
        "            start_indexes.append(index.tolist()[0] // probas.size(0))\n",
        "            end_indexes.append(index.tolist()[0] % probas.size(1))\n",
        "            pred_probas.append(proba.tolist()[0])\n",
        "        return start_indexes, end_indexes, pred_probas\n",
        "    \n",
        "    def forward(self, cxt_sequences, em_sequences, pos_sequences, ner_sequences, tf_sequences, qst_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len] cxt_sequences\n",
        "        :param Tensor[batch_size, cxt_len] em_sequences\n",
        "        :param Tensor[batch_size, cxt_len] pos_sequences\n",
        "        :param Tensor[batch_size, cxt_len] ner_sequences\n",
        "        :param Tensor[batch_size, cxt_len] tf_sequences\n",
        "        :param Tensor[batch_size, qst_len] qst_sequences\n",
        "        :param Tensor[batch_size, cxt_len] cxt_mask\n",
        "        :param Tensor[batch_size, qst_len] qst_mask\n",
        "        :return Tensor[batch_size, cxt_len] starts\n",
        "        :return Tensor[batch_size, cxt_len] ends\n",
        "        \"\"\"\n",
        "        cxt_mask = self.make_cxt_mask(cxt_sequences) # [batch_size, cxt_len]\n",
        "        qst_mask = self.make_qst_mask(qst_sequences) # [batch_size, qst_len]\n",
        "        cxt_embedded = self.dropout(self.embedding(cxt_sequences)) # [batch_size, cxt_len, embedding_size]\n",
        "        qst_embedded = self.dropout(self.embedding(qst_sequences)) # [batch_size, cxt_len, embedding_size]\n",
        "        cxt_aligned = self.align_question_embedding_layer(cxt_embed=cxt_embedded, qst_embed=qst_embedded,\n",
        "                                                          qst_mask=qst_mask) # [batch_size, cxt_len, embedding_size]\n",
        "        cxt_encoded = torch.cat([cxt_embedded.unsqueeze(-1),\n",
        "                                 em_sequences.unsqueeze(-1),\n",
        "                                 pos_sequences.unsqueeze(-1),\n",
        "                                 ner_sequences.unsqueeze(-1),\n",
        "                                 tf_sequences.unsqueeze(-1),\n",
        "                                 cx_aligned], dim=-1) # [batch_size, cxt_len, embedding_size * 2 + 4]\n",
        "        cxt_encoded = self.cxt_stacked_bi_lstm_layer(inputs=cxt_encoded) # [batch_size, cxt_len, hidden_size * n_layers * 2]\n",
        "        qst_encoded = self.qst_encoding_layer(qst_embed=qst_embedded, qst_mask=qst_mask) # [batch_size, hidden_size * n_layers * 2]\n",
        "        starts = self.bilinear_attention_layer_start(cxt_encoded=cxt_encoded, qst_encoded=qst_encoded, cxt_mask=cxt_mask)\n",
        "        ends = self.bilinear_attention_layer_end(cxt_encoded=cxt_encoded, qst_encoded=qst_encoded, cxt_mask=cxt_mask)\n",
        "        return starts, ends"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSpwpOK8QBWV"
      },
      "source": [
        "***Training routines***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu_ziZuDPSyX"
      },
      "source": [
        "class AverageMeter:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.value = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "        self.average = 0.\n",
        "        \n",
        "    def reset(self):\n",
        "        self.value = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "        self.average = 0.\n",
        "        \n",
        "    def update(self, value, n=1):\n",
        "        self.value = value\n",
        "        self.sum += value * n\n",
        "        self.count += n\n",
        "        self.average = self.sum / self.count"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1TOIhXBQGcB"
      },
      "source": [
        "def normalize_answer(s):\n",
        "    \"\"\"Performs a series of cleaning steps on the ground truth and predicted answer.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    \"\"\"Returns maximum value of metrics for predicition by model against\n",
        "    multiple ground truths.\n",
        "    \n",
        "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n",
        "    :param str prediction: predicted answer span by the model\n",
        "    :param list ground_truths: list of ground truths against which\n",
        "                               metrics are calculated. Maximum values of \n",
        "                               metrics are chosen.\n",
        "    \"\"\"\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "        \n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    \"\"\"Returns f1 score of two strings.\"\"\"\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = collectons.Counter(prediction_tokens) & collectons.Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    \"\"\"Returns exact_match_score of two strings.\"\"\"\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jAP2WtVRKUy"
      },
      "source": [
        "def evaluate(predictions, path='./data/valid.json'):\n",
        "    \"\"\"Gets a dictionary of predictions with question_id as key\n",
        "    and prediction as value. The validation dataset has multiple \n",
        "    answers for a single question. Hence we compare our prediction\n",
        "    with all the answers and choose the one that gives us\n",
        "    the maximum metric (em or f1).\n",
        "\n",
        "    :param dict predictions\n",
        "    :return float exact_match: 1 if the prediction and ground truth match exactly, 0 otherwise.\n",
        "    :return float f1_score\n",
        "    \"\"\"\n",
        "    dataset = load_json(path)\n",
        "    f1 = exact_match = total = 0\n",
        "    for article in dataset:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            for qa in paragraph['qas']:\n",
        "                total += 1\n",
        "                if qa['id'] not in predictions:\n",
        "                    continue\n",
        "                ground_truths = [*map(lambda x: x['text'], qa['answers'])]\n",
        "                prediction = predictions[qa['id']]\n",
        "                exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
        "                f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "    return exact_match, f1"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20-KuPdNTu2k"
      },
      "source": [
        "class Trainer:\n",
        "    \n",
        "    def __init__(self, model, optimizer, criterion, id_field, text_field):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.id_field = id_field\n",
        "        self.text_field = text_field\n",
        "        \n",
        "    def train_step(self, loader, epoch, grad_clip):\n",
        "        loss_tracker = AverageMeter()\n",
        "        self.model.train()\n",
        "        progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "        for i, batch in progress_bar:\n",
        "            self.optimizer.zero_grad()\n",
        "            starts, ends = self.model(cxt_sequences=batch.cxt, em_sequences=batch.em, pos_sequences=batch.pos, ner_sequences=batch.ner,\n",
        "                                      tf_sequences=batch.tf, qst_sequences=batch.qst) # [batch_size, cxt_len]\n",
        "            loss = self.criterion(starts, batch.trg[:, 0]) + self.criterion(ends, batch.trg[:, 1])\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
        "            self.optimizer.step()\n",
        "            loss_tracker.update(loss.item())\n",
        "            progress_bar.set_description(f'Epoch: {epoch+1:02d} -     loss: {loss_tracker.average:.3f}%')\n",
        "        return loss_tracker.average\n",
        "    \n",
        "    def validate(self, loader, epoch):\n",
        "        loss_tracker, predictions = AverageMeter(), {}\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "            for i, batch in progress_bar:\n",
        "                starts, ends = self.model(cxt_sequences=batch.cxt, em_sequences=batch.em, pos_sequences=batch.pos, ner_sequences=batch.ner,\n",
        "                                      tf_sequences=batch.tf, qst_sequences=batch.qst) # [batch_size, cxt_len]\n",
        "                loss = self.criterion(starts, batch.trg[:, 0]) + self.criterion(ends, batch.trg[:, 1])\n",
        "                start_indexes, end_indexes, _ = self.model.decode(starts=starts, ends=ends)\n",
        "                for i in range(starts.size(0)):\n",
        "                    id = self.id_field.vocab.itos[batch.ids[i].item()]\n",
        "                    prediction = batch.cxt[i][start_indexes[i]:end_indexes[i]+1]\n",
        "                    predictions[id] = ' '.join([self.text_field.vocab.itos[indice.item()] for indice in prediction])\n",
        "                loss_tracker.update(loss.item())\n",
        "                progress_bar.set_description(f'Epoch: {epoch+1:02d} - val_loss: {loss_tracker.average:.3f}')\n",
        "        return loss_tracker.average, predictions\n",
        "    \n",
        "    def train(self, train_loader, valid_loader, n_epochs, grad_clip):\n",
        "        history, best_loss = {'loss': [], 'val_loss': [], 'em': [], 'f1': []}, np.inf\n",
        "        for epoch in range(n_epochs):\n",
        "            loss = self.train_step(train_loader, epoch, grad_clip)\n",
        "            val_loss, predictions = self.validate(valid_loader, epoch)\n",
        "            em, f1 = evaluate(predictions)\n",
        "            print(f'\\nF1={f1:.3f} - EM={em:.3f}')\n",
        "            history['loss'].append(loss); history['val_loss'].append(val_loss)\n",
        "            history['em'].append(em); history['f1'].append(f1)\n",
        "            if best_loss > val_loss:\n",
        "                best_loss = val_loss\n",
        "                torch.save(self.model.state_dict(), './checkpoints/DrQA.pth')\n",
        "        return history"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7BcJPV_eNGJ"
      },
      "source": [
        "***Train the model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dHfjBuUeHE6"
      },
      "source": [
        "N_LAYERS = 3\n",
        "HIDDEN_SIZE = 128\n",
        "EMBED_SIZE = 300\n",
        "DROPOUT = 0.3\n",
        "N_EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "GRAD_CLIP = 10.0"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVcHGhr-eQfB",
        "outputId": "30b89c8c-6193-46e8-d674-e9a435324e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "drqa = DrQA(vocab_size=len(TEXT.vocab), embedding_size=EMBED_SIZE, n_extra_features=4, hidden_size=HIDDEN_SIZE, n_layers=N_LAYERS,\n",
        "            dropout=DROPOUT, pad_index=TEXT.vocab.stoi[TEXT.pad_token])\n",
        "drqa.load_glove_embeddings('./data/GloVe_DrQA.npy', most_common_indexes)\n",
        "drqa.to(DEVICE)\n",
        "optimizer = optim.Adamax(params=drqa.parameters())\n",
        "criterion = nn.NLLLoss(ignore_index=TEXT.vocab.stoi[TEXT.pad_token])\n",
        "print(f'Number of parameters of the model: {sum(p.numel() for p in drqa.parameters() if p.requires_grad):,}')\n",
        "print(drqa)\n",
        "trainer = Trainer(model=drqa, optimizer=optimizer, criterion=criterion, id_field=ID, text_field=TEXT)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters of the model: 36,221,745\n",
            "DrQA(\n",
            "  (embedding): Embedding(91446, 300, padding_idx=1)\n",
            "  (align_question_embedding_layer): AlignQuestionEmbeddingLayer(\n",
            "    (linear): Linear(in_features=300, out_features=300, bias=True)\n",
            "  )\n",
            "  (cxt_stacked_bi_lstm_layer): StackedBiLSTMsLayer(\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (lstms): ModuleList(\n",
            "      (0): LSTM(604, 128, num_layers=3, bidirectional=True)\n",
            "      (1): LSTM(256, 128, num_layers=3, bidirectional=True)\n",
            "      (2): LSTM(256, 128, num_layers=3, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (qst_encoding_layer): QuestionEncodingLayer(\n",
            "    (stacked_bilstms_layer): StackedBiLSTMsLayer(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (lstms): ModuleList(\n",
            "        (0): LSTM(300, 128, num_layers=3, bidirectional=True)\n",
            "        (1): LSTM(256, 128, num_layers=3, bidirectional=True)\n",
            "        (2): LSTM(256, 128, num_layers=3, bidirectional=True)\n",
            "      )\n",
            "    )\n",
            "    (linear): Linear(in_features=300, out_features=1, bias=True)\n",
            "  )\n",
            "  (bilinear_attention_layer_start): BiLinearAttentionLayer(\n",
            "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "  )\n",
            "  (bilinear_attention_layer_end): BiLinearAttentionLayer(\n",
            "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEzloF3aewPA",
        "outputId": "91a0371b-a38e-4356-d4c4-882f3984d03f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!mkdir -p ./checkpoints\n",
        "train_iterator, valid_iterator =  BucketIterator.splits((train_dataset, valid_dataset), batch_size=BATCH_SIZE, sort=False, device=DEVICE)\n",
        "history = trainer.train(train_loader=train_iterator, valid_loader=valid_iterator, n_epochs=N_EPOCHS, grad_clip=GRAD_CLIP)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2707 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-c56fa1de01bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir -p ./checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mBucketIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGRAD_CLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-115-4f37de7a95e8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, valid_loader, n_epochs, grad_clip)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'em'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'f1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-115-4f37de7a95e8>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, loader, epoch, grad_clip)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             starts, ends = self.model(cxt_sequences=batch.cxt, em_sequences=batch.em, pos_sequences=batch.pos, ner_sequences=batch.ner,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \"\"\"\n\u001b[1;32m    233\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 181 at dim 1 (got 136)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7vy8qHafG0x",
        "outputId": "cd3f9d80-d933-49c9-f89a-c7e62d3b577b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        ""
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-767a20915f9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \"\"\"\n\u001b[1;32m    233\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 105 at dim 1 (got 129)"
          ]
        }
      ]
    }
  ]
}