{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - Document reader Question Answering - DrQA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPh6IH5Zll6UYKlsBu0jX/2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dksifoua/Question-Answering/blob/master/1%20-%20DrQA%20-%20Document%20reader%20Question%20Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h8encuudFC-",
        "outputId": "d4298501-e8a4-462b-db49-70dfb1b17fe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Oct 10 07:18:15 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P8    12W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFHeYfTa5iBg"
      },
      "source": [
        "## Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cIZzQAImLQO"
      },
      "source": [
        "!pip install tqdm --upgrade >> /dev/null 2>&1\n",
        "!pip install torchtext --upgrade >> /dev/null 2>&1\n",
        "!pip install spacy --upgrade >> /dev/null 2>&1\n",
        "!python -m spacy download en >> /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gHtnSaemPVM"
      },
      "source": [
        "import re\n",
        "import json\n",
        "import tqdm\n",
        "import spacy\n",
        "import warnings\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.data import Dataset, Example, Field\n",
        "from torchtext.data.iterator import BucketIterator\n",
        "from torchtext.data.metrics import bleu_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q7DBR5gmTZa",
        "outputId": "17208912-a26e-4ee5-98b0-33c5534f981e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "\n",
        "SEED = 546\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNZkKM5G5fgi"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "***Download data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZhbHNhT4YRn",
        "outputId": "dbe13b15-f68b-492c-93ba-d265133d6b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "%%time\n",
        "!mkdir ./data\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json \\\n",
        "    -O ./data/train.json\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json \\\n",
        "    -O ./data/valid.json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./data’: File exists\n",
            "--2020-10-10 07:18:26--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘./data/train.json’\n",
            "\n",
            "./data/train.json   100%[===================>]  40.17M   138MB/s    in 0.3s    \n",
            "\n",
            "2020-10-10 07:18:27 (138 MB/s) - ‘./data/train.json’ saved [42123633/42123633]\n",
            "\n",
            "--2020-10-10 07:18:27--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘./data/valid.json’\n",
            "\n",
            "./data/valid.json   100%[===================>]   4.17M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-10-10 07:18:27 (32.0 MB/s) - ‘./data/valid.json’ saved [4370528/4370528]\n",
            "\n",
            "CPU times: user 9.46 ms, sys: 23.9 ms, total: 33.4 ms\n",
            "Wall time: 955 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs5S3iohgiMM"
      },
      "source": [
        "***Load data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vTuq9jl3oUJ",
        "outputId": "b2b7dcbf-85c1-400f-a993-47f0f996c7a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def load_json(path):\n",
        "    with open(path, mode='r', encoding='utf-8') as file:\n",
        "        return json.load(file)['data']\n",
        "\n",
        "train_raw = load_json(path='./data/train.json')\n",
        "valid_raw = load_json(path='./data/valid.json')\n",
        "print(f'Length of raw train data: {len(train_raw):,}')\n",
        "print(f'Length of raw valid data: {len(valid_raw):,}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of raw train data: 442\n",
            "Length of raw valid data: 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6rKVZjcgmF7"
      },
      "source": [
        "***Parse data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfhHilFU6fb_",
        "outputId": "4a26015c-2338-4a15-b73d-df4c0817fc81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "def parse_json(data):\n",
        "    qas = []\n",
        "    for paragraphs in data:\n",
        "        for para in paragraphs['paragraphs']:\n",
        "            context = para['context']\n",
        "            for qa in para['qas']:\n",
        "                id = qa['id']\n",
        "                question = qa['question']\n",
        "                for ans in qa['answers']:\n",
        "                    answer = ans['text']\n",
        "                    ans_start = ans['answer_start']\n",
        "                    qas.append({\n",
        "                        'id': id,\n",
        "                        'context': context,\n",
        "                        'question': question,\n",
        "                        'answer': answer,\n",
        "                        'answer_start': ans_start,\n",
        "                    })\n",
        "    return qas\n",
        "\n",
        "train_qas = parse_json(train_raw)\n",
        "valid_qas = parse_json(valid_raw)\n",
        "print(f'Length of train qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs: {len(valid_qas):,}')\n",
        "\n",
        "print('Context:', train_qas[0]['context'])\n",
        "print('Question:', train_qas[0]['question'])\n",
        "print('Answer starts at:', train_qas[0]['answer_start'])\n",
        "print('Answer:', train_qas[0]['answer'])\n",
        "\n",
        "for i in range(len(train_qas)): # Test labels are correct in train set\n",
        "    assert train_qas[i]['answer'] == train_qas[i]['context'][train_qas[i]['answer_start']:train_qas[i]['answer_start']+len(train_qas[i]['answer'])]\n",
        "\n",
        "for i in range(len(valid_qas)): # Test labels are correct in validation set\n",
        "    assert valid_qas[i]['answer'] == valid_qas[i]['context'][valid_qas[i]['answer_start']:valid_qas[i]['answer_start']+len(valid_qas[i]['answer'])]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs: 86,821\n",
            "Length of valid qa pairs: 20,302\n",
            "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "Question: When did Beyonce start becoming popular?\n",
            "Answer starts at: 269\n",
            "Answer: in the late 1990s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs87TY30gpOG"
      },
      "source": [
        "***Add targets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMWsmFjjt5vB",
        "outputId": "a07af170-bd64-4efc-bfee-d7c47881ee0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%%time\n",
        "def add_targets(qas, nlp=spacy.load('en')):\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "        answer = nlp(qa['answer'], disable=['parser','tagger','ner'])\n",
        "        ans_start = qa['answer_start']\n",
        "        for i in range(len(context)):\n",
        "            if context[i].idx == ans_start:\n",
        "                ans = context[i:i + len(answer)]\n",
        "                qa['target'] = (ans[0].i, ans[-1].i)\n",
        "                break\n",
        "\n",
        "def filter_qas(qa, nlp=spacy.load('en')):\n",
        "    if 'target' in [*qa.keys()]:\n",
        "        context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "        start, end = qa['target']\n",
        "        return context[start:end+1].text == qa['answer']\n",
        "    return False\n",
        "\n",
        "def test_targets(qas, nlp=spacy.load('en')):\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        if 'target' in [*qa.keys()]:\n",
        "            context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "            start, end = qa['target']\n",
        "            assert context[start:end + 1].text == qa['answer'], f\"{context[start:end + 1].text} -- {qa['answer']}\"\n",
        "\n",
        "add_targets(train_qas)\n",
        "add_targets(valid_qas)\n",
        "print(f'Length of train qa pairs before filtering out bad qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs before filtering out bad qa pairs: {len(valid_qas):,}')\n",
        "train_qas = [*filter(filter_qas, train_qas)]\n",
        "valid_qas = [*filter(filter_qas, valid_qas)]\n",
        "print(f'Length of train qa pairs after filtering out bad qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs after filtering out bad qa pairs: {len(valid_qas):,}')\n",
        "test_targets(train_qas, spacy.load('en'))\n",
        "test_targets(valid_qas, spacy.load('en'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 86821/86821 [01:38<00:00, 879.66it/s]\n",
            "100%|██████████| 20302/20302 [00:24<00:00, 814.81it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs before filtering out bad qa pairs: 86,821\n",
            "Length of valid qa pairs before filtering out bad qa pairs: 20,302\n",
            "Length of train qa pairs after filtering out bad qa pairs: 85,835\n",
            "Length of valid qa pairs after filtering out bad qa pairs: 20,094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 85835/85835 [01:31<00:00, 935.39it/s]\n",
            "100%|██████████| 20094/20094 [00:13<00:00, 1439.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 36s, sys: 1.92 s, total: 5min 38s\n",
            "Wall time: 5min 38s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RZYINhagvj1"
      },
      "source": [
        "***Build datasets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipM5ew0DnQIP"
      },
      "source": [
        "TEXT = Field(init_token='<sos>', eos_token='<eos>', lower=True, tokenizer_language='en', tokenize='spacy', batch_first=True)\n",
        "TARGET = Field(sequential=False, use_vocab=False, batch_first=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHTjO-B0pUkR",
        "outputId": "7541211f-e4cc-4c28-9e89-b49bc74dcf44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_dataset = Dataset([Example.fromdict(data=qa, fields={\n",
        "    'context': ('cxt', TEXT),\n",
        "    'question': ('qst', TEXT),\n",
        "    'answer': ('ans', TEXT),\n",
        "    'target': ('trg', TARGET)\n",
        "}) for qa in tqdm.tqdm(train_qas)], fields={'cxt': TEXT, 'qst': TEXT,\n",
        "                                            'ans': TEXT, 'trg': TARGET})\n",
        "valid_dataset = Dataset([Example.fromdict(data=qa, fields={\n",
        "    'context': ('cxt', TEXT),\n",
        "    'question': ('qst', TEXT),\n",
        "    'answer': ('ans', TEXT),\n",
        "    'target': ('trg', TARGET)\n",
        "}) for qa in tqdm.tqdm(valid_qas)], fields={'cxt': TEXT, 'qst': TEXT,\n",
        "                                            'ans': TEXT, 'trg': TARGET})\n",
        "print(f'Train dataset size: {len(train_dataset.examples):,}')\n",
        "print(f'valid dataset size: {len(valid_dataset.examples):,}')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 85835/85835 [02:11<00:00, 651.73it/s]\n",
            "100%|██████████| 20094/20094 [00:32<00:00, 614.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train dataset size: 85,835\n",
            "valid dataset size: 20,094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBhRj5JNqR3M",
        "outputId": "71e15cba-4db6-44c4-e13d-76b69866aa59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_dataset)\n",
        "print(f'Length of vocabulary: {len(TEXT.vocab):,}')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of vocabulary: 91,398\n",
            "CPU times: user 2.64 s, sys: 17.9 ms, total: 2.66 s\n",
            "Wall time: 2.66 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYsaVy3hsnHE"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zelMKFUV-ayF"
      },
      "source": [
        "***Stacked Bidirectional LSTM Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA4xvLwGxcfF"
      },
      "source": [
        "class StackedBiLSTMLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, n_layers, dropout):\n",
        "        super(StackedBiLSTMLayer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.lstms = nn.ModuleList([nn.LSTM(input_size=hidden_size * 2 if i > 0 else input_size, hidden_size=hidden_size,\n",
        "                                            bidirectional=True, batch_first=True) for i in range(n_layers)])\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, seq_len, input_size]\n",
        "        :return Tensor[batch_size, seq_len, hidden_size * n_layers * 2]\n",
        "        \"\"\"\n",
        "        outputs = [inputs]\n",
        "        for i, lstm in enumerate(self.lstms):\n",
        "            out, _ = lstm(outputs[-1]) # [batch_size, seq_len, hidden_size * 2]\n",
        "            if i > 0 and i < self.n_layers - 1:\n",
        "                out = self.dropout(out)\n",
        "            outputs.append(out)\n",
        "        return torch.cat(outputs[1:], dim=-1)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZqiiy7wyYrm"
      },
      "source": [
        "***Align Question Embedding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeFiGiibqS3D"
      },
      "source": [
        "class AlignQuestionEmbeddingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_size, hidden_size):\n",
        "        super(AlignQuestionEmbeddingLayer, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fc = nn.Linear(embedding_size, hidden_size)\n",
        "    \n",
        "    def forward(self, cxt, qst, qst_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len, embedding_size] cxt\n",
        "        :param Tensor[batch_size, qst_len, embedding_size] qst\n",
        "        :param Tensor[batch_size, qst_len] mask\n",
        "        :return Tensor[batch_size, cxt_len, hidden_size] context\n",
        "        \"\"\"\n",
        "        cxt = F.relu(self.fc(cxt)) # [batch_size, cxt_len, hidden_size]\n",
        "        qst = F.relu(self.fc(qst)) # [batch_size, qst_len, hidden_size]\n",
        "        scores = torch.matmul(cxt, qst.transpose(-1, -2)) # [batch_size, cxt_len, qst_len]\n",
        "        if qst_mask is not None:\n",
        "            scores = scores.masked_fill(qst_mask.unsqueeze(1) == 0, 1e-18)\n",
        "        attention_weights = F.softmax(scores, dim=-1) # [batch_size, cxt_len, qst_len]\n",
        "        context = torch.matmul(attention_weights, qst) # [batch_size, cxt_len, hidden_size]\n",
        "        return context"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkpr81T-z2XA"
      },
      "source": [
        "***Question Encoding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OizoayNez7Le"
      },
      "source": [
        "class QuestionEncodingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(QuestionEncodingLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "    def forward(self, qst_sequences, qst_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, seq_len, hidden_size] qst_sequences\n",
        "        :param Tensor[batch_size, seq_len] qst_mask\n",
        "        :return Tensor[batch_size, hidden_size] context\n",
        "        \"\"\"\n",
        "        scores = self.W(qst_sequences) # [batch_size, seq_len, 1]\n",
        "        if qst_mask is not None:\n",
        "            scores = scores.masked_fill(qst_mask.unsqueeze(-1) == 0, 1e-18)\n",
        "        attention_weights = F.softmax(scores, dim=1) # [batch_size, seq_len, 1]\n",
        "        context = torch.matmul(attention_weights.transpose(-1, -2), qst_sequences) # [batch_size, 1, hidden_size]\n",
        "        return context.squeeze(1)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFfqiP4V9k99"
      },
      "source": [
        "***BiLinear Attention Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbPaa3U69oO0"
      },
      "source": [
        "class BiLinearAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BiLinearAttentionLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, cxt_encoded, qst_encoded, cxt_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len, hidden_size] cxt_encoded\n",
        "        :param Tensor[batch_size, hidden_size] qst_encoded\n",
        "        :param Tensor[batch_size, cxt_len] cxt_mask\n",
        "        :param Tensor[batch_size, cxt_len] scores\n",
        "        \"\"\"\n",
        "        qst_encoded = self.linear(qst_encoded) # [batch_size, hidden_size]\n",
        "        scores = torch.matmul(cxt_encoded, qst_encoded.unsqueeze(-1)) # [batch_size, cxt_len, 1]\n",
        "        if cxt_mask is not None:\n",
        "            scores = scores.squeeze(-1).masked_fill(cxt_mask == 0, 1e-18) # [batch_size, cxt_len]\n",
        "        return scores"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC8Jh46f-j22"
      },
      "source": [
        "***DrQA Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8xrxN8R7Em1"
      },
      "source": [
        "class DrQA(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, n_layers, dropout, pad_index):\n",
        "        super(DrQA, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pad_index = pad_index\n",
        "        self.cxt_embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.qst_embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.align_question_embedding_layer = AlignQuestionEmbeddingLayer(embedding_size=embedding_size, hidden_size=hidden_size)\n",
        "        self.cxt_stacked_bi_lstm_layer = StackedBiLSTMLayer(input_size=hidden_size, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
        "        self.qst_stacked_bi_lstm_layer = StackedBiLSTMLayer(input_size=embedding_size, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
        "        self.qst_encoding_layer = QuestionEncodingLayer(hidden_size=hidden_size * n_layers * 2)\n",
        "        self.bilinear_attention_layer_start = BiLinearAttentionLayer(hidden_size=hidden_size * n_layers * 2)\n",
        "        self.bilinear_attention_layer_end = BiLinearAttentionLayer(hidden_size=hidden_size * n_layers * 2)\n",
        "    \n",
        "    def make_cxt_mask(self, cxt_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len]\n",
        "        :return Tensor[batch_size, cxt_len]\n",
        "        \"\"\"\n",
        "        return cxt_sequences != self.pad_index\n",
        "    \n",
        "    def make_qst_mask(self, qst_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, qst_len]\n",
        "        :return Tensor[batch_size, qst_len]\n",
        "        \"\"\"\n",
        "        return qst_sequences != self.pad_index\n",
        "\n",
        "    def forward(self, cxt_sequences, qst_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len] cxt_sequences\n",
        "        :param Tensor[batch_size, qst_len] qst_sequences\n",
        "        :param Tensor[batch_size, cxt_len] cxt_mask\n",
        "        :param Tensor[batch_size, qst_len] qst_mask\n",
        "        :return Tensor[batch_size, cxt_len] start_scores\n",
        "        :return Tensor[batch_size, cxt_len] end_scores\n",
        "        \"\"\"\n",
        "        cxt_mask = self.make_cxt_mask(cxt_sequences) # [batch_size, cxt_len]\n",
        "        qst_mask = self.make_qst_mask(qst_sequences) # [batch_size, cxt_len]\n",
        "        \n",
        "        cxt_embedded = self.dropout(self.cxt_embedding(cxt_sequences)) # [batch_size, cxt_len, embedding_size]\n",
        "        qst_embedded = self.dropout(self.qst_embedding(qst_sequences)) # [batch_size, cxt_len, embedding_size]\n",
        "        \n",
        "        cxt_qst_aligned = self.align_question_embedding_layer(cxt=cxt_embedded, qst=qst_embedded, qst_mask=qst_mask) # [batch_size, cxt_len, hidden_size]\n",
        "        cxt_encoded = self.cxt_stacked_bi_lstm_layer(inputs=cxt_qst_aligned) # [batch_size, cxt_len, hidden_size * n_layers * 2]\n",
        "        \n",
        "        qst_lstm = self.qst_stacked_bi_lstm_layer(inputs=qst_embedded) # [batch_size, qst_len, hidden_size * n_layers * 2]\n",
        "        qst_encoded = self.qst_encoding_layer(qst_sequences=qst_lstm, qst_mask=qst_mask) # [batch_size, hidden_size * n_layers * 2]\n",
        "\n",
        "        start_scores = self.bilinear_attention_layer_start(cxt_encoded=cxt_encoded, qst_encoded=qst_encoded, cxt_mask=cxt_mask) # [batch_size, cxt_len]\n",
        "        end_scores = self.bilinear_attention_layer_end(cxt_encoded=cxt_encoded, qst_encoded=qst_encoded, cxt_mask=cxt_mask) # [batch_size, cxt_len]\n",
        "\n",
        "        return start_scores, end_scores"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNCQG-tjBHcZ"
      },
      "source": [
        "***Training routines***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBWI7liR6gH2"
      },
      "source": [
        "class AverageMeter:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.value = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "        self.average = 0.\n",
        "        \n",
        "    def reset(self):\n",
        "        self.value = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "        self.average = 0.\n",
        "        \n",
        "    def update(self, value, n=1):\n",
        "        self.value = value\n",
        "        self.sum += value * n\n",
        "        self.count += n\n",
        "        self.average = self.sum / self.count"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV2gVekkBMS3"
      },
      "source": [
        "def exact_match_score(predictions, targets):\n",
        "    \"\"\"\n",
        "    :param list[batch_size] predictions\n",
        "    :param list[batch_size] targets\n",
        "    :return float Exact Match score\n",
        "    \"\"\"\n",
        "    return 100.0 * sum(map(lambda y: y[0] == y[1], zip(predictions, targets))) / len(targets)\n",
        "\n",
        "def f1_scores(predictions, targets):\n",
        "    \"\"\"\n",
        "    :param list[batch_size] predictions\n",
        "    :param list[batch_size] targets\n",
        "    :return float F1 score\n",
        "    \"\"\"\n",
        "    f1_scores = []\n",
        "    for (prediction, target) in zip(predictions, targets):\n",
        "        common = collections.Counter(prediction) & collections.Counter(target)\n",
        "        num_same = sum(common.values())\n",
        "        if num_same == 0:\n",
        "            return 0\n",
        "        precision = 1.0 * num_same / len(prediction)\n",
        "        recall = 1.0 * num_same / len(target)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        f1_scores.append(f1)\n",
        "    return np.mean(f1_scores) * 100.0"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72GXApi2H23S"
      },
      "source": [
        "class Trainer:\n",
        "    \n",
        "    def __init__(self, model, optimizer, criterion):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_metrics(cxt, ans, starts, ends):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len] cxt\n",
        "        :param Tensor[batch_size, ans_len] ans\n",
        "        :param Tensor[batch_size,] starts\n",
        "        :param Tensor[batch_size,] ends\n",
        "        \"\"\"\n",
        "        cxt, ans = cxt.tolist(), ans.tolist()\n",
        "        starts, ends = starts.tolist(), ends.tolist()\n",
        "        preds = [*map(lambda x: x[0][x[1]:x[2] + 1], zip(cxt, starts, ends))]\n",
        "        em, f1 = exact_match_score(preds, ans), f1_scores(preds, ans)\n",
        "        return em, f1\n",
        "        \n",
        "    def train_step(self, loader, epoch, grad_clip):\n",
        "        loss_tracker, em_tracker, f1_tracker = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "        self.model.train()\n",
        "        progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "        for i, batch in progress_bar:\n",
        "            cxt, qst, ans, trg = batch.cxt, batch.qst, batch.ans, batch.trg\n",
        "            self.optimizer.zero_grad()\n",
        "            start_scores, end_scores = self.model(cxt_sequences=cxt, qst_sequences=qst) # [batch_size, cxt_len]\n",
        "            loss = self.criterion(start_scores, trg[:, 0]) + self.criterion(end_scores, trg[:, 1])\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
        "            self.optimizer.step()\n",
        "            starts = torch.argmax(F.log_softmax(start_scores, dim=1), dim=1) # [batch_size,]\n",
        "            ends = torch.argmax(F.log_softmax(end_scores, dim=1), dim=1) # [batch_size,]\n",
        "            em, f1 = self.calculate_metrics(cxt, ans, starts, ends)\n",
        "            loss_tracker.update(loss.item()); em_tracker.update(em); f1_tracker.update(f1)\n",
        "            loss_, em_, f1_ = loss_tracker.average, em_tracker.average, f1_tracker.average\n",
        "            progress_bar.set_description(f'Epoch: {epoch+1:02d} -     loss: {loss_:.3f} -     em: {em_:.3f}% -     f1: {f1_:.3f}%')\n",
        "        return loss_tracker.average, em_tracker.average, f1_tracker.average\n",
        "    \n",
        "    def validate(self, loader, epoch):\n",
        "        loss_tracker, em_tracker, f1_tracker = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "            for i, batch in progress_bar:\n",
        "                cxt, qst, ans, trg = batch.cxt, batch.qst, batch.ans, batch.trg\n",
        "                start_scores, end_scores = self.model(cxt_sequences=cxt, qst_sequences=qst) # [batch_size, cxt_len]\n",
        "                loss = self.criterion(start_scores, trg[:, 0]) + self.criterion(end_scores, trg[:, 1])\n",
        "                starts = torch.argmax(F.log_softmax(start_scores, dim=1), dim=1) # [batch_size,]\n",
        "                ends = torch.argmax(F.log_softmax(end_scores, dim=1), dim=1) # [batch_size,]\n",
        "                em, f1 = self.calculate_metrics(cxt, ans, starts, ends)\n",
        "                loss_tracker.update(loss.item()); em_tracker.update(em); f1_tracker.update(f1)\n",
        "                loss_, em_, f1_ = loss_tracker.average, em_tracker.average, f1_tracker.average\n",
        "                progress_bar.set_description(f'Epoch: {epoch+1:02d} - val_loss: {loss_:.3f} - val_em: {em_:.3f}% - val_f1: {f1_:.3f}%')\n",
        "        return loss_tracker.average, em_tracker.average, f1_tracker.average\n",
        "    \n",
        "    def train(self, train_loader, valid_loader, n_epochs, grad_clip):\n",
        "        history, best_loss = {'loss': [], 'em': [], 'f1': [], 'val_loss': [], 'val_em': [], 'val_f1': []}, np.inf\n",
        "        for epoch in range(n_epochs):\n",
        "            loss, em, f1 = self.train_step(train_loader, epoch, grad_clip)\n",
        "            val_loss, val_em, val_f1 = self.validate(valid_loader, epoch)\n",
        "            if best_loss > val_loss:\n",
        "                best_loss = val_loss\n",
        "                torch.save(self.model.state_dict(), './checkpoints/DrQA.pth')\n",
        "            history['em'].append(em); history['val_em'].append(val_em)\n",
        "            history['f1'].append(f1); history['val_f1'].append(f1)\n",
        "            history['loss'].append(loss); history['val_loss'].append(val_loss)\n",
        "        return history"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlWgRbN9ROLA"
      },
      "source": [
        "***Train the model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ-a11PIRRRq"
      },
      "source": [
        "N_LAYERS = 3\n",
        "HIDDEN_SIZE = 128\n",
        "EMBED_SIZE = 300\n",
        "DROPOUT = 0.25\n",
        "N_EPOCHS = 5\n",
        "BATCH_SIZE = 128\n",
        "GRAD_CLIP = 10.0"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LodDrpgLRo_h",
        "outputId": "b7123624-5381-409d-e53e-117a69ce871e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "drqa = DrQA(vocab_size=len(TEXT.vocab), embedding_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, n_layers=N_LAYERS,\n",
        "            dropout=DROPOUT, pad_index=TEXT.vocab.stoi[TEXT.pad_token]).to(DEVICE)\n",
        "optimizer = optim.Adamax(params=drqa.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TEXT.vocab.stoi[TEXT.pad_token])\n",
        "print(f'Number of parameters of the model: {sum(p.numel() for p in drqa.parameters() if p.requires_grad):,}')\n",
        "print(drqa)\n",
        "trainer = Trainer(model=drqa, optimizer=optimizer, criterion=criterion)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters of the model: 58,344,849\n",
            "DrQA(\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (cxt_embedding): Embedding(91398, 300)\n",
            "  (qst_embedding): Embedding(91398, 300)\n",
            "  (align_question_embedding_layer): AlignQuestionEmbeddingLayer(\n",
            "    (fc): Linear(in_features=300, out_features=128, bias=True)\n",
            "  )\n",
            "  (cxt_stacked_bi_lstm_layer): StackedBiLSTMLayer(\n",
            "    (dropout): Dropout(p=0.25, inplace=False)\n",
            "    (lstms): ModuleList(\n",
            "      (0): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
            "      (1): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "      (2): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (qst_stacked_bi_lstm_layer): StackedBiLSTMLayer(\n",
            "    (dropout): Dropout(p=0.25, inplace=False)\n",
            "    (lstms): ModuleList(\n",
            "      (0): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
            "      (1): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "      (2): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (qst_encoding_layer): QuestionEncodingLayer(\n",
            "    (W): Linear(in_features=768, out_features=1, bias=True)\n",
            "  )\n",
            "  (bilinear_attention_layer_start): BiLinearAttentionLayer(\n",
            "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "  )\n",
            "  (bilinear_attention_layer_end): BiLinearAttentionLayer(\n",
            "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahAzyTraSsy5",
        "outputId": "358b7981-e9bb-4e21-f1a1-4f6b2e98ffc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "train_iterator, valid_iterator =  BucketIterator.splits((train_dataset, valid_dataset), batch_size=BATCH_SIZE, sort=False, device=DEVICE)\n",
        "!mkdir -p ./checkpoints\n",
        "history = trainer.train(train_loader=train_iterator, valid_loader=valid_iterator, n_epochs=N_EPOCHS, grad_clip=GRAD_CLIP)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 -     loss: 7.471 -     em: 0.000% -     f1: 0.000%: 100%|██████████| 671/671 [02:26<00:00,  4.58it/s]\n",
            "Epoch: 01 - val_loss: 7.709 - val_em: 0.000% - val_f1: 0.000%: 100%|██████████| 157/157 [00:07<00:00, 20.78it/s]\n",
            "Epoch: 02 -     loss: 7.124 -     em: 0.000% -     f1: 0.000%: 100%|██████████| 671/671 [02:25<00:00,  4.60it/s]\n",
            "Epoch: 02 - val_loss: 7.452 - val_em: 0.000% - val_f1: 0.000%: 100%|██████████| 157/157 [00:07<00:00, 20.74it/s]\n",
            "Epoch: 03 -     loss: 6.873 -     em: 0.000% -     f1: 0.000%: 100%|██████████| 671/671 [02:26<00:00,  4.57it/s]\n",
            "Epoch: 03 - val_loss: 7.300 - val_em: 0.000% - val_f1: 0.000%: 100%|██████████| 157/157 [00:07<00:00, 20.75it/s]\n",
            "Epoch: 04 -     loss: 6.628 -     em: 0.000% -     f1: 0.000%:  42%|████▏     | 280/671 [01:00<01:26,  4.51it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17zun4BjVqgW"
      },
      "source": [
        "_, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axes[0].plot(history['loss'], label='train')\n",
        "axes[0].plot(history['val_loss'], label='valid')\n",
        "axes[0].set_title('Loss history')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].grid(True)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(history['em'], label='train')\n",
        "axes[1].plot(history['val_em'], label='valid')\n",
        "axes[1].set_title('Exact Match history')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Exact Match (%)')\n",
        "axes[1].grid(True)\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].plot(history['f1'], label='train')\n",
        "axes[2].plot(history['val_f1'], label='valid')\n",
        "axes[2].set_title('F1 score history')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('F1 score (%)')\n",
        "axes[2].grid(True)\n",
        "axes[2].legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}