{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - Document reader Question Answering - DrQA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2FKMq2MB9HIXS/C7lCKfQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dksifoua/Question-Answering/blob/master/1%20-%20DrQA%20-%20Document%20reader%20Question%20Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h8encuudFC-",
        "outputId": "4e7fdf4a-21f8-492e-a117-582ffc65de73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Oct 10 02:56:31 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFHeYfTa5iBg"
      },
      "source": [
        "## Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cIZzQAImLQO"
      },
      "source": [
        "!pip install tqdm --upgrade >> /dev/null 2>&1\n",
        "!pip install torchtext --upgrade >> /dev/null 2>&1\n",
        "!pip install spacy --upgrade >> /dev/null 2>&1\n",
        "!python -m spacy download en >> /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gHtnSaemPVM"
      },
      "source": [
        "import re\n",
        "import json\n",
        "import tqdm\n",
        "import spacy\n",
        "import warnings\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.data import Dataset, Example, Field\n",
        "from torchtext.data.iterator import BucketIterator\n",
        "from torchtext.data.metrics import bleu_score"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q7DBR5gmTZa",
        "outputId": "e0c41148-f0b3-4519-bedf-275544fa5e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "\n",
        "SEED = 546\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNZkKM5G5fgi"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "***Download data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZhbHNhT4YRn",
        "outputId": "c64598cd-d35c-4d2b-bbf3-b265ef3c76b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "%%time\n",
        "!mkdir ./data\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json \\\n",
        "    -O ./data/train.json\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json \\\n",
        "    -O ./data/valid.json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-10 02:57:02--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘./data/train.json’\n",
            "\n",
            "./data/train.json   100%[===================>]  40.17M  85.5MB/s    in 0.5s    \n",
            "\n",
            "2020-10-10 02:57:04 (85.5 MB/s) - ‘./data/train.json’ saved [42123633/42123633]\n",
            "\n",
            "--2020-10-10 02:57:04--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘./data/valid.json’\n",
            "\n",
            "./data/valid.json   100%[===================>]   4.17M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-10-10 02:57:04 (34.1 MB/s) - ‘./data/valid.json’ saved [4370528/4370528]\n",
            "\n",
            "CPU times: user 19.1 ms, sys: 15.6 ms, total: 34.7 ms\n",
            "Wall time: 2.25 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs5S3iohgiMM"
      },
      "source": [
        "***Load data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vTuq9jl3oUJ",
        "outputId": "551809c7-0bf6-4af1-d989-4194112b1bbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def load_json(path):\n",
        "    with open(path, mode='r', encoding='utf-8') as file:\n",
        "        return json.load(file)['data']\n",
        "\n",
        "train_raw = load_json(path='./data/train.json')\n",
        "valid_raw = load_json(path='./data/valid.json')\n",
        "print(f'Length of raw train data: {len(train_raw):,}')\n",
        "print(f'Length of raw valid data: {len(valid_raw):,}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of raw train data: 442\n",
            "Length of raw valid data: 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6rKVZjcgmF7"
      },
      "source": [
        "***Parse data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfhHilFU6fb_",
        "outputId": "fd5fbc72-871d-4658-9185-79110f29173c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "def parse_json(data):\n",
        "    qas = []\n",
        "    for paragraphs in data:\n",
        "        for para in paragraphs['paragraphs']:\n",
        "            context = para['context']\n",
        "            for qa in para['qas']:\n",
        "                id = qa['id']\n",
        "                question = qa['question']\n",
        "                for ans in qa['answers']:\n",
        "                    answer = ans['text']\n",
        "                    ans_start = ans['answer_start']\n",
        "                    qas.append({\n",
        "                        'id': id,\n",
        "                        'context': context,\n",
        "                        'question': question,\n",
        "                        'answer': answer,\n",
        "                        'answer_start': ans_start,\n",
        "                    })\n",
        "    return qas\n",
        "\n",
        "train_qas = parse_json(train_raw)\n",
        "valid_qas = parse_json(valid_raw)\n",
        "print(f'Length of train qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs: {len(valid_qas):,}')\n",
        "\n",
        "print('Context:', train_qas[0]['context'])\n",
        "print('Question:', train_qas[0]['question'])\n",
        "print('Answer starts at:', train_qas[0]['answer_start'])\n",
        "print('Answer:', train_qas[0]['answer'])\n",
        "\n",
        "for i in range(len(train_qas)): # Test labels are correct in train set\n",
        "    assert train_qas[i]['answer'] == train_qas[i]['context'][train_qas[i]['answer_start']:train_qas[i]['answer_start']+len(train_qas[i]['answer'])]\n",
        "\n",
        "for i in range(len(valid_qas)): # Test labels are correct in validation set\n",
        "    assert valid_qas[i]['answer'] == valid_qas[i]['context'][valid_qas[i]['answer_start']:valid_qas[i]['answer_start']+len(valid_qas[i]['answer'])]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs: 86,821\n",
            "Length of valid qa pairs: 20,302\n",
            "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "Question: When did Beyonce start becoming popular?\n",
            "Answer starts at: 269\n",
            "Answer: in the late 1990s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs87TY30gpOG"
      },
      "source": [
        "***Add targets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMWsmFjjt5vB",
        "outputId": "d08c00c5-5f70-4515-892e-f195df6382f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%%time\n",
        "def add_targets(qas, nlp=spacy.load('en')):\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "        answer = nlp(qa['answer'], disable=['parser','tagger','ner'])\n",
        "        ans_start = qa['answer_start']\n",
        "        for i in range(len(context)):\n",
        "            if context[i].idx == ans_start:\n",
        "                ans = context[i:i + len(answer)]\n",
        "                qa['target'] = (ans[0].i, ans[-1].i)\n",
        "                break\n",
        "\n",
        "def filter_qas(qa, nlp=spacy.load('en')):\n",
        "    if 'target' in [*qa.keys()]:\n",
        "        context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "        start, end = qa['target']\n",
        "        return context[start:end+1].text == qa['answer']\n",
        "    return False\n",
        "\n",
        "def test_targets(qas, nlp=spacy.load('en')):\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        if 'target' in [*qa.keys()]:\n",
        "            context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "            start, end = qa['target']\n",
        "            assert context[start:end + 1].text == qa['answer'], f\"{context[start:end + 1].text} -- {qa['answer']}\"\n",
        "\n",
        "add_targets(train_qas)\n",
        "add_targets(valid_qas)\n",
        "print(f'Length of train qa pairs before filtering out bad qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs before filtering out bad qa pairs: {len(valid_qas):,}')\n",
        "train_qas = [*filter(filter_qas, train_qas)]\n",
        "valid_qas = [*filter(filter_qas, valid_qas)]\n",
        "print(f'Length of train qa pairs after filtering out bad qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs after filtering out bad qa pairs: {len(valid_qas):,}')\n",
        "test_targets(train_qas, spacy.load('en'))\n",
        "test_targets(valid_qas, spacy.load('en'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 86821/86821 [01:40<00:00, 864.39it/s]\n",
            "100%|██████████| 20302/20302 [00:25<00:00, 810.83it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs before filtering out bad qa pairs: 86,821\n",
            "Length of valid qa pairs before filtering out bad qa pairs: 20,302\n",
            "Length of train qa pairs after filtering out bad qa pairs: 85,835\n",
            "Length of valid qa pairs after filtering out bad qa pairs: 20,094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 85835/85835 [01:32<00:00, 923.55it/s]\n",
            "100%|██████████| 20094/20094 [00:14<00:00, 1389.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 41s, sys: 1.91 s, total: 5min 43s\n",
            "Wall time: 5min 43s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6fcQWPBcLo7"
      },
      "source": [
        "***Add context features: Exact Match, Part-of-Speech, Name Entity Recognition, (Normalized) Term Frequency***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF6hHpSNcIk6",
        "outputId": "272261e7-f989-415b-a6aa-6eb11819f3a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def add_context_features(qas, nlp=spacy.load('en')):\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        context = nlp(qa['context'].lower(), disable=['parser'])\n",
        "        question = nlp(qa['question'].lower(), disable=['parser', 'tagger', 'ner'])\n",
        "        count_ = collections.Counter([*map(lambda token: token.text, context)])\n",
        "        cxt_len = len(context)\n",
        "        qa['em'], qa['pos'], qa['ner'], qa['tf'] = \\\n",
        "            zip(*[*map(lambda token: (1 if token.text in [*map(lambda token: token.text, question)] else 0,\n",
        "                                      token.tag_, token.ent_type_ if token.ent_type_ != '' else 'None',\n",
        "                                      count_[token.text] / cxt_len), context)])\n",
        "        mean, std = np.mean(qa['tf']), np.std(qa['tf'])\n",
        "        qa['tf'] = [*map(lambda tf: (tf - mean) / std, qa['tf'])] # Normalized Term Frequencies\n",
        "\n",
        "add_context_features(train_qas)\n",
        "add_context_features(valid_qas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 49%|████▊     | 41636/85835 [11:34<32:28, 22.68it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RZYINhagvj1"
      },
      "source": [
        "***Build datasets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipM5ew0DnQIP"
      },
      "source": [
        "TEXT = Field(init_token='<sos>', eos_token='<eos>', lower=True, tokenizer_language='en', tokenize='spacy', batch_first=True)\n",
        "TARGET = Field(sequential=False, use_vocab=False, is_target=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHTjO-B0pUkR",
        "outputId": "f60bebbc-3dcb-4753-e60a-13b366c433e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_dataset = Dataset([Example.fromdict(data=qa, fields={\n",
        "    'context': ('cxt', TEXT),\n",
        "    'question': ('qst', TEXT),\n",
        "    'answer': ('ans', TEXT),\n",
        "    'target': ('trg', TARGET)\n",
        "}) for qa in tqdm.tqdm(train_qas)], fields={'cxt': TEXT, 'qst': TEXT,\n",
        "                                            'ans': TEXT, 'trg': TARGET})\n",
        "valid_dataset = Dataset([Example.fromdict(data=qa, fields={\n",
        "    'context': ('cxt', TEXT),\n",
        "    'question': ('qst', TEXT),\n",
        "    'answer': ('ans', TEXT),\n",
        "    'target': ('trg', TARGET)\n",
        "}) for qa in tqdm.tqdm(valid_qas)], fields={'cxt': TEXT, 'qst': TEXT,\n",
        "                                            'ans': TEXT, 'trg': TARGET})\n",
        "print(f'Train dataset size: {len(train_dataset.examples):,}')\n",
        "print(f'valid dataset size: {len(valid_dataset.examples):,}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 85835/85835 [02:01<00:00, 708.07it/s]\n",
            "100%|██████████| 20094/20094 [00:29<00:00, 673.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train dataset size: 85,835\n",
            "valid dataset size: 20,094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBhRj5JNqR3M",
        "outputId": "e7a8d5b6-ce10-4915-ce17-d91342345da2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_dataset, min_freq=2)\n",
        "print(f'Length of vocabulary: {len(TEXT.vocab):,}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of vocabulary: 87,691\n",
            "CPU times: user 2.45 s, sys: 2.93 ms, total: 2.45 s\n",
            "Wall time: 2.45 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYsaVy3hsnHE"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "***Align Question Embedding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeFiGiibqS3D"
      },
      "source": [
        "class AlignQuestionEmbeddingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AlignQuestionEmbeddingLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
        "    \n",
        "    def forward(self, cxt, qst, qst_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len, hidden_size] cxt\n",
        "        :param Tensor[batch_size, qst_len, hidden_size] qst\n",
        "        :param Tensor[batch_size, qst_len] mask\n",
        "        :return Tensor[batch_size, cxt_len, hidden_size] context\n",
        "        \"\"\"\n",
        "        cxt = F.relu(self.fc(cxt)) # [batch_size, cxt_len, hidden_size]\n",
        "        qst = F.relu(self.fc(qst)) # [batch_size, qst_len, hidden_size]\n",
        "        scores = torch.matmul(cxt, qst.transpose(-1, -2)) # [batch_size, cxt_len, qst_len]\n",
        "        if qst_mask is not None:\n",
        "            scores = scores.masked_fill(qst_mask.unsqeeze(1) == 0, 1e-18)\n",
        "        attention_weights = F.softmax(scores, dim=-1) # [batch_size, cxt_len, qst_len]\n",
        "        context = torch.matmul(attention_weights, qst) # [batch_size, cxt_len, hidden_size]\n",
        "        return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zelMKFUV-ayF"
      },
      "source": [
        "***Stacked Bidirectional LSTM Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA4xvLwGxcfF"
      },
      "source": [
        "class StackedBiLSTMlayer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, n_layers, dropout):\n",
        "        super(StackedBiLSTMlayer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.lstms = nn.ModuleList([nn.LSTM(input_size=hidden_size * 2 if i > 0 else input_size, hidden_size=hidden_size,\n",
        "                                            bidirectional=True, batch_first=True) for i in range(n_layers)])\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, seq_len, input_size]\n",
        "        :return Tensor[batch_size, seq_len, hidden_size * n_layers * 2]\n",
        "        \"\"\"\n",
        "        outputs = [inputs]\n",
        "        for i, lstm in enumerate(self.lstms):\n",
        "            out, _ = lstm(outputs[-1]) # [batch_size, seq_len, hidden_size * 2]\n",
        "            if i > 0 and i < self.n_layes - 1:\n",
        "                out = self.dropout(out)\n",
        "            outputs.append(out)\n",
        "        return torch.cat(outputs[1:], dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0kKO_ZOA34s"
      },
      "source": [
        "***Linear Self Attention Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PpGSFSQA7so"
      },
      "source": [
        "class LinearSelfAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(LinearSelfAttentionLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W = nn.Linear(hidden_size, 1, bias=False)\n",
        "    \n",
        "    def forward(self, qst, qst_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, qst_len, hidden_size] qst\n",
        "        :param Tensor[batch_size, qst_len] qst_mask\n",
        "        :return Tensor[batch_size, qst_len, 1] attention_weights\n",
        "        \"\"\"\n",
        "        scores = self.W(qst) # [batch_size, qst_len, 1]\n",
        "        if qst_mask is not None:\n",
        "            scores = scores.masked_fill(qst_mask.unsqueeze(-1) == 0, 1e-18) # [batch_size, qst_len, 1]\n",
        "        attention_weights = F.softmax(scores, dim=1) # [batch_size, qst_len, 1]\n",
        "        return attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhSBbY4kHYle"
      },
      "source": [
        "***Bi-Linear Attention Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbUkpzbaHX3g"
      },
      "source": [
        "class BiLinearAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BiLinearAttentionLayer, self).__init__()\n",
        "        self."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC8Jh46f-j22"
      },
      "source": [
        "***DrQA Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8xrxN8R7Em1"
      },
      "source": [
        "class DrQA(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, n_layers, dropout):\n",
        "        super(DrQA, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.cxt_embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.qst_embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.align_question_embedding_layer = AlignQuestionEmbedding(hidden_size=embedding_size)\n",
        "        self.cxt_stacked_bi_lstm_layer = StackedBiLSTMlayer(input_size=embedding_size, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
        "        self.qst_stacked_bi_lstm_layer = StackedBiLSTMlayer(input_size=embedding_size, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}