{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - Document reader Question Answering - DrQA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWd0uBnHE61La+ODGnBXjS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dksifoua/Question-Answering/blob/master/1%20-%20DrQA%20-%20Document%20reader%20Question%20Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h8encuudFC-",
        "outputId": "99298546-37af-4c33-f859-0b3abb49d5f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Oct 10 22:00:06 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFHeYfTa5iBg"
      },
      "source": [
        "## Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cIZzQAImLQO"
      },
      "source": [
        "!pip install tqdm --upgrade >> /dev/null 2>&1\n",
        "!pip install torchtext --upgrade >> /dev/null 2>&1\n",
        "!pip install spacy --upgrade >> /dev/null 2>&1\n",
        "!python -m spacy download en >> /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gHtnSaemPVM"
      },
      "source": [
        "import re\n",
        "import json\n",
        "import tqdm\n",
        "import spacy\n",
        "import warnings\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.data import Dataset, Example, Field\n",
        "from torchtext.data.iterator import BucketIterator\n",
        "from torchtext.data.metrics import bleu_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q7DBR5gmTZa",
        "outputId": "f3b93945-26e1-4846-b5d4-5d0988329c6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "\n",
        "SEED = 546\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNZkKM5G5fgi"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "***Download data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZhbHNhT4YRn",
        "outputId": "2b4fd753-33d5-411b-ae43-04323e3f5789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "%%time\n",
        "!mkdir ./data\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json \\\n",
        "    -O ./data/train.json\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json \\\n",
        "    -O ./data/valid.json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-10 22:00:33--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘./data/train.json’\n",
            "\n",
            "./data/train.json   100%[===================>]  40.17M  92.9MB/s    in 0.4s    \n",
            "\n",
            "2020-10-10 22:00:34 (92.9 MB/s) - ‘./data/train.json’ saved [42123633/42123633]\n",
            "\n",
            "--2020-10-10 22:00:34--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘./data/valid.json’\n",
            "\n",
            "./data/valid.json   100%[===================>]   4.17M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-10-10 22:00:34 (33.6 MB/s) - ‘./data/valid.json’ saved [4370528/4370528]\n",
            "\n",
            "CPU times: user 14.3 ms, sys: 21 ms, total: 35.3 ms\n",
            "Wall time: 1.15 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs5S3iohgiMM"
      },
      "source": [
        "***Load data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vTuq9jl3oUJ",
        "outputId": "16b5bc7d-9cea-4444-d01b-fbdb58e7b953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def load_json(path):\n",
        "    with open(path, mode='r', encoding='utf-8') as file:\n",
        "        return json.load(file)['data']\n",
        "\n",
        "train_raw = load_json(path='./data/train.json')\n",
        "valid_raw = load_json(path='./data/valid.json')\n",
        "print(f'Length of raw train data: {len(train_raw):,}')\n",
        "print(f'Length of raw valid data: {len(valid_raw):,}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of raw train data: 442\n",
            "Length of raw valid data: 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6rKVZjcgmF7"
      },
      "source": [
        "***Parse data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfhHilFU6fb_",
        "outputId": "ffc40974-c7aa-46a3-a232-714ba52e0753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "def parse_json(data):\n",
        "    qas = []\n",
        "    for paragraphs in data:\n",
        "        for para in paragraphs['paragraphs']:\n",
        "            context = para['context']\n",
        "            for qa in para['qas']:\n",
        "                id = qa['id']\n",
        "                question = qa['question']\n",
        "                for ans in qa['answers']:\n",
        "                    answer = ans['text']\n",
        "                    ans_start = ans['answer_start']\n",
        "                    qas.append({\n",
        "                        'id': id,\n",
        "                        'context': context,\n",
        "                        'question': question,\n",
        "                        'answer': answer,\n",
        "                        'answer_start': ans_start,\n",
        "                    })\n",
        "    return qas\n",
        "\n",
        "train_qas = parse_json(train_raw)\n",
        "valid_qas = parse_json(valid_raw)\n",
        "print(f'Length of train qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs: {len(valid_qas):,}')\n",
        "\n",
        "print('Context:', train_qas[0]['context'])\n",
        "print('Question:', train_qas[0]['question'])\n",
        "print('Answer starts at:', train_qas[0]['answer_start'])\n",
        "print('Answer:', train_qas[0]['answer'])\n",
        "\n",
        "for i in range(len(train_qas)): # Test labels are correct in train set\n",
        "    assert train_qas[i]['answer'] == train_qas[i]['context'][train_qas[i]['answer_start']:train_qas[i]['answer_start']+len(train_qas[i]['answer'])]\n",
        "\n",
        "for i in range(len(valid_qas)): # Test labels are correct in validation set\n",
        "    assert valid_qas[i]['answer'] == valid_qas[i]['context'][valid_qas[i]['answer_start']:valid_qas[i]['answer_start']+len(valid_qas[i]['answer'])]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs: 86,821\n",
            "Length of valid qa pairs: 20,302\n",
            "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "Question: When did Beyonce start becoming popular?\n",
            "Answer starts at: 269\n",
            "Answer: in the late 1990s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs87TY30gpOG"
      },
      "source": [
        "***Add targets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMWsmFjjt5vB",
        "outputId": "1eb73c8e-8918-4dc2-bfc7-8df9964e49ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%%time\n",
        "def add_targets(qas, nlp=spacy.load('en')):\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "        answer = nlp(qa['answer'], disable=['parser','tagger','ner'])\n",
        "        ans_start = qa['answer_start']\n",
        "        for i in range(len(context)):\n",
        "            if context[i].idx == ans_start:\n",
        "                ans = context[i:i + len(answer)]\n",
        "                qa['target'] = (ans[0].i, ans[-1].i)\n",
        "                break\n",
        "\n",
        "def filter_qas(qa, nlp=spacy.load('en')):\n",
        "    if 'target' in [*qa.keys()]:\n",
        "        context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "        start, end = qa['target']\n",
        "        return context[start:end+1].text == qa['answer']\n",
        "    return False\n",
        "\n",
        "def test_targets(qas, nlp=spacy.load('en')):\n",
        "    for qa in tqdm.tqdm(qas):\n",
        "        if 'target' in [*qa.keys()]:\n",
        "            context = nlp(qa['context'], disable=['parser','tagger','ner'])\n",
        "            start, end = qa['target']\n",
        "            assert context[start:end + 1].text == qa['answer'], f\"{context[start:end + 1].text} -- {qa['answer']}\"\n",
        "\n",
        "add_targets(train_qas)\n",
        "add_targets(valid_qas)\n",
        "print(f'Length of train qa pairs before filtering out bad qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs before filtering out bad qa pairs: {len(valid_qas):,}')\n",
        "train_qas = [*filter(filter_qas, train_qas)]\n",
        "valid_qas = [*filter(filter_qas, valid_qas)]\n",
        "print(f'Length of train qa pairs after filtering out bad qa pairs: {len(train_qas):,}')\n",
        "print(f'Length of valid qa pairs after filtering out bad qa pairs: {len(valid_qas):,}')\n",
        "test_targets(train_qas, spacy.load('en'))\n",
        "test_targets(valid_qas, spacy.load('en'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 86821/86821 [01:34<00:00, 917.05it/s]\n",
            "100%|██████████| 20302/20302 [00:23<00:00, 862.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of train qa pairs before filtering out bad qa pairs: 86,821\n",
            "Length of valid qa pairs before filtering out bad qa pairs: 20,302\n",
            "Length of train qa pairs after filtering out bad qa pairs: 85,835\n",
            "Length of valid qa pairs after filtering out bad qa pairs: 20,094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 85835/85835 [01:27<00:00, 983.90it/s]\n",
            "100%|██████████| 20094/20094 [00:13<00:00, 1532.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 20s, sys: 1.96 s, total: 5min 22s\n",
            "Wall time: 5min 22s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RZYINhagvj1"
      },
      "source": [
        "***Build datasets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipM5ew0DnQIP"
      },
      "source": [
        "TEXT = Field(lower=True, tokenizer_language='en', tokenize='spacy', batch_first=True)\n",
        "TARGET = Field(sequential=False, use_vocab=False, batch_first=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHTjO-B0pUkR",
        "outputId": "52ec26d5-2c93-4b53-9507-380190b3afa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_dataset = Dataset([Example.fromdict(data=qa, fields={\n",
        "    'context': ('cxt', TEXT),\n",
        "    'question': ('qst', TEXT),\n",
        "    'answer': ('ans', TEXT),\n",
        "    'target': ('trg', TARGET)\n",
        "}) for qa in tqdm.tqdm(train_qas)], fields={'cxt': TEXT, 'qst': TEXT,\n",
        "                                            'ans': TEXT, 'trg': TARGET})\n",
        "valid_dataset = Dataset([Example.fromdict(data=qa, fields={\n",
        "    'context': ('cxt', TEXT),\n",
        "    'question': ('qst', TEXT),\n",
        "    'answer': ('ans', TEXT),\n",
        "    'target': ('trg', TARGET)\n",
        "}) for qa in tqdm.tqdm(valid_qas)], fields={'cxt': TEXT, 'qst': TEXT,\n",
        "                                            'ans': TEXT, 'trg': TARGET})\n",
        "print(f'Train dataset size: {len(train_dataset.examples):,}')\n",
        "print(f'valid dataset size: {len(valid_dataset.examples):,}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 85835/85835 [02:04<00:00, 691.41it/s]\n",
            "100%|██████████| 20094/20094 [00:30<00:00, 649.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train dataset size: 85,835\n",
            "valid dataset size: 20,094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBhRj5JNqR3M",
        "outputId": "a2530e16-ec35-4a7f-d7ce-649b39df7a0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_dataset)\n",
        "print(f'Length of vocabulary: {len(TEXT.vocab):,}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of vocabulary: 91,396\n",
            "CPU times: user 2.56 s, sys: 16 ms, total: 2.58 s\n",
            "Wall time: 2.59 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYsaVy3hsnHE"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zelMKFUV-ayF"
      },
      "source": [
        "***Stacked Bidirectional LSTM Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA4xvLwGxcfF"
      },
      "source": [
        "class StackedBiLSTMLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, n_layers, dropout):\n",
        "        super(StackedBiLSTMLayer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.lstms = nn.ModuleList([nn.LSTM(input_size=hidden_size * 2 if i > 0 else input_size, hidden_size=hidden_size,\n",
        "                                            bidirectional=True, batch_first=True) for i in range(n_layers)])\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, seq_len, input_size]\n",
        "        :return Tensor[batch_size, seq_len, hidden_size * n_layers * 2]\n",
        "        \"\"\"\n",
        "        outputs = [inputs]\n",
        "        for i, lstm in enumerate(self.lstms):\n",
        "            out, _ = lstm(outputs[-1]) # [batch_size, seq_len, hidden_size * 2]\n",
        "            if i > 0 and i < self.n_layers - 1:\n",
        "                out = self.dropout(out)\n",
        "            outputs.append(out)\n",
        "        return torch.cat(outputs[1:], dim=-1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZqiiy7wyYrm"
      },
      "source": [
        "***Align Question Embedding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeFiGiibqS3D"
      },
      "source": [
        "class AlignQuestionEmbeddingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AlignQuestionEmbeddingLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
        "    \n",
        "    def forward(self, cxt, qst, qst_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len, hidden_size] cxt\n",
        "        :param Tensor[batch_size, qst_len, hidden_size] qst\n",
        "        :param Tensor[batch_size, qst_len] mask\n",
        "        :return Tensor[batch_size, cxt_len, hidden_size] context\n",
        "        \"\"\"\n",
        "        cxt = F.relu(self.fc(cxt)) # [batch_size, cxt_len, hidden_size]\n",
        "        qst = F.relu(self.fc(qst)) # [batch_size, qst_len, hidden_size]\n",
        "        scores = torch.matmul(cxt, qst.transpose(-1, -2)) # [batch_size, cxt_len, qst_len]\n",
        "        if qst_mask is not None:\n",
        "            scores = scores.masked_fill(qst_mask.unsqueeze(1) == 0, 1e-18)\n",
        "        attention_weights = F.softmax(scores, dim=-1) # [batch_size, cxt_len, qst_len]\n",
        "        context = torch.matmul(attention_weights, qst) # [batch_size, cxt_len, hidden_size]\n",
        "        return context"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkpr81T-z2XA"
      },
      "source": [
        "***Question Encoding Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OizoayNez7Le"
      },
      "source": [
        "class QuestionEncodingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(QuestionEncodingLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "    def forward(self, qst_sequences, qst_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, seq_len, hidden_size] qst_sequences\n",
        "        :param Tensor[batch_size, seq_len] qst_mask\n",
        "        :return Tensor[batch_size, hidden_size] context\n",
        "        \"\"\"\n",
        "        scores = self.W(qst_sequences) # [batch_size, seq_len, 1]\n",
        "        if qst_mask is not None:\n",
        "            scores = scores.masked_fill(qst_mask.unsqueeze(-1) == 0, 1e-18)\n",
        "        attention_weights = F.softmax(scores, dim=1) # [batch_size, seq_len, 1]\n",
        "        context = torch.matmul(attention_weights.transpose(-1, -2), qst_sequences) # [batch_size, 1, hidden_size]\n",
        "        return context.squeeze(1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFfqiP4V9k99"
      },
      "source": [
        "***BiLinear Attention Layer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbPaa3U69oO0"
      },
      "source": [
        "class BiLinearAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, cxt_size, qst_size):\n",
        "        super(BiLinearAttentionLayer, self).__init__()\n",
        "        self.cxt_size = cxt_size\n",
        "        self.qst_size = qst_size\n",
        "        self.linear = nn.Linear(qst_size, cxt_size)\n",
        "\n",
        "    def forward(self, cxt_encoded, qst_encoded, cxt_mask):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len, cxt_size] cxt_encoded\n",
        "        :param Tensor[batch_size, qst_size] qst_encoded\n",
        "        :param Tensor[batch_size, cxt_len] cxt_mask\n",
        "        :param Tensor[batch_size, cxt_len] scores\n",
        "        \"\"\"\n",
        "        qst_encoded = self.linear(qst_encoded) # [batch_size, cxt_size]\n",
        "        scores = torch.matmul(cxt_encoded, qst_encoded.unsqueeze(-1)) # [batch_size, cxt_len, 1]\n",
        "        if cxt_mask is not None:\n",
        "            scores = scores.squeeze(-1).masked_fill(cxt_mask == 0, 1e-18) # [batch_size, cxt_len]\n",
        "        return scores"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC8Jh46f-j22"
      },
      "source": [
        "***DrQA Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8xrxN8R7Em1"
      },
      "source": [
        "class DrQA(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, n_layers, dropout, pad_index):\n",
        "        super(DrQA, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pad_index = pad_index\n",
        "        self.cxt_embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.qst_embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.align_question_embedding_layer = AlignQuestionEmbeddingLayer(hidden_size=embedding_size)\n",
        "        self.cxt_stacked_bi_lstm_layer = StackedBiLSTMLayer(input_size=embedding_size, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
        "        self.qst_stacked_bi_lstm_layer = StackedBiLSTMLayer(input_size=embedding_size, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
        "        self.qst_encoding_layer = QuestionEncodingLayer(hidden_size=hidden_size * n_layers * 2)\n",
        "        self.bilinear_attention_layer_start = BiLinearAttentionLayer(cxt_size=hidden_size * n_layers * 2, qst_size=hidden_size * n_layers * 2)\n",
        "        self.bilinear_attention_layer_end = BiLinearAttentionLayer(cxt_size=hidden_size * n_layers * 2, qst_size=hidden_size * n_layers * 2)\n",
        "    \n",
        "    def make_cxt_mask(self, cxt_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len]\n",
        "        :return Tensor[batch_size, cxt_len]\n",
        "        \"\"\"\n",
        "        return cxt_sequences != self.pad_index\n",
        "    \n",
        "    def make_qst_mask(self, qst_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, qst_len]\n",
        "        :return Tensor[batch_size, qst_len]\n",
        "        \"\"\"\n",
        "        return qst_sequences != self.pad_index\n",
        "    \n",
        "    @staticmethod\n",
        "    def decode(start_scores, end_scores, max_len=None):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len] start_scores\n",
        "        :param Tensor[batch_size, cxt_len] end_scores\n",
        "        \"\"\"\n",
        "        pred_start, pred_end, pred_score = [], [], []\n",
        "        max_len = max_len or start_scores.size(1)\n",
        "        for i in range(start_scores.size(0)):\n",
        "            scores = torch.ger(start_scores[i], end_scores[i]) # Outer product of scores to get a full matrix            \n",
        "            scores = scores.triu_().tril_(max_len - 1) # Zero out negative length and over-length span scores\n",
        "            max_scores, max_idx = scores.view(-1).topk(1)\n",
        "            pred_score.append(max_scores)\n",
        "            pred_start.append(max_idx.tolist()[0] // scores.size(0))\n",
        "            pred_end.append(max_idx.tolist()[0] % scores.size(0))\n",
        "        return pred_start, pred_end, pred_score\n",
        "\n",
        "    def forward(self, cxt_sequences, qst_sequences):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len] cxt_sequences\n",
        "        :param Tensor[batch_size, qst_len] qst_sequences\n",
        "        :param Tensor[batch_size, cxt_len] cxt_mask\n",
        "        :param Tensor[batch_size, qst_len] qst_mask\n",
        "        :return Tensor[batch_size, cxt_len] start_scores\n",
        "        :return Tensor[batch_size, cxt_len] end_scores\n",
        "        \"\"\"\n",
        "        cxt_mask = self.make_cxt_mask(cxt_sequences) # [batch_size, cxt_len]\n",
        "        qst_mask = self.make_qst_mask(qst_sequences) # [batch_size, cxt_len]\n",
        "        \n",
        "        cxt_embedded = self.dropout(self.cxt_embedding(cxt_sequences)) # [batch_size, cxt_len, embedding_size]\n",
        "        qst_embedded = self.dropout(self.qst_embedding(qst_sequences)) # [batch_size, cxt_len, embedding_size]\n",
        "        \n",
        "        cxt_qst_aligned = self.align_question_embedding_layer(cxt=cxt_embedded, qst=qst_embedded, qst_mask=qst_mask) # [batch_size, cxt_len, embedding_size]\n",
        "        cxt_encoded = self.cxt_stacked_bi_lstm_layer(inputs=cxt_qst_aligned) # [batch_size, cxt_len, embedding_size * n_layers * 2]\n",
        "        \n",
        "        qst_lstm = self.qst_stacked_bi_lstm_layer(inputs=qst_embedded) # [batch_size, qst_len, hidden_size * n_layers * 2]\n",
        "        qst_encoded = self.qst_encoding_layer(qst_sequences=qst_lstm, qst_mask=qst_mask) # [batch_size, hidden_size * n_layers * 2]\n",
        "\n",
        "        start_scores = self.bilinear_attention_layer_start(cxt_encoded=cxt_encoded, qst_encoded=qst_encoded, cxt_mask=cxt_mask) # [batch_size, cxt_len]\n",
        "        end_scores = self.bilinear_attention_layer_end(cxt_encoded=cxt_encoded, qst_encoded=qst_encoded, cxt_mask=cxt_mask) # [batch_size, cxt_len]\n",
        "\n",
        "        return F.log_softmax(start_scores, dim=1), F.log_softmax(end_scores, dim=1)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNCQG-tjBHcZ"
      },
      "source": [
        "***Training routines***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBWI7liR6gH2"
      },
      "source": [
        "class AverageMeter:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.value = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "        self.average = 0.\n",
        "        \n",
        "    def reset(self):\n",
        "        self.value = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "        self.average = 0.\n",
        "        \n",
        "    def update(self, value, n=1):\n",
        "        self.value = value\n",
        "        self.sum += value * n\n",
        "        self.count += n\n",
        "        self.average = self.sum / self.count"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV2gVekkBMS3"
      },
      "source": [
        "def exact_match_score(predictions, targets):\n",
        "    \"\"\"\n",
        "    :param list[batch_size] predictions\n",
        "    :param list[batch_size] targets\n",
        "    :return float Exact Match score\n",
        "    \"\"\"\n",
        "    return 100.0 * sum(map(lambda y: y[0] == y[1], zip(predictions, targets))) / len(targets)\n",
        "\n",
        "def f1_scores(predictions, targets):\n",
        "    \"\"\"\n",
        "    precision = correct / length_prediction\n",
        "    recall = correct / length_target\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    :param list[batch_size] predictions\n",
        "    :param list[batch_size] targets\n",
        "    :return float F1 score\n",
        "    \"\"\"\n",
        "    f1_scores = []\n",
        "    for (pred, target) in zip(predictions, targets):\n",
        "        if len(pred) == 0:\n",
        "            f1_scores.append(0)\n",
        "            continue\n",
        "        correct = sum([token in target for token in pred])\n",
        "        precision = correct / len(pred)\n",
        "        recall = correct / len(target)\n",
        "        if precision + recall == 0:\n",
        "            f1_scores.append(0)\n",
        "            continue\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "        f1_scores.append(f1)\n",
        "    return np.mean(f1_scores) * 100.0"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72GXApi2H23S"
      },
      "source": [
        "class Trainer:\n",
        "    \n",
        "    def __init__(self, model, optimizer, criterion):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "\n",
        "    def calculate_metrics(self, cxt, ans, starts, ends):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len] cxt\n",
        "        :param Tensor[batch_size, ans_len] ans\n",
        "        :param Tensor[batch_size,] starts\n",
        "        :param Tensor[batch_size,] ends\n",
        "        :return tuple(float, float) em, f1\n",
        "        \"\"\"\n",
        "        cxt, ans = cxt.tolist(), ans.tolist()\n",
        "        cxt = [*map(lambda x: [*filter(lambda y: y != self.model.pad_index, x)], cxt)] # Remove pad tokens\n",
        "        ans = [*map(lambda x: [*filter(lambda y: y != self.model.pad_index, x)], ans)] # Remove pad tokens\n",
        "        preds = [*map(lambda x: x[0][x[1]:x[2] + 1], zip(cxt, starts, ends))] # Retreive prediction from contexts\n",
        "        \n",
        "        print()\n",
        "        print(starts[0], ends[0])\n",
        "        print([*map(TEXT.vocab.itos.__getitem__, preds[0])])\n",
        "        print([*map(TEXT.vocab.itos.__getitem__, ans[0])])\n",
        "\n",
        "        em, f1 = exact_match_score(preds, ans), f1_scores(preds, ans)\n",
        "        return em, f1\n",
        "    \n",
        "    def get_indexes(self, start_scores, end_scores):\n",
        "        \"\"\"\n",
        "        :param Tensor[batch_size, cxt_len] start_scores\n",
        "        :param Tensor[batch_size, cxt_len] end_scores\n",
        "        \"\"\"\n",
        "        \n",
        "    def train_step(self, loader, epoch, grad_clip, max_len):\n",
        "        loss_tracker, em_tracker, f1_tracker = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "        self.model.train()\n",
        "        progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "        for i, batch in progress_bar:\n",
        "            cxt, qst, ans, trg = batch.cxt, batch.qst, batch.ans, batch.trg\n",
        "            self.optimizer.zero_grad()\n",
        "            start_scores, end_scores = self.model(cxt_sequences=cxt, qst_sequences=qst) # [batch_size, cxt_len]\n",
        "            loss = self.criterion(start_scores, trg[:, 0]) + self.criterion(end_scores, trg[:, 1])\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
        "            self.optimizer.step()\n",
        "            starts, ends, _ = self.model.decode(start_scores, end_scores, max_len=max_len)\n",
        "            em, f1 = self.calculate_metrics(cxt, ans, starts, ends)\n",
        "            loss_tracker.update(loss.item()); em_tracker.update(em); f1_tracker.update(f1)\n",
        "            loss_, em_, f1_ = loss_tracker.average, em_tracker.average, f1_tracker.average\n",
        "            progress_bar.set_description(f'Epoch: {epoch+1:02d} -     loss: {loss_:.3f} -     em: {em_:.3f}% -     f1: {f1_:.3f}%')\n",
        "        return loss_tracker.average, em_tracker.average, f1_tracker.average\n",
        "    \n",
        "    def validate(self, loader, epoch, max_len):\n",
        "        loss_tracker, em_tracker, f1_tracker = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "            for i, batch in progress_bar:\n",
        "                cxt, qst, ans, trg = batch.cxt, batch.qst, batch.ans, batch.trg\n",
        "                start_scores, end_scores = self.model(cxt_sequences=cxt, qst_sequences=qst) # [batch_size, cxt_len]\n",
        "                loss = self.criterion(start_scores, trg[:, 0]) + self.criterion(end_scores, trg[:, 1])\n",
        "                starts, ends, _ = self.model.decode(start_scores, end_scores, max_len=max_len)\n",
        "                em, f1 = self.calculate_metrics(cxt, ans, starts, ends)\n",
        "                loss_tracker.update(loss.item()); em_tracker.update(em); f1_tracker.update(f1)\n",
        "                loss_, em_, f1_ = loss_tracker.average, em_tracker.average, f1_tracker.average\n",
        "                progress_bar.set_description(f'Epoch: {epoch+1:02d} - val_loss: {loss_:.3f} - val_em: {em_:.3f}% - val_f1: {f1_:.3f}%')\n",
        "        return loss_tracker.average, em_tracker.average, f1_tracker.average\n",
        "    \n",
        "    def train(self, train_loader, valid_loader, n_epochs, grad_clip, max_len):\n",
        "        history, best_loss = {'loss': [], 'em': [], 'f1': [], 'val_loss': [], 'val_em': [], 'val_f1': []}, np.inf\n",
        "        for epoch in range(n_epochs):\n",
        "            loss, em, f1 = self.train_step(train_loader, epoch, grad_clip, max_len)\n",
        "            val_loss, val_em, val_f1 = self.validate(valid_loader, epoch, max_len)\n",
        "            if best_loss > val_loss:\n",
        "                best_loss = val_loss\n",
        "                torch.save(self.model.state_dict(), './checkpoints/DrQA.pth')\n",
        "            history['em'].append(em); history['val_em'].append(val_em)\n",
        "            history['f1'].append(f1); history['val_f1'].append(f1)\n",
        "            history['loss'].append(loss); history['val_loss'].append(val_loss)\n",
        "        return history"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlWgRbN9ROLA"
      },
      "source": [
        "***Train the model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ-a11PIRRRq"
      },
      "source": [
        "N_LAYERS = 3\n",
        "HIDDEN_SIZE = 128\n",
        "EMBED_SIZE = 300\n",
        "DROPOUT = 0.3\n",
        "N_EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "GRAD_CLIP = 10.0\n",
        "MAX_LEN = 15"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LodDrpgLRo_h",
        "outputId": "adbca449-a12a-44d0-9b17-ccfecb4dff43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "drqa = DrQA(vocab_size=len(TEXT.vocab), embedding_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, n_layers=N_LAYERS,\n",
        "            dropout=DROPOUT, pad_index=TEXT.vocab.stoi[TEXT.pad_token]).to(DEVICE)\n",
        "optimizer = optim.Adamax(params=drqa.parameters())\n",
        "criterion = nn.NLLLoss(ignore_index=TEXT.vocab.stoi[TEXT.pad_token])\n",
        "print(f'Number of parameters of the model: {sum(p.numel() for p in drqa.parameters() if p.requires_grad):,}')\n",
        "print(drqa)\n",
        "trainer = Trainer(model=drqa, optimizer=optimizer, criterion=criterion)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters of the model: 58,571,549\n",
            "DrQA(\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (cxt_embedding): Embedding(91396, 300)\n",
            "  (qst_embedding): Embedding(91396, 300)\n",
            "  (align_question_embedding_layer): AlignQuestionEmbeddingLayer(\n",
            "    (fc): Linear(in_features=300, out_features=300, bias=True)\n",
            "  )\n",
            "  (cxt_stacked_bi_lstm_layer): StackedBiLSTMLayer(\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (lstms): ModuleList(\n",
            "      (0): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
            "      (1): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "      (2): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (qst_stacked_bi_lstm_layer): StackedBiLSTMLayer(\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (lstms): ModuleList(\n",
            "      (0): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
            "      (1): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "      (2): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (qst_encoding_layer): QuestionEncodingLayer(\n",
            "    (W): Linear(in_features=768, out_features=1, bias=True)\n",
            "  )\n",
            "  (bilinear_attention_layer_start): BiLinearAttentionLayer(\n",
            "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "  )\n",
            "  (bilinear_attention_layer_end): BiLinearAttentionLayer(\n",
            "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahAzyTraSsy5",
        "outputId": "ecc33ab9-280e-45b3-f5dd-012a7550da10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "train_iterator, valid_iterator =  BucketIterator.splits((train_dataset, valid_dataset), batch_size=BATCH_SIZE, sort=False, device=DEVICE)\n",
        "!mkdir -p ./checkpoints\n",
        "history = trainer.train(train_loader=train_iterator, valid_loader=valid_iterator, n_epochs=N_EPOCHS, grad_clip=GRAD_CLIP, max_len=MAX_LEN)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 -     loss: 10.715 -     em: 0.000% -     f1: 5.421%:   0%|          | 1/1342 [00:00<08:33,  2.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "151 156\n",
            "['and', 'what', 'goes', 'out', ',', 'and']\n",
            "['pneuma', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 -     loss: 10.720 -     em: 0.000% -     f1: 6.586%:   0%|          | 2/1342 [00:00<07:43,  2.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "143 143\n",
            "['<pad>']\n",
            "['anokha', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-161-5ea83b8a0e4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mBucketIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir -p ./checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGRAD_CLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-158-22b6110eb81a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, valid_loader, n_epochs, grad_clip, max_len)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'em'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'f1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_em'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_f1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_em\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-158-22b6110eb81a>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, loader, epoch, grad_clip, max_len)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mcxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mstart_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcxt_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqst_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, cxt_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-ae1a6d1767e8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, cxt_sequences, qst_sequences)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mcxt_qst_aligned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_question_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcxt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcxt_embedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqst_embedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqst_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqst_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, cxt_len, embedding_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mcxt_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcxt_stacked_bi_lstm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcxt_qst_aligned\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, cxt_len, embedding_size * n_layers * 2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mqst_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqst_stacked_bi_lstm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqst_embedded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, qst_len, hidden_size * n_layers * 2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-08308cb0cc0c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, seq_len, hidden_size * 2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 577\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 202.00 MiB (GPU 0; 14.73 GiB total capacity; 12.34 GiB already allocated; 59.88 MiB free; 13.79 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17zun4BjVqgW"
      },
      "source": [
        "_, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axes[0].plot(history['loss'], label='train')\n",
        "axes[0].plot(history['val_loss'], label='valid')\n",
        "axes[0].set_title('Loss history')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].grid(True)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(history['em'], label='train')\n",
        "axes[1].plot(history['val_em'], label='valid')\n",
        "axes[1].set_title('Exact Match history')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Exact Match (%)')\n",
        "axes[1].grid(True)\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].plot(history['f1'], label='train')\n",
        "axes[2].plot(history['val_f1'], label='valid')\n",
        "axes[2].set_title('F1 score history')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('F1 score (%)')\n",
        "axes[2].grid(True)\n",
        "axes[2].legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSdUKxAOwwkv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}